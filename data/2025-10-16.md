<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 22]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models](https://arxiv.org/abs/2510.12864)
*Imran Khan*

Main category: cs.AI

TL;DR: 提出了RID框架，一种零样本元提示技术，通过结构化认知模式让LLM从字面规则遵循转向目标导向推理，显著提升人类对齐度。


<details>
  <summary>Details</summary>
Motivation: LLM作为智能体推理引擎存在规则僵化问题，即过度遵循显式规则而忽视人类常识和意图，这阻碍了可信自主智能体的构建。

Method: RID框架是一种低计算成本的元提示技术，为模型提供结构化认知模式，包括任务解构、规则分类、冲突结果权衡和最终决策论证。

Result: 在20个需要细微判断的场景基准测试中，RID框架达到95%的人类对齐分数，显著优于基线(80%)和思维链提示(75%)，且产生更高质量的意图驱动推理。

Conclusion: RID框架提供了一种实用、易用且有效的方法，使LLM从字面指令遵循转向更灵活的目标导向推理，为构建更可靠实用的AI智能体铺平道路。

Abstract: Large Language Models (LLMs) are increasingly being deployed as the reasoning
engines for agentic AI systems, yet they exhibit a critical flaw: a rigid
adherence to explicit rules that leads to decisions misaligned with human
common sense and intent. This "rule-rigidity" is a significant barrier to
building trustworthy autonomous agents. While prior work has shown that
supervised fine-tuning (SFT) with human explanations can mitigate this issue,
SFT is computationally expensive and inaccessible to many practitioners. To
address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a
novel, low-compute meta-prompting technique designed to elicit human-aligned
exception handling in LLMs in a zero-shot manner. The RID framework provides
the model with a structured cognitive schema for deconstructing tasks,
classifying rules, weighing conflicting outcomes, and justifying its final
decision. We evaluated the RID framework against baseline and Chain-of-Thought
(CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced
judgment across diverse domains. Our human-verified results demonstrate that
the RID framework significantly improves performance, achieving a 95% Human
Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT.
Furthermore, it consistently produces higher-quality, intent-driven reasoning.
This work presents a practical, accessible, and effective method for steering
LLMs from literal instruction-following to liberal, goal-oriented reasoning,
paving the way for more reliable and pragmatic AI agents.

</details>


### [2] [DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping](https://arxiv.org/abs/2510.12979)
*Wei Fan,Wenlin Yao,Zheng Li,Feng Yao,Xin Liu,Liang Qiu,Qingyu Yin,Yangqiu Song,Bing Yin*

Main category: cs.AI

TL;DR: DeepPlanner是一个端到端的强化学习框架，通过基于熵的令牌级优势塑造和选择性样本级优势加权，有效增强深度研究代理的规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖推理阶段的隐式规划，要么引入显式规划器但未系统优化规划阶段。研究发现，在标准强化学习下，规划令牌的熵显著高于其他动作令牌，表明存在未优化的不确定决策点。

Method: 提出DeepPlanner框架：1）使用基于熵的令牌级优势塑造，为高熵令牌分配更大的更新；2）选择性加权规划密集型rollouts的样本级优势。

Result: 在七个深度研究基准测试上的广泛实验表明，DeepPlanner提高了规划质量，并在显著降低的训练预算下实现了最先进的结果。

Conclusion: DeepPlanner通过专门优化规划阶段，有效解决了现有方法中规划令牌熵过高的问题，提升了深度研究代理的规划能力。

Abstract: Large language models (LLMs) augmented with multi-step reasoning and action
generation abilities have shown promise in leveraging external tools to tackle
complex tasks that require long-horizon planning. However, existing approaches
either rely on implicit planning in the reasoning stage or introduce explicit
planners without systematically addressing how to optimize the planning stage.
As evidence, we observe that under vanilla reinforcement learning (RL),
planning tokens exhibit significantly higher entropy than other action tokens,
revealing uncertain decision points that remain under-optimized. To address
this, we propose DeepPlanner, an end-to-end RL framework that effectively
enhances the planning capabilities of deep research agents. Our approach shapes
token-level advantage with an entropy-based term to allocate larger updates to
high entropy tokens, and selectively upweights sample-level advantages for
planning-intensive rollouts. Extensive experiments across seven deep research
benchmarks demonstrate that DeepPlanner improves planning quality and achieves
state-of-the-art results under a substantially lower training budget.

</details>


### [3] [SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents](https://arxiv.org/abs/2510.12985)
*Simon Sinong Zhan,Yao Liu,Philip Wang,Zinan Wang,Qineng Wang,Zhian Ruan,Xiangyu Shi,Xinyu Cao,Frank Yang,Kangrui Wang,Huajie Shao,Manling Li,Qi Zhu*

Main category: cs.AI

TL;DR: Sentinel是首个基于形式化时序逻辑的框架，用于在多层级（语义、规划、轨迹）上评估LLM具身智能体的物理安全性，相比启发式方法提供更严谨的安全验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式规则或主观LLM判断来评估具身智能体安全性，缺乏形式化基础，无法精确指定状态不变性、时序依赖和时间约束等物理安全要求。

Method: 使用时序逻辑将自然语言安全需求形式化，构建三级验证管道：语义级验证LLM对安全需求的理解；规划级验证高层动作计划；轨迹级合并执行轨迹为计算树进行最终安全检查。

Result: 在VirtualHome和ALFRED环境中评估多个LLM具身智能体，Sentinel能够发现先前方法忽略的安全违规，并提供对失败模式的深入分析。

Conclusion: 通过将物理安全基于时序逻辑并在多层级应用验证方法，Sentinel为系统评估LLM具身智能体在物理环境中的安全性提供了严谨基础。

Abstract: We present Sentinel, the first framework for formally evaluating the physical
safety of Large Language Model(LLM-based) embodied agents across the semantic,
plan, and trajectory levels. Unlike prior methods that rely on heuristic rules
or subjective LLM judgments, Sentinel grounds practical safety requirements in
formal temporal logic (TL) semantics that can precisely specify state
invariants, temporal dependencies, and timing constraints. It then employs a
multi-level verification pipeline where (i) at the semantic level, intuitive
natural language safety requirements are formalized into TL formulas and the
LLM agent's understanding of these requirements is probed for alignment with
the TL formulas; (ii) at the plan level, high-level action plans and subgoals
generated by the LLM agent are verified against the TL formulas to detect
unsafe plans before execution; and (iii) at the trajectory level, multiple
execution trajectories are merged into a computation tree and efficiently
verified against physically-detailed TL specifications for a final safety
check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate
multiple LLM-based embodied agents against diverse safety requirements. Our
experiments show that by grounding physical safety in temporal logic and
applying verification methods across multiple levels, Sentinel provides a
rigorous foundation for systematically evaluating LLM-based embodied agents in
physical environments, exposing safety violations overlooked by previous
methods and offering insights into their failure modes.

</details>


### [4] [From Narratives to Probabilistic Reasoning: Predicting and Interpreting Drivers' Hazardous Actions in Crashes Using Large Language Model](https://arxiv.org/abs/2510.13002)
*Boyou Chen,Gerui Xu,Zifei Wang,Huizhong Guo,Ananna Ahmed,Zhaonan Sun,Zhen Hu,Kaihan Zhang,Shan Bao*

Main category: cs.AI

TL;DR: 提出基于微调大语言模型的框架，自动从交通事故文本叙述中推断驾驶员危险行为，提高分类的有效性和可解释性，在二车事故数据上达到80%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据库中驾驶员危险行为数据可靠性问题，传统人工编码方法不一致且劳动密集，需要自动化的解决方案。

Method: 使用MTCF五年二车事故数据，微调Llama 3.2 1B模型处理详细事故叙述，并与随机森林、XGBoost、CatBoost和神经网络等传统机器学习分类器进行基准比较。

Result: 微调LLM达到80%整体准确率，优于所有基线模型，在数据不平衡场景中表现尤其突出，并通过概率推理方法分析驾驶员分心和年龄等因素的影响。

Conclusion: 该框架为大规模自动DHA检测提供了稳健且可解释的解决方案，为交通安全分析和干预开辟了新机会。

Abstract: Vehicle crashes involve complex interactions between road users, split-second
decisions, and challenging environmental conditions. Among these, two-vehicle
crashes are the most prevalent, accounting for approximately 70% of roadway
crashes and posing a significant challenge to traffic safety. Identifying
Driver Hazardous Action (DHA) is essential for understanding crash causation,
yet the reliability of DHA data in large-scale databases is limited by
inconsistent and labor-intensive manual coding practices. Here, we present an
innovative framework that leverages a fine-tuned large language model to
automatically infer DHAs from textual crash narratives, thereby improving the
validity and interpretability of DHA classifications. Using five years of
two-vehicle crash data from MTCF, we fine-tuned the Llama 3.2 1B model on
detailed crash narratives and benchmarked its performance against conventional
machine learning classifiers, including Random Forest, XGBoost, CatBoost, and a
neural network. The fine-tuned LLM achieved an overall accuracy of 80%,
surpassing all baseline models and demonstrating pronounced improvements in
scenarios with imbalanced data. To increase interpretability, we developed a
probabilistic reasoning approach, analyzing model output shifts across original
test sets and three targeted counterfactual scenarios: variations in driver
distraction and age. Our analysis revealed that introducing distraction for one
driver substantially increased the likelihood of "General Unsafe Driving";
distraction for both drivers maximized the probability of "Both Drivers Took
Hazardous Actions"; and assigning a teen driver markedly elevated the
probability of "Speed and Stopping Violations." Our framework and analytical
methods provide a robust and interpretable solution for large-scale automated
DHA detection, offering new opportunities for traffic safety analysis and
intervention.

</details>


### [5] [Toward Reasoning-Centric Time-Series Analysis](https://arxiv.org/abs/2510.13029)
*Xinlei Wang,Mingtian Tan,Jing Qiu,Junhua Zhao,Jinjin Gu*

Main category: cs.AI

TL;DR: 本文主张将时间序列分析重新构想为推理任务，利用LLMs的深层推理能力而非数值回归，强调因果结构和可解释性，以实现更贴近人类理解的透明分析。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分析依赖静态基准，但现实世界存在政策变化、人类行为适应等动态因素。现有LLM方法多使用数值回归能力，忽视了其深层推理潜力，需要重新思考如何有效利用LLMs进行时间序列分析。

Method: 将时间序列分析重新定义为推理任务，利用LLMs的多模态输入整合能力，强调因果结构和可解释性分析，而非传统的数值回归方法。

Result: 该方法使时间序列分析更贴近人类对齐的理解，能够在复杂现实环境中提供透明和上下文感知的洞察。

Conclusion: 将LLMs作为推理工具而非回归工具，能够更好地揭示时间序列背后的驱动力量，实现更有效、可解释的时间序列分析。

Abstract: Traditional time series analysis has long relied on pattern recognition,
trained on static and well-established benchmarks. However, in real-world
settings -- where policies shift, human behavior adapts, and unexpected events
unfold -- effective analysis must go beyond surface-level trends to uncover the
actual forces driving them. The recent rise of Large Language Models (LLMs)
presents new opportunities for rethinking time series analysis by integrating
multimodal inputs. However, as the use of LLMs becomes popular, we must remain
cautious, asking why we use LLMs and how to exploit them effectively. Most
existing LLM-based methods still employ their numerical regression ability and
ignore their deeper reasoning potential. This paper argues for rethinking time
series with LLMs as a reasoning task that prioritizes causal structure and
explainability. This shift brings time series analysis closer to human-aligned
understanding, enabling transparent and context-aware insights in complex
real-world environments.

</details>


### [6] [Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking](https://arxiv.org/abs/2510.13036)
*Stephane Hatgis-Kessell,Logan Mondal Bhamidipaty,Emma Brunskill*

Main category: cs.AI

TL;DR: 提出了Preference-Based Reward Repair (PBRR)框架，通过从人类偏好中学习加性、状态转移相关的修正项来自动修复人工设计的代理奖励函数，解决奖励函数错配问题。


<details>
  <summary>Details</summary>
Motivation: 人工设计的奖励函数经常与人类真实目标不一致，导致策略错配；而从人类反馈中学习奖励函数成本高昂。需要一种结合两者优势的方法。

Method: PBRR使用目标探索策略和新的偏好学习目标，在少量关键状态转移上学习修正项，而不是从头学习整个奖励函数。

Result: 在表格域中证明PBRR的累积遗憾与现有偏好强化学习方法相当；在奖励黑客基准测试中，PBRR始终优于基线方法，学习高性能策略所需偏好数量显著减少。

Conclusion: PBRR通过修正代理奖励函数而非完全替换，能以更少的人类偏好学习到高性能策略，有效解决奖励错配问题。

Abstract: Human-designed reward functions for reinforcement learning (RL) agents are
frequently misaligned with the humans' true, unobservable objectives, and thus
act only as proxies. Optimizing for a misspecified proxy reward function often
induces reward hacking, resulting in a policy misaligned with the human's true
objectives. An alternative is to perform RL from human feedback, which involves
learning a reward function from scratch by collecting human preferences over
pairs of trajectories. However, building such datasets is costly. To address
the limitations of both approaches, we propose Preference-Based Reward Repair
(PBRR): an automated iterative framework that repairs a human-specified proxy
reward function by learning an additive, transition-dependent correction term
from preferences. A manually specified reward function can yield policies that
are highly suboptimal under the ground-truth objective, yet corrections on only
a few transitions may suffice to recover optimal performance. To identify and
correct for those transitions, PBRR uses a targeted exploration strategy and a
new preference-learning objective. We prove in tabular domains PBRR has a
cumulative regret that matches, up to constants, that of prior preference-based
RL methods. In addition, on a suite of reward-hacking benchmarks, PBRR
consistently outperforms baselines that learn a reward function from scratch
from preferences or modify the proxy reward function using other approaches,
requiring substantially fewer preferences to learn high performing policies.

</details>


### [7] [Emotional Cognitive Modeling Framework with Desire-Driven Objective Optimization for LLM-empowered Agent in Social Simulation](https://arxiv.org/abs/2510.13195)
*Qun Ma,Xiao Xue,Xuwen Zhang,Zihan Zhao,Yuwei Guo,Ming Zhang*

Main category: cs.AI

TL;DR: 提出了一个情感认知框架，通过欲望生成和目标管理实现LLM智能体与人类的情感对齐，显著提升了智能体在模拟人类行为时的生态效度。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体在情感认知方面存在严重局限：无法模拟连接虚拟与现实服务的有限理性；缺乏经过实证验证的情感整合机制嵌入智能体决策架构。

Method: 构建了包含欲望生成和目标管理的情感认知框架，模拟LLM智能体的完整决策过程，包括状态演化、欲望生成、目标优化、决策生成和行动执行。

Result: 实验表明，采用该框架的智能体不仅表现出与其情感状态一致的行为，而且在与其他类型智能体的比较评估中，展现出更高的生态效度，其决策结果显著更接近人类行为模式。

Conclusion: 该情感认知框架成功实现了LLM智能体与人类的情感对齐，为构建更真实的虚拟人类模拟提供了有效解决方案。

Abstract: The advent of large language models (LLMs) has enabled agents to represent
virtual humans in societal simulations, facilitating diverse interactions
within complex social systems. However, existing LLM-based agents exhibit
severe limitations in affective cognition: They fail to simulate the bounded
rationality essential for bridging virtual and real-world services; They lack
empirically validated integration mechanisms embedding emotions within agent
decision architectures. This paper constructs an emotional cognition framework
incorporating desire generation and objective management, designed to achieve
emotion alignment between LLM-based agents and humans, modeling the complete
decision-making process of LLM-based agents, encompassing state evolution,
desire generation, objective optimization, decision generation, and action
execution. This study implements the proposed framework within our proprietary
multi-agent interaction environment. Experimental results demonstrate that
agents governed by our framework not only exhibit behaviors congruent with
their emotional states but also, in comparative assessments against other agent
types, demonstrate superior ecological validity and generate decision outcomes
that significantly more closely approximate human behavioral patterns.

</details>


### [8] [Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning](https://arxiv.org/abs/2510.13214)
*Zehui Ling,Deshu Chen,Yichi Zhang,Yuchen Liu,Xigui Li,Xin Guo,Yuan Cheng*

Main category: cs.AI

TL;DR: 提出了一种结合大小语言模型的互补代理系统，通过小模型生成初始答案，大模型验证，仅在必要时进行深度推理，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 深度推理虽然能提升复杂任务性能，但对所有问题都应用计算成本过高，需要更高效的推理策略。

Method: 使用小LLM生成初始答案，大LLM验证正确性，仅在答案错误时进行深度推理。

Result: 简单问题上大模型计算成本降低50%以上，准确率损失可忽略，复杂任务性能保持稳定。

Conclusion: 该互补代理系统能有效平衡计算效率与推理准确性，为LLM应用提供实用解决方案。

Abstract: Recent advances in Large Language Models (LLMs) demonstrate that
chain-of-thought prompting and deep reasoning substantially enhance performance
on complex tasks, and multi-agent systems can further improve accuracy by
enabling model debates. However, applying deep reasoning to all problems is
computationally expensive. To mitigate these costs, we propose a complementary
agent system integrating small and large LLMs. The small LLM first generates an
initial answer, which is then verified by the large LLM. If correct, the answer
is adopted directly; otherwise, the large LLM performs in-depth reasoning.
Experimental results show that, for simple problems, our approach reduces the
computational cost of the large LLM by more than 50% with negligible accuracy
loss, while consistently maintaining robust performance on complex tasks.

</details>


### [9] [Personalized Learning Path Planning with Goal-Driven Learner State Modeling](https://arxiv.org/abs/2510.13215)
*Joy Jia Yin Lim,Ye He,Jifan Yu,Xin Cong,Daniel Zhang-Li,Zhiyuan Liu,Huiqin Liu,Lei Hou,Juanzi Li,Bin Xu*

Main category: cs.AI

TL;DR: Pxplore是一个个性化学习路径规划框架，结合强化学习和LLM教育架构，通过结构化学习者状态模型和自动奖励函数实现目标对齐的学习路径生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的个性化学习方法缺乏目标对齐的规划机制，无法有效设计符合个人目标的自适应学习路径。

Method: 设计结构化学习者状态模型和自动奖励函数，结合监督微调(SFT)和组相对策略优化(GRPO)训练策略，并在真实学习平台中部署。

Result: 大量实验验证了Pxplore在生成连贯、个性化和目标驱动的学习路径方面的有效性。

Conclusion: Pxplore框架成功解决了目标对齐的个性化学习路径规划问题，并发布了代码和数据集以促进未来研究。

Abstract: Personalized Learning Path Planning (PLPP) aims to design adaptive learning
paths that align with individual goals. While large language models (LLMs) show
potential in personalizing learning experiences, existing approaches often lack
mechanisms for goal-aligned planning. We introduce Pxplore, a novel framework
for PLPP that integrates a reinforcement-based training paradigm and an
LLM-driven educational architecture. We design a structured learner state model
and an automated reward function that transforms abstract objectives into
computable signals. We train the policy combining supervised fine-tuning (SFT)
and Group Relative Policy Optimization (GRPO), and deploy it within a
real-world learning platform. Extensive experiments validate Pxplore's
effectiveness in producing coherent, personalized, and goal-driven learning
paths. We release our code and dataset to facilitate future research.

</details>


### [10] [EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems](https://arxiv.org/abs/2510.13220)
*Yufei He,Juncheng Liu,Yue Liu,Yibo Li,Tri Cao,Zhiyuan Hu,Xinxing Xu,Bryan Hooi*

Main category: cs.AI

TL;DR: 提出了J-TTL基准测试来衡量AI代理在测试时的学习能力，并开发了EvoTest进化框架，通过进化整个代理系统来提升性能，无需微调或梯度计算。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理无法在测试时学习复杂技能，在陌生环境中表现不佳，这限制了其实际应用价值。

Method: EvoTest进化测试时学习框架，包含执行游戏的Actor代理和分析游戏记录以提出改进配置的Evolver代理，通过重写提示、更新记忆、调整超参数和学习工具使用例程来优化系统。

Result: 在J-TTL基准测试中，EvoTest持续提升性能，优于反射、仅记忆基线和更复杂的在线微调方法，是唯一能赢得两个游戏的方法。

Conclusion: EvoTest框架有效解决了AI代理测试时学习能力不足的问题，为开发更实用的AI代理提供了新方向。

Abstract: A fundamental limitation of current AI agents is their inability to learn
complex skills on the fly at test time, often behaving like "clever but
clueless interns" in novel environments. This severely limits their practical
utility. To systematically measure and drive progress on this challenge, we
first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a
new evaluation setup where an agent must play the same game for several
consecutive episodes, attempting to improve its performance from one episode to
the next. On J-TTL, we find that existing adaptation methods like reflection,
memory, or reinforcement learning struggle. To address the challenges posed by
our benchmark, we present EvoTest, an evolutionary test-time learning framework
that improves an agent without any fine-tuning or gradients-by evolving the
entire agentic system after every episode. EvoTest has two roles: the Actor
Agent, which plays the game, and the Evolver Agent, which analyzes the episode
transcript to propose a revised configuration for the next run. This
configuration rewrites the prompt, updates memory by logging effective
state-action choices, tunes hyperparameters, and learns the tool-use routines.
On our J-TTL benchmark, EvoTest consistently increases performance,
outperforming not only reflection and memory-only baselines but also more
complex online fine-tuning methods. Notably, our method is the only one capable
of winning two games (Detective and Library), while all baselines fail to win
any.

</details>


### [11] [An Analytical Framework to Enhance Autonomous Vehicle Perception for Smart Cities](https://arxiv.org/abs/2510.13230)
*Jalal Khan,Manzoor Khan,Sherzod Turaev,Sumbal Malik,Hesham El-Sayed,Farman Ullah*

Main category: cs.AI

TL;DR: 提出了一个基于效用的分析模型，用于评估自动驾驶车辆的感知系统性能，通过YOLOv8s模型进行目标检测，并使用nuScense数据集进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要准确感知道路上的多个物体并预测驾驶员感知，以控制车辆运动，需要开发能够理解驾驶环境的感知系统。

Method: 使用自定义数据集（包含摩托车手、三轮车等），采用YOLOv8s深度学习模型进行目标检测，并基于训练模型实例的性能值测量感知服务的效用。

Result: 实验结果显示三个最佳性能的YOLOv8s实例：SGD-based (mAP@0.5=0.832)、Adam-based (0.810)和AdamW-based (0.822)。虽然SGD的总体mAP最高，但AdamW在类别级别性能更好（如汽车：0.921，摩托车手：0.899）。

Conclusion: 提出的感知模型能够有效评估学习模型的效用，并为自动驾驶车辆确定合适的感知方案，验证了该函数能够找到正确的自动驾驶感知。

Abstract: The driving environment perception has a vital role for autonomous driving
and nowadays has been actively explored for its realization. The research
community and relevant stakeholders necessitate the development of Deep
Learning (DL) models and AI-enabled solutions to enhance autonomous vehicles
(AVs) for smart mobility. There is a need to develop a model that accurately
perceives multiple objects on the road and predicts the driver's perception to
control the car's movements. This article proposes a novel utility-based
analytical model that enables perception systems of AVs to understand the
driving environment. The article consists of modules: acquiring a custom
dataset having distinctive objects, i.e., motorcyclists, rickshaws, etc; a
DL-based model (YOLOv8s) for object detection; and a module to measure the
utility of perception service from the performance values of trained model
instances. The perception model is validated based on the object detection
task, and its process is benchmarked by state-of-the-art deep learning models'
performance metrics from the nuScense dataset. The experimental results show
three best-performing YOLOv8s instances based on mAP@0.5 values, i.e.,
SGD-based (0.832), Adam-based (0.810), and AdamW-based (0.822). However, the
AdamW-based model (i.e., car: 0.921, motorcyclist: 0.899, truck: 0.793, etc.)
still outperforms the SGD-based model (i.e., car: 0.915, motorcyclist: 0.892,
truck: 0.781, etc.) because it has better class-level performance values,
confirmed by the proposed perception model. We validate that the proposed
function is capable of finding the right perception for AVs. The results above
encourage using the proposed perception model to evaluate the utility of
learning models and determine the appropriate perception for AVs.

</details>


### [12] [SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2510.13262)
*Weiqi Guo,Guanjun Liu,Ziyuan Zhou*

Main category: cs.AI

TL;DR: 提出SAJA框架，联合攻击状态和动作，利用梯度上升方法计算对抗性状态和动作，在MPE环境中验证其优于单一攻击且现有防御方法无效


<details>
  <summary>Details</summary>
Motivation: 现有研究只关注状态或动作攻击，未考虑如何有效联合两者。简单组合状态和动作扰动无法发挥协同效应，需要研究联合攻击方法

Method: SAJA框架包含两个阶段：状态攻击阶段使用actor和critic网络计算对抗性状态；动作攻击阶段基于扰动状态使用critic网络计算最终对抗性动作，并添加启发式正则化器

Result: 在MPE环境中验证，SAJA优于单一状态或动作攻击，更具隐蔽性，且现有防御方法无法防御其攻击

Conclusion: SAJA框架有效实现了状态和动作的联合攻击，具有良好协同效应，为MADRL模型的安全性评估提供了新视角

Abstract: Multi-Agent Deep Reinforcement Learning (MADRL) has shown potential for
cooperative and competitive tasks such as autonomous driving and strategic
gaming. However, models trained by MADRL are vulnerable to adversarial
perturbations on states and actions. Therefore, it is essential to investigate
the robustness of MADRL models from an attack perspective. Existing studies
focus on either state-only attacks or action-only attacks, but do not consider
how to effectively joint them. Simply combining state and action perturbations
such as randomly perturbing states and actions does not exploit their potential
synergistic effects. In this paper, we propose the State-Action Joint Attack
(SAJA) framework that has a good synergistic effects. SAJA consists of two
important phases: (1) In the state attack phase, a multi-step gradient ascent
method utilizes both the actor network and the critic network to compute an
adversarial state, and (2) in the action attack phase, based on the perturbed
state, a second gradient ascent uses the critic network to craft the final
adversarial action. Additionally, a heuristic regularizer measuring the
distance between the perturbed actions and the original clean ones is added
into the loss function to enhance the effectiveness of the critic's guidance.
We evaluate SAJA in the Multi-Agent Particle Environment (MPE), demonstrating
that (1) it outperforms and is more stealthy than state-only or action-only
attacks, and (2) existing state or action defense methods cannot defend its
attacks.

</details>


### [13] [Learnable Game-theoretic Policy Optimization for Data-centric Self-explanation Rationalization](https://arxiv.org/abs/2510.13393)
*Yunxiao Zhao,Zhiqiang Wang,Xingtong Yu,Xiaoli Li,Jiye Liang,Ru Li*

Main category: cs.AI

TL;DR: 本文提出PORAT方法解决理性化框架中的模式崩溃问题，通过博弈论视角分析并引入策略干预来优化合作博弈过程，在多个数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统理性化方法通过正则化约束来校准生成过程，但存在模式崩溃问题，即预测器能正确预测但生成器持续输出崩溃模式的理性。现有研究缺乏统一考虑，本文从博弈论角度系统分析该问题。

Method: 提出PORAT方法，从博弈论视角重新审视合作理性化，识别模式崩溃的根本原因，并通过逐步引入策略干预来解决合作博弈过程中的均衡问题，引导模型达到更优解状态。

Result: 在9个真实世界数据集和2个合成设置上验证，PORAT相比现有最先进方法实现了最高8.1%的性能提升。

Conclusion: 从博弈论角度分析理性化中的模式崩溃问题具有理论可行性，PORAT方法通过策略干预有效解决了该问题，为构建更好的自解释模型提供了新思路。

Abstract: Rationalization, a data-centric framework, aims to build self-explanatory
models to explain the prediction outcome by generating a subset of
human-intelligible pieces of the input data. It involves a cooperative game
model where a generator generates the most human-intelligible parts of the
input (i.e., rationales), followed by a predictor that makes predictions based
on these generated rationales. Conventional rationalization methods typically
impose constraints via regularization terms to calibrate or penalize undesired
generation. However, these methods are suffering from a problem called mode
collapse, in which the predictor produces correct predictions yet the generator
consistently outputs rationales with collapsed patterns. Moreover, existing
studies are typically designed separately for specific collapsed patterns,
lacking a unified consideration. In this paper, we systematically revisit
cooperative rationalization from a novel game-theoretic perspective and
identify the fundamental cause of this problem: the generator no longer tends
to explore new strategies to uncover informative rationales, ultimately leading
the system to converge to a suboptimal game equilibrium (correct predictions
v.s collapsed rationales). To solve this problem, we then propose a novel
approach, Game-theoretic Policy Optimization oriented RATionalization (PORAT),
which progressively introduces policy interventions to address the game
equilibrium in the cooperative game process, thereby guiding the model toward a
more optimal solution state. We theoretically analyse the cause of such a
suboptimal equilibrium and prove the feasibility of the proposed method.
Furthermore, we validate our method on nine widely used real-world datasets and
two synthetic settings, where PORAT achieves up to 8.1% performance
improvements over existing state-of-the-art methods.

</details>


### [14] [Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse](https://arxiv.org/abs/2510.13417)
*Liesbeth Allein,Nataly Pineda-Castañeda,Andrea Rocci,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 该研究评估了大型语言模型在隐式因果链发现任务中的机制性因果推理能力，发现LLMs主要依赖关联模式匹配而非真正的因果推理，但生成的因果链在逻辑上具有连贯性。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型如何理解因果关系中的中间因果步骤，以及它们是否能够进行机制性因果推理，特别是在气候变化等争议性话题的论证环境中。

Method: 使用诊断评估框架，让9个LLMs为给定的因果对生成所有可能的中间因果步骤，这些因果对来自气候变化论证研究资源。

Result: LLMs在生成的因果步骤数量和粒度上存在差异，虽然它们对中间因果连接表现出自信和一致性，但主要基于关联模式匹配而非真正的因果推理。人类评估确认了生成因果链的逻辑连贯性。

Conclusion: 研究为推进论证环境中隐式机制性因果推理的未来工作提供了基线方法、诊断评估见解和基准数据集。

Abstract: How does a cause lead to an effect, and which intermediate causal steps
explain their connection? This work scrutinizes the mechanistic causal
reasoning capabilities of large language models (LLMs) to answer these
questions through the task of implicit causal chain discovery. In a diagnostic
evaluation framework, we instruct nine LLMs to generate all possible
intermediate causal steps linking given cause-effect pairs in causal chain
structures. These pairs are drawn from recent resources in argumentation
studies featuring polarized discussion on climate change. Our analysis reveals
that LLMs vary in the number and granularity of causal steps they produce.
Although they are generally self-consistent and confident about the
intermediate causal connections in the generated chains, their judgments are
mainly driven by associative pattern matching rather than genuine causal
reasoning. Nonetheless, human evaluations confirmed the logical coherence and
integrity of the generated chains. Our baseline causal chain discovery
approach, insights from our diagnostic evaluation, and benchmark dataset with
causal chains lay a solid foundation for advancing future work in implicit,
mechanistic causal reasoning in argumentation settings.

</details>


### [15] [Mobile Coverage Analysis using Crowdsourced Data](https://arxiv.org/abs/2510.13459)
*Timothy Wong,Tom Freeman,Joseph Feehily*

Main category: cs.AI

TL;DR: 提出了一种利用众包QoE数据评估移动网络覆盖和识别服务弱点的框架，采用OC-SVM算法建模覆盖边界，能够准确绘制移动覆盖图并识别信号缺陷区域。


<details>
  <summary>Details</summary>
Motivation: 移动网络覆盖的有效评估和服务弱点的精确识别对于网络运营商提升用户体验质量(QoE)至关重要。

Method: 在单个小区层面进行覆盖分析，然后聚合到站点层面，使用经验地理定位数据。核心是应用OC-SVM算法计算移动网络覆盖，将决策超平面建模为有效覆盖轮廓。

Result: 该框架在准确绘制移动覆盖图方面表现出高效性，特别是在复杂城市环境中能够突出显示细粒度的信号缺陷区域。

Conclusion: 该新颖框架能够有效分析移动网络覆盖并识别服务弱点，为网络优化提供了有力工具。

Abstract: Effective assessment of mobile network coverage and the precise
identification of service weak spots are paramount for network operators
striving to enhance user Quality of Experience (QoE). This paper presents a
novel framework for mobile coverage and weak spot analysis utilising
crowdsourced QoE data. The core of our methodology involves coverage analysis
at the individual cell (antenna) level, subsequently aggregated to the site
level, using empirical geolocation data. A key contribution of this research is
the application of One-Class Support Vector Machine (OC-SVM) algorithm for
calculating mobile network coverage. This approach models the decision
hyperplane as the effective coverage contour, facilitating robust calculation
of coverage areas for individual cells and entire sites. The same methodology
is extended to analyse crowdsourced service loss reports, thereby identifying
and quantifying geographically localised weak spots. Our findings demonstrate
the efficacy of this novel framework in accurately mapping mobile coverage and,
crucially, in highlighting granular areas of signal deficiency, particularly
within complex urban environments.

</details>


### [16] [Confidence as a Reward: Transforming LLMs into Reward Models](https://arxiv.org/abs/2510.13501)
*He Du,Bowen Li,Chengxing Xie,Chang Gao,Kai Chen,Dacheng Tao*

Main category: cs.AI

TL;DR: 提出了Confidence-as-a-Reward (CRew)方法，利用模型对最终答案的token级置信度作为奖励信号，无需训练即可在数学推理任务中超越现有训练免费方法，甚至优于大多数训练过的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型需要大量标注数据和昂贵训练，而训练免费方法如LLM-as-a-Judge虽然有效但未系统研究置信度作为奖励的潜力。

Method: 使用模型对最终答案的token级置信度作为奖励代理，特别适用于封闭式任务。提出了CRew-DPO训练策略，结合置信度得分和正确性信号构建偏好数据。

Result: 在MATH500和RewardMATH基准测试中，CRew优于现有训练免费奖励方法，甚至超过大多数训练过的奖励模型。CRew得分与模型实际推理性能强相关，并能有效筛选高质量训练数据。

Conclusion: 置信度作为奖励是一种简单而强大的训练免费方法，CRew-DPO进一步提升了模型的判断能力，持续优于现有自训练方法。

Abstract: Reward models can significantly enhance the reasoning capabilities of large
language models (LLMs), but they typically require extensive curated data and
costly training. To mitigate these challenges, training-free approaches such as
LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate
responses, achieving promising results. Recent works have also indicated that
model confidence can serve effectively as a reward metric, distinguishing
between chain-of-thought (CoT) and non-CoT paths. However, the concept of using
confidence as a reward has not been comprehensively studied. In this work, we
systematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful
training-free method that utilizes token-level confidence in the model's final
answers as a proxy for reward, especially suitable for close-ended tasks.
Through extensive experiments on mathematical reasoning tasks, we demonstrate
that CRew outperforms existing training-free reward approaches on the MATH500
and RewardMATH benchmarks, and even surpasses most trained reward models. We
further identify a strong correlation between CRew scores and the actual
reasoning performance of the model. Additionally, we find that CRew can
effectively filter high-quality training data. Building upon these insights, we
propose CRew-DPO, a training strategy that constructs preference data from
confidence scores combined with correctness signals. Finetuning with CRew-DPO
further enhances the model's judging capabilities and consistently outperforms
existing self-training methods.

</details>


### [17] [A Methodology for Assessing the Risk of Metric Failure in LLMs Within the Financial Domain](https://arxiv.org/abs/2510.13524)
*William Flanagan,Mukunda Das,Rajitha Ramanyake,Swaunja Maslekar,Meghana Manipuri,Joong Ho Choi,Shruti Nair,Shambhavi Bhusan,Sanjana Dulam,Mouni Pendharkar,Nidhi Singh,Vashisth Doshi,Sachi Shah Paresh*

Main category: cs.AI

TL;DR: 本文提出了一个风险评估框架，用于解决生成式AI在金融服务行业应用中的性能评估挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在金融服务行业应用时面临性能评估困难，传统机器学习指标无法很好泛化到GenAI工作负载，而专家评估也存在局限性。

Method: 开发了一个风险评估框架，结合领域专家评估和机器学习指标，以更好地评估生成式AI模型性能。

Result: 该框架能够更有效地应用专家评估和机器学习指标，解决行业特定需求。

Conclusion: 提出的风险评估框架有助于金融服务行业更好地采用生成式AI技术，解决性能评估的挑战。

Abstract: As Generative Artificial Intelligence is adopted across the financial
services industry, a significant barrier to adoption and usage is measuring
model performance. Historical machine learning metrics can oftentimes fail to
generalize to GenAI workloads and are often supplemented using Subject Matter
Expert (SME) Evaluation. Even in this combination, many projects fail to
account for various unique risks present in choosing specific metrics.
Additionally, many widespread benchmarks created by foundational research labs
and educational institutions fail to generalize to industrial use. This paper
explains these challenges and provides a Risk Assessment Framework to allow for
better application of SME and machine learning Metrics

</details>


### [18] [Tandem Training for Language Models](https://arxiv.org/abs/2510.13551)
*Robert West,Ashton Anderson,Ece Kamar,Eric Horvitz*

Main category: cs.AI

TL;DR: 提出了串联训练方法，通过随机将控制权交给较弱模型来确保强模型的解决方案对弱模型可理解，同时保持任务准确性


<details>
  <summary>Details</summary>
Motivation: 随着语言模型快速发展，其推理过程可能超出较弱代理和人类的理解范围，影响可解释性和监督。需要开发方法使模型产生对较弱合作者仍可理解的解决方案

Method: 引入串联训练，一种强化学习范式，在训练强模型时随机从冻结的弱模型采样token，确保强模型的推理过程能够被弱模型继续完成

Result: 在GSM8K数学推理任务中，串联训练可靠地教会模型放弃专业术语，适应较弱合作伙伴的语言，同时保持高任务准确率

Conclusion: 串联训练为构建可被较弱代理审计的AI系统提供了一条有前景的路径，对人与AI协作和多智能体通信具有重要意义

Abstract: As language models continue to rapidly improve, we can expect their actions
and reasoning to become difficult or impossible for weaker agents and humans to
follow, undermining interpretability and oversight. With an eye on long-term
futures, we pursue methods that encourage models to produce solutions that
remain intelligible to weaker collaborators. We formalize intelligibility as
handoff robustness: a strong model's solution is intelligible to a weaker model
if randomly handing off control to the weaker model along the solution path
does not cause failure. Building on this criterion, we introduce tandem
training for language models, a reinforcement learning (RL) paradigm in which
rollout tokens are intermittently and randomly sampled from a frozen weak model
rather than the strong model being trained. Because rollouts succeed only when
the strong model's actions and reasoning process can be continued by the weak
model -- when the two can co-construct a successful solution -- optimizing
standard RL objectives with tandem training implicitly incentivizes both
correctness and intelligibility. In the GSM8K math reasoning task, tandem
training reliably teaches models to abandon jargon and adapt their language to
weaker partners while keeping task accuracy high. Our results demonstrate a
promising route to building AI systems that remain auditable by weaker agents,
with implications for human--AI collaboration and multi-agent communication.

</details>


### [19] [A Modal Logic for Temporal and Jurisdictional Classifier Models](https://arxiv.org/abs/2510.13691)
*Cecilia Di Florio,Huimin Dong,Antonino Rotolo*

Main category: cs.AI

TL;DR: 提出一种用于形式化法律案例推理的分类器模态逻辑，结合时间维度和法院层级来解决先例冲突


<details>
  <summary>Details</summary>
Motivation: 基于逻辑的模型可用于构建法律领域机器学习分类器的验证工具，这些分类器通过案例推理预测新案件结果

Method: 引入分类器模态逻辑，整合案件时间维度和法院系统层级来解决先例之间的冲突

Result: 开发了一个能够正式捕捉法律案例推理的逻辑框架

Conclusion: 该逻辑框架为验证法律机器学习分类器提供了形式化基础

Abstract: Logic-based models can be used to build verification tools for machine
learning classifiers employed in the legal field. ML classifiers predict the
outcomes of new cases based on previous ones, thereby performing a form of
case-based reasoning (CBR). In this paper, we introduce a modal logic of
classifiers designed to formally capture legal CBR. We incorporate principles
for resolving conflicts between precedents, by introducing into the logic the
temporal dimension of cases and the hierarchy of courts within the legal
system.

</details>


### [20] [Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)
*Evan Ellis,Vivek Myers,Jens Tuyls,Sergey Levine,Anca Dragan,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: 提出了一种基于最大化人类赋权（empowerment）的辅助语言模型调优方法Empower，该方法仅需离线文本数据，无需额外人工反馈或可验证奖励，就能训练出更好地辅助人类的AI助手。


<details>
  <summary>Details</summary>
Motivation: 当前构建辅助代理的方法（模仿专家人类或基于推断奖励的RL微调）往往鼓励代理独立完成任务而非真正辅助人类达成目标，且需要昂贵的人工反馈。

Method: 提出Empower方法，通过最大化人类在环境中的赋权能力来微调语言模型，仅使用离线文本数据进行自监督训练。

Result: 在18人用户研究中，参与者78%的时间偏好Empower助手（p=0.015），接受率提高31%，建议减少38%。在模拟编程环境中，Empower将模拟程序员在挑战性编程问题上的成功率平均提高192%。

Conclusion: Empower提供了一个仅使用离线数据、无需额外人工反馈或可验证奖励的框架，用于规模化构建有用且对齐的AI助手。

Abstract: Assistive agents should not only take actions on behalf of a human, but also
step out of the way and cede control when there are important decisions to be
made. However, current methods for building assistive agents, whether via
mimicking expert humans or via RL finetuning on an inferred reward, often
encourage agents to complete tasks on their own rather than truly assisting the
human attain their objectives. Additionally, these methods often require costly
explicit human feedback to provide a training signal. We propose a new approach
to tuning assistive language models based on maximizing the human's
empowerment, their ability to effect desired changes in the environment. Our
empowerment-maximizing method, Empower, only requires offline text data,
providing a self-supervised method for fine-tuning language models to better
assist humans. To study the efficacy of our approach, we conducted an 18-person
user study comparing our empowerment assistant with a strong baseline.
Participants preferred our assistant 78% of the time (p=0.015), with a 31%
higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a
new environment for evaluating multi-turn code assistance using simulated
humans. Using this environment, we show that agents trained with Empower
increase the success rate of a simulated human programmer on challenging coding
questions by an average of 192% over an SFT baseline. With this empowerment
objective, we provide a framework for useful aligned AI agents at scale using
only offline data without the need for any additional human feedback or
verifiable rewards.

</details>


### [21] [From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails](https://arxiv.org/abs/2510.13727)
*Ravi Pandya,Madison Bland,Duy P. Nguyen,Changliu Liu,Jaime Fernández Fisac,Andrea Bajcsy*

Main category: cs.AI

TL;DR: 本文提出了一种基于控制理论的安全护栏方法，用于预防AI代理在交互过程中可能造成的下游危害，如财务或物理伤害。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全护栏主要依赖输出分类和人工指定标准，对新危险情况脆弱，且检测到不安全时通常只是拒绝行动，这并不总是安全的选择。

Method: 将AI代理安全视为序列决策问题，基于安全关键控制理论在AI模型的潜在表示中构建预测性护栏，实时监控并主动修正风险输出。

Result: 在模拟驾驶和电子商务环境中，控制理论护栏能可靠地引导LLM代理避免灾难性结果（从碰撞到破产），同时保持任务性能。

Conclusion: 控制理论护栏为当前标记-阻止式护栏提供了原则性的动态替代方案，能够以模型无关的方式保护任何AI模型。

Abstract: Generative AI systems are increasingly assisting and acting on behalf of end
users in practical settings, from digital shopping assistants to
next-generation autonomous cars. In this context, safety is no longer about
blocking harmful content, but about preempting downstream hazards like
financial or physical harm. Yet, most AI guardrails continue to rely on output
classification based on labeled datasets and human-specified criteria,making
them brittle to new hazardous situations. Even when unsafe conditions are
flagged, this detection offers no path to recovery: typically, the AI system
simply refuses to act--which is not always a safe choice. In this work, we
argue that agentic AI safety is fundamentally a sequential decision problem:
harmful outcomes arise from the AI system's continually evolving interactions
and their downstream consequences on the world. We formalize this through the
lens of safety-critical control theory, but within the AI model's latent
representation of the world. This enables us to build predictive guardrails
that (i) monitor an AI system's outputs (actions) in real time and (ii)
proactively correct risky outputs to safe ones, all in a model-agnostic manner
so the same guardrail can be wrapped around any AI model. We also offer a
practical training recipe for computing such guardrails at scale via
safety-critical reinforcement learning. Our experiments in simulated driving
and e-commerce settings demonstrate that control-theoretic guardrails can
reliably steer LLM agents clear of catastrophic outcomes (from collisions to
bankruptcy) while preserving task performance, offering a principled dynamic
alternative to today's flag-and-block guardrails.

</details>


### [22] [Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math](https://arxiv.org/abs/2510.13744)
*Shrey Pandit,Austin Xu,Xuan-Phi Nguyen,Yifei Ming,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: Hard2Verify是一个人工标注的步骤级验证基准，用于评估前沿LLM在数学推理中的错误检测能力，发现开源验证器普遍落后于闭源模型。


<details>
  <summary>Details</summary>
Motivation: 在IMO 2025等需要严格数学证明的竞赛中，LLM推理系统需要强大的验证器来捕捉步骤级错误，但目前缺乏合适的评估基准。

Method: 通过500多小时人工标注构建Hard2Verify基准，评估29个生成式批评器和过程奖励模型在步骤级验证任务上的表现。

Result: 除了少数表现优异者外，开源验证器在步骤级验证任务上普遍落后于闭源模型。

Conclusion: 步骤级验证是LLM推理系统的关键瓶颈，需要进一步研究验证器计算规模、自验证机制以及验证与生成的动态关系。

Abstract: Large language model (LLM)-based reasoning systems have recently achieved
gold medal-level performance in the IMO 2025 competition, writing mathematical
proofs where, to receive full credit, each step must be not only correct but
also sufficiently supported. To train LLM-based reasoners in such challenging,
open-ended settings, strong verifiers capable of catching step-level mistakes
are necessary prerequisites. We introduce Hard2Verify, a human-annotated,
step-level verification benchmark produced with over 500 hours of human labor.
Hard2Verify is designed to rigorously assess step-level verifiers at the
frontier: Verifiers must provide step-level annotations or identify the first
error in responses generated by frontier LLMs for very recent, challenging, and
open-ended math questions. We evaluate 29 generative critics and process reward
models, demonstrating that, beyond a few standouts, open-source verifiers lag
closed source models. We subsequently analyze what drives poor performance in
step-level verification, the impacts of scaling verifier compute, as well as
fundamental questions such as self-verification and verification-generation
dynamics.

</details>
