<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 52]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context](https://arxiv.org/abs/2510.13858)
*Raheleh Biglari,Joachim Denil*

Main category: cs.AI

TL;DR: 提出了DOTechnique方法，通过决策一致性而非输出相似性来确定模型有效性，在缺乏明确有效性边界时也能高效识别有效性区域。


<details>
  <summary>Details</summary>
Motivation: 模型有效性对决策过程至关重要，但传统方法依赖预定义的有效性框架，这些框架可能不可用或不充分。

Method: DOTechnique通过评估替代模型是否与高保真模型产生等效决策来确定模型有效性，结合领域约束和符号推理来缩小搜索空间。

Result: 以高速公路变道系统为例，展示了DOTechnique如何发现仿真模型的有效性区域。

Conclusion: 该技术有潜力通过决策者上下文来支持发现模型有效性。

Abstract: Model validity is as critical as the model itself, especially when guiding
decision-making processes. Traditional approaches often rely on predefined
validity frames, which may not always be available or sufficient. This paper
introduces the Decision Oriented Technique (DOTechnique), a novel method for
determining model validity based on decision consistency rather than output
similarity. By evaluating whether surrogate models lead to equivalent decisions
compared to high-fidelity models, DOTechnique enables efficient identification
of validity regions, even in the absence of explicit validity boundaries. The
approach integrates domain constraints and symbolic reasoning to narrow the
search space, enhancing computational efficiency. A highway lane change system
serves as a motivating example, demonstrating how DOTechnique can uncover the
validity region of a simulation model. The results highlight the potential of
the technique to support finding model validity through decision-maker context.

</details>


### [2] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra,Jan Niehues*

Main category: cs.AI

TL;DR: 该论文提出了一种融合视觉信息（演讲者图像和演示文稿幻灯片）的多模态自动语音识别系统，特别针对科学演讲场景，通过数据增强方法解决了相关数据集缺乏的问题，显著降低了词错误率。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的自动语音识别系统主要依赖声学信息而忽略多模态上下文，但视觉信息对于消歧和适应至关重要。特别是在科学演讲场景中，演示文稿幻灯片包含重要的领域特定术语信息。

Method: 首先创建多模态演讲基准并自动分析领域特定术语转录；然后探索用多模态信息增强语音模型的方法；通过合适的数据增强方法解决缺乏配套幻灯片数据集的问题；最后使用增强数据集训练模型。

Result: 与基线模型相比，训练后的模型在所有词汇上实现了约34%的相对词错误率降低，在领域特定术语上实现了35%的相对词错误率降低。

Conclusion: 集成视觉信息（特别是演示文稿幻灯片）的多模态方法能显著提升自动语音识别性能，特别是在处理领域特定术语方面效果显著。

Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.

</details>


### [3] [Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment](https://arxiv.org/abs/2510.13985)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Giovanni Franco Gabriel Marraffini,Mario Alejandro Leiva,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在因果推理任务中容易产生因果幻觉，即使在没有足够证据支持的情况下也会错误地推断因果关系。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否会在经典的认知科学范式——列联判断任务中发展出因果幻觉，这种认知偏见被认为是许多社会问题的基础。

Method: 构建了1000个医学背景下的零列联场景数据集，在这些场景中可用信息不足以建立变量间的因果关系，然后提示LLMs评估潜在原因的有效性。

Result: 所有评估的模型都系统地推断出无根据的因果关系，显示出对因果幻觉的强烈易感性。

Conclusion: 研究结果支持LLMs只是复制因果语言而非真正理解因果关系的假设，并对在需要准确因果推理的领域中使用语言模型提出了担忧。

Abstract: Causal learning is the cognitive process of developing the capability of
making causal inferences based on available information, often guided by
normative principles. This process is prone to errors and biases, such as the
illusion of causality, in which people perceive a causal relationship between
two variables despite lacking supporting evidence. This cognitive bias has been
proposed to underlie many societal problems, including social prejudice,
stereotype formation, misinformation, and superstitious thinking. In this work,
we examine whether large language models are prone to developing causal
illusions when faced with a classic cognitive science paradigm: the contingency
judgment task. To investigate this, we constructed a dataset of 1,000 null
contingency scenarios (in which the available information is not sufficient to
establish a causal relationship between variables) within medical contexts and
prompted LLMs to evaluate the effectiveness of potential causes. Our findings
show that all evaluated models systematically inferred unwarranted causal
relationships, revealing a strong susceptibility to the illusion of causality.
While there is ongoing debate about whether LLMs genuinely understand causality
or merely reproduce causal language without true comprehension, our findings
support the latter hypothesis and raise concerns about the use of language
models in domains where accurate causal reasoning is essential for informed
decision-making.

</details>


### [4] [GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations](https://arxiv.org/abs/2510.14035)
*Rajesh Mangannavar,Prasad Tadepalli*

Main category: cs.AI

TL;DR: GammaZero是一种基于动作中心图表示框架的POMDP规划方法，能够通过在小规模问题上学习图结构模式，实现零样本泛化到更大规模问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要特定领域的神经网络架构且难以扩展，GammaZero旨在开发统一的图表示框架，实现跨问题规模的泛化能力。

Method: 将信念状态转换为动作中心图，使用图神经网络从专家演示中学习价值函数和策略，然后将学习到的启发式方法应用于更大问题的蒙特卡洛树搜索。

Result: 在标准POMDP基准测试中，GammaZero在相同规模问题上与BetaZero性能相当，且能零样本泛化到训练时未见过的2-4倍大规模问题，保持解质量的同时减少搜索需求。

Conclusion: GammaZero通过统一的图表示框架实现了POMDP规划的可扩展性和泛化能力，为大规模部分可观测环境下的决策问题提供了有效解决方案。

Abstract: We introduce an action-centric graph representation framework for learning to
guide planning in Partially Observable Markov Decision Processes (POMDPs).
Unlike existing approaches that require domain-specific neural architectures
and struggle with scalability, GammaZero leverages a unified graph-based belief
representation that enables generalization across problem sizes within a
domain. Our key insight is that belief states can be systematically transformed
into action-centric graphs where structural patterns learned on small problems
transfer to larger instances. We employ a graph neural network with a decoder
architecture to learn value functions and policies from expert demonstrations
on computationally tractable problems, then apply these learned heuristics to
guide Monte Carlo tree search on larger problems. Experimental results on
standard POMDP benchmarks demonstrate that GammaZero achieves comparable
performance to BetaZero when trained and tested on the same-sized problems,
while uniquely enabling zero-shot generalization to problems 2-4 times larger
than those seen during training, maintaining solution quality with reduced
search requirements.

</details>


### [5] [Position: Require Frontier AI Labs To Release Small "Analog" Models](https://arxiv.org/abs/2510.14053)
*Shriyash Upadhyay,Chaithanya Bandi,Narmeen Oozeer,Philip Quirke*

Main category: cs.AI

TL;DR: 本文提出通过强制大型AI实验室发布小型开放模拟模型来平衡AI安全与创新，这些模型作为公开代理，允许广泛参与安全验证和透明度研究，同时避免披露全规模模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全监管面临安全与创新的权衡困境，大多数监管措施因成本问题被搁置。本文旨在寻找既能确保AI安全又能促进创新的替代监管方案。

Method: 要求大型AI实验室发布小型开放模拟模型，这些模型是大型专有模型的缩小版本，采用类似训练方法并从大模型中蒸馏而来，作为公共代理供研究使用。

Result: 研究表明，基于这些小模型开发的安全和可解释性方法能有效推广到前沿规模系统，显著降低监管负担并加速安全进展。

Conclusion: 这种监管方法以最小额外成本实现公共利益，通过加深对模型的理解来缓解安全与创新的权衡，让两者都能得到更好的发展。

Abstract: Recent proposals for regulating frontier AI models have sparked concerns
about the cost of safety regulation, and most such regulations have been
shelved due to the safety-innovation tradeoff. This paper argues for an
alternative regulatory approach that ensures AI safety while actively promoting
innovation: mandating that large AI laboratories release small, openly
accessible analog models (scaled-down versions) trained similarly to and
distilled from their largest proprietary models.
  Analog models serve as public proxies, allowing broad participation in safety
verification, interpretability research, and algorithmic transparency without
forcing labs to disclose their full-scale models. Recent research demonstrates
that safety and interpretability methods developed using these smaller models
generalize effectively to frontier-scale systems. By enabling the wider
research community to directly investigate and innovate upon accessible
analogs, our policy substantially reduces the regulatory burden and accelerates
safety advancements.
  This mandate promises minimal additional costs, leveraging reusable resources
like data and infrastructure, while significantly contributing to the public
good. Our hope is not only that this policy be adopted, but that it illustrates
a broader principle supporting fundamental research in machine learning: deeper
understanding of models relaxes the safety-innovation tradeoff and lets us have
more of both.

</details>


### [6] [Generating Fair Consensus Statements with Social Choice on Token-Level MDPs](https://arxiv.org/abs/2510.14106)
*Carter Blair,Kate Larson*

Main category: cs.AI

TL;DR: 提出了一种基于多目标马尔可夫决策过程的共识声明生成框架，通过社会选择理论提供公平性保证，包括基于比例公平的随机生成策略和基于平等福利的搜索方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型共识声明生成框架缺乏提供可证明公平性保证的结构，无法在聚合多样化自由形式意见时确保公平性。

Method: 将任务建模为多目标、令牌级的马尔可夫决策过程，每个目标对应一个代理的偏好。提出两种基于社会选择理论的方法：保证处于事前核心的随机生成策略，以及基于平等福利最大化的搜索算法。

Result: 实验表明，使用语言模型实例化代理策略时，基于平等福利目标的搜索生成的共识声明在代理对齐的最坏情况下优于基线方法，包括Habermas Machine。

Conclusion: 该框架为共识声明生成提供了具有可证明公平性保证的正式结构，通过社会选择理论原则实现了更好的代理对齐效果。

Abstract: Current frameworks for consensus statement generation with large language
models lack the inherent structure needed to provide provable fairness
guarantees when aggregating diverse free-form opinions. We model the task as a
multi-objective, token-level Markov Decision Process (MDP), where each
objective corresponds to an agent's preference. Token-level rewards for each
agent are derived from their policy (e.g., a personalized language model). This
approach utilizes the finding that such policies implicitly define optimal
Q-functions, providing a principled way to quantify rewards at each generation
step without a value function (Rafailov et al., 2024). This MDP formulation
creates a formal structure amenable to analysis using principles from social
choice theory. We propose two approaches grounded in social choice theory.
First, we propose a stochastic generation policy guaranteed to be in the
ex-ante core, extending core stability concepts from voting theory to text
generation. This policy is derived from an underlying distribution over
complete statements that maximizes proportional fairness (Nash Welfare).
Second, for generating a single statement, we target the maximization of
egalitarian welfare using search algorithms within the MDP framework.
Empirically, experiments using language models to instantiate agent policies
show that search guided by the egalitarian objective generates consensus
statements with improved worst-case agent alignment compared to baseline
methods, including the Habermas Machine (Tessler et al., 2024).

</details>


### [7] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: 提出STEMS框架，一种安全约束的多智能体强化学习方法，用于协调建筑能源管理，通过空间-时间图表示学习和控制屏障函数实现安全保证。


<details>
  <summary>Details</summary>
Motivation: 解决多建筑能源系统中空间-时间依赖关系利用不足、缺乏严格安全保证和系统复杂性等关键挑战。

Method: 集成GCN-Transformer融合架构的空间-时间图表示学习框架，以及结合控制屏障函数的安全约束多智能体强化学习算法。

Result: 在真实建筑数据集上，STEMS实现21%成本降低、18%排放减少，安全违规从35.1%降至5.6%，不适感比例仅0.13%。

Conclusion: STEMS框架在极端天气条件下表现出强鲁棒性，在不同建筑类型中保持有效性，为协调建筑能源管理提供了安全可靠的解决方案。

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [8] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: 提出了一个建模框架来解决多智能体AI系统中的语义鸿沟问题，包含主机智能体模型和任务生命周期模型，定义了31个形式化属性用于系统验证。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体AI系统的通信协议碎片化，导致系统属性无法严格分析，存在架构错位和可被利用的协调问题风险。

Method: 引入两个基础模型：主机智能体模型（负责用户交互、任务分解和编排）和任务生命周期模型（详细描述子任务状态转换），并定义了17个主机智能体属性和14个任务生命周期属性。

Result: 建立了首个严格基础、领域无关的框架，能够对多AI智能体系统进行系统化分析、设计和部署，支持形式化验证。

Conclusion: 该框架为构建正确、可靠和鲁棒的智能体AI系统提供了统一的语义基础，能够检测协调边界情况、预防死锁和安全漏洞。

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [9] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui,Adèle Cormier,nistor Grozavu,Ann Bourges*

Main category: cs.AI

TL;DR: 提出轻量级多模态架构，融合传感器数据和视觉图像预测文化遗产地退化程度，在数据稀缺情况下实现76.9%准确率


<details>
  <summary>Details</summary>
Motivation: 传统单模态监测方法无法捕捉环境应力与材料退化间的复杂相互作用，文化遗产地面临气候变化加速退化的问题

Method: 基于PerceiverIO的轻量多模态架构，采用简化编码器(64D潜在空间)和自适应Barlow Twins损失函数，鼓励模态互补性

Result: 在斯特拉斯堡大教堂数据上达到76.9%准确率，比标准多模态架构提升43%，比单传感器(61.5%)和单图像(46.2%)表现更好

Conclusion: 架构简化与对比正则化结合可在数据稀缺的遗产监测场景中实现有效的多模态学习，为AI驱动的保护决策支持系统奠定基础

Abstract: Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.

</details>


### [10] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve是一个开源的进化编码代理，将大语言模型与遗传算法结合来解决复杂计算问题，在多个数学基准测试中超越了Google DeepMind的AlphaEvolve。


<details>
  <summary>Details</summary>
Motivation: 结合大语言模型与进化算法来解决复杂计算问题，并构建开源框架以促进协作和加速进展。

Method: 采用基于岛屿的遗传算法保持种群多样性，引入基于启发的交叉机制利用LLM上下文窗口组合成功解决方案的特征，并实现元提示策略动态探索解空间。

Result: 在用于评估Google DeepMind AlphaEvolve的数学基准测试子集上，CodeEvolve在多个挑战性问题上的表现超越了AlphaEvolve。

Conclusion: CodeEvolve成功地将进化概念应用于LLM领域，展示了开源框架在解决复杂计算问题方面的潜力，并发布了完整的开源代码库。

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [11] [Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola](https://arxiv.org/abs/2510.14154)
*Tian Liu,Alex Cann,Ian Colbert,Mehdi Saeedi*

Main category: cs.AI

TL;DR: 本文探讨了在商业视频游戏中应用强化学习（RL）的挑战，并提出将RL与传统行为树（BTs）结合的方法，通过AMD Schola插件在复杂3D环境中训练多任务NPC。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习研究进展迅速，但在商业视频游戏中的应用仍然缓慢。本文旨在解决游戏AI社区在实际使用RL驱动NPC时面临的常见挑战。

Method: 使用AMD Schola插件在Unreal Engine中训练RL代理，将RL模型与行为树（BTs）联合训练，在受《最后生还者》启发的复杂3D环境中创建多任务NPC。

Result: 展示了RL与BTs结合方法的可行性，成功训练出具备多种技能的NPC，提供了详细的联合训练方法。

Conclusion: RL与行为树的交叉点是一个值得进一步探索的关键节点，这种方法能够有效解决RL在商业游戏应用中面临的挑战。

Abstract: While the rapid advancements in the reinforcement learning (RL) research
community have been remarkable, the adoption in commercial video games remains
slow. In this paper, we outline common challenges the Game AI community faces
when using RL-driven NPCs in practice, and highlight the intersection of RL
with traditional behavior trees (BTs) as a crucial juncture to be explored
further. Although the BT+RL intersection has been suggested in several research
papers, its adoption is rare. We demonstrate the viability of this approach
using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by
creating multi-task NPCs in a complex 3D environment inspired by the commercial
video game ``The Last of Us". We provide detailed methodologies for jointly
training RL models with BTs while showcasing various skills.

</details>


### [12] [JEDA: Query-Free Clinical Order Search from Ambient Dialogues](https://arxiv.org/abs/2510.14169)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Amitabh Saikia,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: JEDA是一个用于临床订单检索的双编码器系统，能够直接从临床对话中检索规范订单，支持显式查询和无查询模式，实现了实时、可解释的临床订单检索。


<details>
  <summary>Details</summary>
Motivation: 临床对话包含显式指令和隐式推理，现有系统依赖LLM重写导致延迟、不稳定和不透明，阻碍实时订单处理。

Method: 基于PubMedBERT初始化，使用重复安全对比目标进行微调，通过受限LLM指导将签名订单与互补表述对齐，采用双编码器架构支持显式查询和无查询检索。

Result: JEDA在实际部署中取得显著提升，大幅优于基础编码器和最新开源嵌入模型，无查询模式对噪声具有鲁棒性。

Conclusion: JEDA提供了一个快速、可解释、无需LLM的检索层，能够实时连接环境上下文与可操作的临床订单。

Abstract: Clinical conversations mix explicit directives (order a chest X-ray) with
implicit reasoning (the cough worsened overnight, we should check for
pneumonia). Many systems rely on LLM rewriting, adding latency, instability,
and opacity that hinder real-time ordering. We present JEDA (Joint Embedding
for Direct and Ambient clinical orders), a domain-initialized bi-encoder that
retrieves canonical orders directly and, in a query-free mode, encodes a short
rolling window of ambient dialogue to trigger retrieval. Initialized from
PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA
aligns heterogeneous expressions of intent to shared order concepts. Training
uses constrained LLM guidance to tie each signed order to complementary
formulations (command only, context only, command+context, context+reasoning),
producing clearer inter-order separation, tighter query extendash order
coupling, and stronger generalization. The query-free mode is noise-resilient,
reducing sensitivity to disfluencies and ASR errors by conditioning on a short
window rather than a single utterance. Deployed in practice, JEDA yields large
gains and substantially outperforms its base encoder and recent open embedders
(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The
result is a fast, interpretable, LLM-free retrieval layer that links ambient
context to actionable clinical orders in real time.

</details>


### [13] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer,Faisal Mohamed,Pablo Samuel Castro,Cyrus Neary,Glen Berseth*

Main category: cs.AI

TL;DR: ARM-FM是一个利用基础模型自动生成奖励机制的强化学习框架，能够从自然语言描述自动构建奖励机器，实现任务分解和零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法对奖励函数设计高度敏感，这限制了其广泛应用。需要一种自动化的、组合式的奖励设计方法来降低人工设计成本。

Method: 使用基础模型从自然语言规范自动生成奖励机器，为每个自动机状态关联语言嵌入以实现跨任务泛化，利用奖励机器的结构化形式进行任务分解。

Result: 在多个挑战性环境中验证了ARM-FM的有效性，包括展示了零样本泛化能力。

Conclusion: ARM-FM通过结合基础模型和奖励机器，实现了从自然语言到强化学习目标的自动化转换，为奖励设计提供了有效的解决方案。

Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.

</details>


### [14] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*Göktuğ Bender,Samer Faraj,Anand Bhardwaj*

Main category: cs.AI

TL;DR: 本文对2019-2024年精准医学中AI实施的文献进行了范围综述，识别了数据质量、临床可靠性、工作流程整合和治理方面的关键障碍和促进因素，提出了支持可信和可持续实施的未来方向。


<details>
  <summary>Details</summary>
Motivation: 人工智能在精准医学中日益重要，能够整合和解释多模态数据，但在临床环境中的实施仍然有限。

Method: 采用生态系统框架对2019-2024年文献进行范围综述，分析数据质量、临床可靠性、工作流程整合和治理方面的障碍和促进因素。

Result: 识别了影响现实世界转化的相互依赖关系，提出了支持可信和可持续AI实施的关键方向。

Conclusion: 通过生态系统框架强调了相互依赖关系如何塑造现实世界转化，并提出了支持可信和可持续实施的未来方向。

Abstract: Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.

</details>


### [15] [Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks](https://arxiv.org/abs/2510.14207)
*Trilok Padhi,Pinxian Lu,Abdulkadir Erol,Tanmay Sutar,Gauri Sharma,Mina Sonmez,Munmun De Choudhury,Ugur Kursuncu*

Main category: cs.AI

TL;DR: 提出了一个在线骚扰代理基准，包含多轮骚扰对话数据集、多代理模拟、三种越狱攻击方法和混合评估框架。研究发现越狱调优使骚扰成功率大幅提升，闭源和开源模型表现出不同的升级轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有越狱研究主要关注单轮提示，而真实骚扰通常发生在多轮互动中，需要评估LLM代理在多轮交互中的安全性。

Method: 构建了多轮骚扰对话数据集，采用重复博弈理论指导的多代理模拟，开发了针对记忆、规划和微调的三种越狱攻击方法，使用混合方法评估框架。

Result: 越狱调优使Llama模型的骚扰成功率从57.25-64.19%提升至95.78-96.89%，Gemini从98.46%提升至99.33%。侮辱行为发生率从44.2-50.8%升至84.9-87.8%，辱骂行为从31.5-38.8%升至81.2-85.1%。

Conclusion: 多轮和理论基础的攻击不仅成功率高，还能模拟人类骚扰动态，需要开发强大的安全防护措施来确保在线平台的安全性和责任性。

Abstract: Large Language Model (LLM) agents are powering a growing share of interactive
web applications, yet remain vulnerable to misuse and harm. Prior jailbreak
research has largely focused on single-turn prompts, whereas real harassment
often unfolds over multi-turn interactions. In this work, we present the Online
Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn
harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)
simulation informed by repeated game theory, (iii) three jailbreak methods
attacking agents across memory, planning, and fine-tuning, and (iv) a
mixed-methods evaluation framework. We utilize two prominent LLMs,
LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our
results show that jailbreak tuning makes harassment nearly guaranteed with an
attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,
and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal
rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with
84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.
31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive
categories such as sexual or racial harassment. Qualitative evaluation further
reveals that attacked agents reproduce human-like aggression profiles, such as
Machiavellian/psychopathic patterns under planning, and narcissistic tendencies
with memory. Counterintuitively, closed-source and open-source models exhibit
distinct escalation trajectories across turns, with closed-source models
showing significant vulnerability. Overall, our findings show that multi-turn
and theory-grounded attacks not only succeed at high rates but also mimic
human-like harassment dynamics, motivating the development of robust safety
guardrails to ultimately keep online platforms safe and responsible.

</details>


### [16] [LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild](https://arxiv.org/abs/2510.14240)
*Jiayu Wang,Yifei Ming,Riya Dulepet,Qinglin Chen,Austin Xu,Zixuan Ke,Frederic Sala,Aws Albarghouthi,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 提出了LiveResearchBench基准和DeepEval评估套件，用于系统评估深度研究系统的能力，并对17个前沿系统进行了全面分析。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法充分评估深度研究系统，存在领域狭窄、问题模糊等问题，需要建立符合用户需求、动态、明确且搜索密集的评估框架。

Method: 基于四个原则构建LiveResearchBench基准（100个专家策划任务），开发DeepEval评估套件（涵盖内容和报告质量），并对17个系统进行综合评估。

Result: 识别了当前系统的优势、常见失败模式，以及推进可靠深度研究所需的关键系统组件。

Conclusion: LiveResearchBench和DeepEval为深度研究系统提供了严格的评估基础，揭示了当前技术现状和发展方向。

Abstract: Deep research -- producing comprehensive, citation-grounded reports by
searching and synthesizing information from hundreds of live web sources --
marks an important frontier for agentic systems. To rigorously evaluate this
ability, four principles are essential: tasks should be (1) user-centric,
reflecting realistic information needs, (2) dynamic, requiring up-to-date
information beyond parametric knowledge, (3) unambiguous, ensuring consistent
interpretation across users, and (4) multi-faceted and search-intensive,
requiring search over numerous web sources and in-depth analysis. Existing
benchmarks fall short of these principles, often focusing on narrow domains or
posing ambiguous questions that hinder fair comparison. Guided by these
principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated
tasks spanning daily life, enterprise, and academia, each requiring extensive,
dynamic, real-time web search and synthesis. Built with over 1,500 hours of
human labor, LiveResearchBench provides a rigorous basis for systematic
evaluation. To evaluate citation-grounded long-form reports, we introduce
DeepEval, a comprehensive suite covering both content- and report-level
quality, including coverage, presentation, citation accuracy and association,
consistency and depth of analysis. DeepEval integrates four complementary
evaluation protocols, each designed to ensure stable assessment and high
agreement with human judgments. Using LiveResearchBench and DeepEval, we
conduct a comprehensive evaluation of 17 frontier deep research systems,
including single-agent web search, single-agent deep research, and multi-agent
systems. Our analysis reveals current strengths, recurring failure modes, and
key system components needed to advance reliable, insightful deep research.

</details>


### [17] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 本文提出Agentic Self-Learning (ASL)框架，通过多角色协同进化的强化学习实现无需人工标注数据的智能体自我学习，证明了生成奖励模型和任务数据规模是开放域智能体学习的关键因素。


<details>
  <summary>Details</summary>
Motivation: 研究如何在不依赖人工标注数据集或预定义规则奖励的情况下，实现基于LLM的智能体的规模化训练。

Method: 提出ASL框架，包含提示生成器、策略模型和生成奖励模型，在共享工具环境和LLM骨干网络中形成任务生成、策略执行和评估的闭环强化学习。

Result: ASL实现稳定轮次增益，超越强基线方法，在零标注数据条件下持续改进，表现出优异的样本效率和鲁棒性。

Conclusion: 奖励来源和数据规模是开放域智能体学习的关键杠杆，多角色协同进化是实现可扩展自我改进智能体的有效方法。

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [18] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: 提出了MorphoBench基准测试，用于评估大型模型的推理能力，通过多学科问题和自适应难度调整来克服现有基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估大型模型推理能力的基准测试范围有限，缺乏根据模型推理能力演进调整难度的灵活性。

Method: 从现有基准和奥赛级别竞赛中收集复杂推理问题，利用模型推理过程中的关键陈述自适应修改问题分析难度，并包含使用仿真软件生成的问题以实现动态难度调整。

Result: 收集了超过1300个测试问题，基于o3和GPT-5等模型的推理能力迭代调整了MorphoBench的难度。

Conclusion: MorphoBench提高了模型推理评估的全面性和有效性，为改进大型模型的推理能力和科学稳健性提供了可靠指导。

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [19] [A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space](https://arxiv.org/abs/2510.14301)
*Bingjie Zhang,Yibo Yang,Renzhe,Dandan Guo,Jindong Gu,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: GuardSpace是一个在微调过程中保护大语言模型安全对齐的框架，通过安全敏感子空间和有害抵抗零空间来防止安全行为退化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在微调过程中容易丧失预训练的安全对齐能力，即使使用良性数据或低秩适配也会产生有害响应，需要保护安全机制。

Method: 使用协方差预条件奇异值分解将预训练权重分解为安全相关和安全无关组件，从安全无关组件初始化低秩适配器并冻结安全相关组件；构建零空间投影器限制适配器更新对有害提示的响应。

Result: 在多个预训练模型和下游任务上表现优异，对Llama-2-7B-Chat在GSM8K上的微调，有害分数从14.4%降至3.6%，准确率从26.0%提升至28.0%。

Conclusion: GuardSpace能有效保护微调过程中的安全对齐，在保持安全性的同时提升任务性能。

Abstract: Large language models (LLMs) have achieved remarkable success in diverse
tasks, yet their safety alignment remains fragile during adaptation. Even when
fine-tuning on benign data or with low-rank adaptation, pre-trained safety
behaviors are easily degraded, leading to harmful responses in the fine-tuned
models. To address this challenge, we propose GuardSpace, a guardrail framework
for preserving safety alignment throughout fine-tuning, composed of two key
components: a safety-sensitive subspace and a harmful-resistant null space.
First, we explicitly decompose pre-trained weights into safety-relevant and
safety-irrelevant components using covariance-preconditioned singular value
decomposition, and initialize low-rank adapters from the safety-irrelevant
ones, while freezing safety-relevant components to preserve their associated
safety mechanism. Second, we construct a null space projector that restricts
adapter updates from altering safe outputs on harmful prompts, thereby
maintaining the original refusal behavior. Experiments with various pre-trained
models on multiple downstream tasks demonstrate that GuardSpace achieves
superior performance over existing methods. Notably, for Llama-2-7B-Chat
fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,
reducing the average harmful score from 14.4% to 3.6%, while improving the
accuracy from from 26.0% to 28.0%.

</details>


### [20] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 提出了Terrarium框架，用于细粒度研究基于LLM的多智能体系统中的安全、隐私和安全性问题，通过重构黑板设计创建模块化、可配置的测试平台。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统能够自动化繁琐的用户任务，但引入了新的风险，包括错位、恶意方攻击、智能体被破坏或用户数据被盗等问题。

Method: 重新利用多智能体系统中的早期方法——黑板设计，创建模块化、可配置的测试平台，识别关键攻击向量（错位、恶意智能体、通信被破坏、数据投毒），实现三个协作多智能体场景和四种代表性攻击。

Result: 开发了Terrarium框架，提供了快速原型设计、评估和迭代防御与设计的工具，展示了框架的灵活性。

Conclusion: Terrarium框架旨在加速可信多智能体系统的进展，通过提供工具来快速原型设计、评估和迭代防御与设计。

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [21] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC是一个元认知框架，通过实时无监督的步骤级错误检测和自我纠正来增强多智能体系统的鲁棒性，防止错误级联传播。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中单个错误步骤会跨智能体传播并破坏整个轨迹的问题，提高系统的稳定性和可靠性。

Method: 采用两种互补设计：1）下一执行重构，从查询和交互历史预测下一步的嵌入以捕捉因果一致性；2）原型引导增强，学习正常步骤嵌入的原型先验，在稀疏上下文下稳定重构和异常评分。

Result: 在Who&When基准测试中，MASC显著优于所有基线方法，步骤级错误检测AUC-ROC提升高达8.47%；在不同MAS框架中都能带来一致的端到端性能提升。

Conclusion: MASC的元认知监控和针对性纠正能够以最小开销有效缓解错误传播，提高多智能体系统的鲁棒性。

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [22] [AI for Service: Proactive Assistance with AI Glasses](https://arxiv.org/abs/2510.14359)
*Zichen Wen,Yiyu Wang,Chenfei Liao,Boxue Yang,Junxian Li,Weifeng Liu,Haocong He,Bolong Feng,Xuyang Liu,Yuanhuiyi Lyu,Xu Zheng,Xuming Hu,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出了AI4Service新范式，通过Alpha-Service框架实现主动服务，解决"何时干预"和"如何服务"两大挑战，基于AI眼镜部署多智能体系统。


<details>
  <summary>Details</summary>
Motivation: 现有AI服务多为被动响应，无法主动预测用户需求。需要发展能够主动提供实时帮助的智能助手。

Method: 基于冯·诺依曼架构设计Alpha-Service框架，包含输入单元、中央处理单元、算术逻辑单元、内存单元和输出单元五个组件，通过多智能体系统在AI眼镜上实现。

Result: 开发了实时21点顾问、博物馆导览和购物搭配助手等案例，证明系统能够无缝感知环境、推断用户意图，无需明确提示即可提供及时有用的帮助。

Conclusion: AI4Service范式将AI从被动工具转变为主动伴侣，Alpha-Service框架为实现这一愿景提供了可行方案，展示了主动服务在实际场景中的应用潜力。

Abstract: In an era where AI is evolving from a passive tool into an active and
adaptive companion, we introduce AI for Service (AI4Service), a new paradigm
that enables proactive and real-time assistance in daily life. Existing AI
services remain largely reactive, responding only to explicit user commands. We
argue that a truly intelligent and helpful assistant should be capable of
anticipating user needs and taking actions proactively when appropriate. To
realize this vision, we propose Alpha-Service, a unified framework that
addresses two fundamental challenges: Know When to intervene by detecting
service opportunities from egocentric video streams, and Know How to provide
both generalized and personalized services. Inspired by the von Neumann
computer architecture and based on AI glasses, Alpha-Service consists of five
key components: an Input Unit for perception, a Central Processing Unit for
task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit
for long-term personalization, and an Output Unit for natural human
interaction. As an initial exploration, we implement Alpha-Service through a
multi-agent system deployed on AI glasses. Case studies, including a real-time
Blackjack advisor, a museum tour guide, and a shopping fit assistant,
demonstrate its ability to seamlessly perceive the environment, infer user
intent, and provide timely and useful assistance without explicit prompts.

</details>


### [23] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: 提出IP-Merging方法，无需调优即可将数学推理能力从数学LLM直接迁移到多模态LLM，通过识别推理相关参数并在MLLM子空间中进行投影和融合。


<details>
  <summary>Details</summary>
Motivation: 多模态LLM的数学推理能力落后于纯文本LLM，但直接使用模型融合方法会因参数空间不对齐导致性能下降。

Method: IP-Merging方法：1）识别MLLM和数学LLM中的推理相关参数；2）将这些参数投影到MLLM的子空间；3）在子空间中进行参数融合。

Result: 实验表明IP-Merging能有效提升MLLM的数学推理能力，且不损害其他能力。

Conclusion: IP-Merging是一种无需调优的方法，能成功将数学推理能力从数学LLM迁移到MLLM，解决了参数空间不对齐的问题。

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [24] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu,Hongjin Lu,Junliang Xing,Changhao Zhang,Yin Zhu,Yuhao Yang,Yuheng Jing,Kai Li,Kun Shao,Jianye Hao,Jun Wang,Yuanchun Shi*

Main category: cs.AI

TL;DR: Hi-Agent是一个可训练的分层视觉语言代理，用于移动设备控制，通过高层推理模型和低层动作模型的联合优化，在Android-in-the-Wild基准测试中达到87.9%的任务成功率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖直接的状态到动作映射，缺乏结构化推理和规划，导致在新任务或未见UI布局上泛化能力差。

Method: 提出分层架构：高层推理模型和低层动作模型联合优化。将多步决策重新表述为单步子目标序列，并设计前瞻优势函数，利用低层模型的执行反馈指导高层优化。

Result: 在Android-in-the-Wild基准测试中达到87.9%的任务成功率，显著优于提示型(17.7%)、监督学习(54.5%)和强化学习(71.9%)方法。在ScreenSpot-v2上展示竞争性的零样本泛化能力。

Conclusion: Hi-Agent通过分层设计和前瞻优势函数，有效解决了长视野任务中的路径爆炸问题，在移动控制场景中表现出强大的适应性和泛化能力。

Abstract: Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.

</details>


### [25] [IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning](https://arxiv.org/abs/2510.14406)
*Xikai Zhang,Bo Wang,Likang Xiao,Yongzhi Li,Quan Chen,Wenju Wu,Liu Liu*

Main category: cs.AI

TL;DR: 提出了IMAGINE框架，将多智能体系统的推理规划能力集成到单一紧凑模型中，显著超越MAS性能，在TravelPlanner基准上达到82.7%的最终通过率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理和规划方面仍面临挑战，现有多智能体系统虽然能提供更好的集体推理，但存在推理成本高、延迟长和端到端训练困难等问题。

Method: 提出IMAGINE框架，通过端到端训练将多智能体系统的能力集成到单一模型中，使用Qwen3-8B-Instruct作为基础模型进行训练。

Result: 训练后的模型在TravelPlanner基准上达到82.7%的最终通过率，远超DeepSeek-R1-671B的40%，同时保持更小的模型规模。

Conclusion: IMAGINE框架成功将多智能体系统的结构化推理和规划能力集成到单一紧凑模型中，显著提升了性能并降低了推理成本。

Abstract: Although large language models (LLMs) have made significant strides across
various tasks, they still face significant challenges in complex reasoning and
planning. For example, even with carefully designed prompts and prior
information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on
the TravelPlanner dataset in the sole-planning mode. Similarly, even in the
thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass
Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent
Systems (MAS) can offer improved collective reasoning, they often suffer from
high reasoning costs due to multi-round internal interactions, long
per-response latency, and difficulties in end-to-end training. To address these
challenges, we propose a general and scalable framework called IMAGINE, short
for Integrating Multi-Agent System into One Model. This framework not only
integrates the reasoning and planning capabilities of MAS into a single,
compact model, but also significantly surpass the capabilities of the MAS
through a simple end-to-end training. Through this pipeline, a single
small-scale model is not only able to acquire the structured reasoning and
planning capabilities of a well-organized MAS but can also significantly
outperform it. Experimental results demonstrate that, when using
Qwen3-8B-Instruct as the base model and training it with our method, the model
achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding
the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.

</details>


### [26] [Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms](https://arxiv.org/abs/2510.14412)
*Claudia Grundke,Gabriele Röger*

Main category: cs.AI

TL;DR: 本文研究了PDDL中公理的负谓词出现限制问题，提出了将包含负谓词的公理转换为等价的非负形式的方法。


<details>
  <summary>Details</summary>
Motivation: PDDL标准限制公理体中负谓词只能出现在由动作直接设置的谓词上，但文献中常偏离此限制，仅要求公理集可分层。这两种变体都能表达与最小不动点逻辑相同的查询，表明负谓词可以被消除。

Method: 提出了相应的转换方法，将包含负谓词的公理转换为等价的非负形式。

Result: 证明了包含负谓词的公理可以通过转换消除负谓词，同时保持查询表达能力不变。

Conclusion: 负谓词在公理中的出现可以被消除，PDDL标准中的限制和文献中的分层要求实际上是等价的。

Abstract: Axioms are a feature of the Planning Domain Definition Language PDDL that can
be considered as a generalization of database query languages such as Datalog.
The PDDL standard restricts negative occurrences of predicates in axiom bodies
to predicates that are directly set by actions and not derived by axioms. In
the literature, authors often deviate from this limitation and only require
that the set of axioms is stratifiable. Both variants can express exactly the
same queries as least fixed-point logic, indicating that negative occurrences
of derived predicates can be eliminated. We present the corresponding
transformation.

</details>


### [27] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: Helmsman是一个多智能体系统，能够从高级用户规范自动合成端到端的联邦学习系统，通过模拟研发工作流程生成与手工基线相竞争甚至更优的解决方案。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在去中心化数据训练模型方面具有强大潜力，但设计和部署鲁棒系统的巨大复杂性阻碍了其发展，需要自动化解决方案来克服这一瓶颈。

Method: 采用三阶段协作方法：交互式人机循环规划制定研究计划、监督智能体团队进行模块化代码生成、在沙盒模拟环境中进行自主评估和优化的闭环流程。

Result: 实验表明，该方法生成的解决方案与已建立的手工基线相竞争，且通常更优。同时引入了AgentFL-Bench基准来评估FL中智能体系统的系统级生成能力。

Conclusion: 这项研究代表了向复杂去中心化AI系统自动化工程迈出的重要一步，Helmsman系统能够有效解决联邦学习系统设计的复杂性挑战。

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [28] [JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol](https://arxiv.org/abs/2510.14537)
*Emanuele Antonioni,Stefan Markovic,Anirudha Shankar,Jaime Bernardo,Lovro Markovic,Silvia Pareti,Benedetto Proietti*

Main category: cs.AI

TL;DR: JSPLIT是一个基于分类学的框架，用于解决大型语言模型在使用MCP工具时出现的提示膨胀问题，通过层次化分类和工具选择算法显著减少提示大小，同时保持甚至提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统发展，用户期望从简单的文本交互转向更复杂的代理系统，需要LLMs与外部工具交互。但工具数量增加导致提示膨胀，带来高令牌成本、延迟增加和任务成功率下降的问题。

Method: JSPLIT将工具组织成层次化分类学，使用用户提示基于查询和分类结构识别并仅包含最相关的工具，包括分类设计、工具选择算法和评估数据集。

Result: JSPLIT显著减少了提示大小，且没有显著影响代理的响应能力。当可用工具数量大幅增加时，JSPLIT甚至提高了工具选择准确性，在降低成本的同时改善了高复杂度环境中的任务成功率。

Conclusion: JSPLIT框架有效解决了MCP工具使用中的提示膨胀问题，通过智能工具选择在减少成本的同时维持或提高代理性能，特别适用于工具数量庞大的复杂代理环境。

Abstract: AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today's
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user's prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent's ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.

</details>


### [29] [Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts](https://arxiv.org/abs/2510.14538)
*Emanuele Marconato,Samuele Bortolotti,Emile van Krieken,Paolo Morettin,Elena Umili,Antonio Vergari,Efthymia Tsamoura,Andrea Passerini,Stefano Teso*

Main category: cs.AI

TL;DR: 本文概述了神经符号AI中的推理捷径问题，讨论了其成因、后果及应对方法，旨在为开发可靠的神经符号AI模型提供统一视角。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI模型在缺乏概念监督时容易出现推理捷径问题，这会损害模型的可解释性、分布外性能及可靠性。现有研究分散，需要系统梳理以降低研究门槛。

Method: 通过直观方式介绍推理捷径的成因和后果，回顾现有理论表征，详细分析处理推理捷径的方法（包括缓解和认知策略），并评估其优缺点。

Result: 提供了对推理捷径问题的统一视角，系统梳理了相关理论和应对方法，有助于研究人员更好地理解和解决这一挑战性问题。

Conclusion: 本文通过重新表述复杂内容，为处理推理捷径问题降低了入门门槛，有望促进可靠神经符号AI和可信AI模型的发展。

Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose
predictions comply with prior knowledge encoding, e.g. safety or structural
constraints. As such, it represents one of the most promising avenues for
reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural
and symbolic steps: neural networks are typically responsible for mapping
low-level inputs into high-level symbolic concepts, while symbolic reasoning
infers predictions compatible with the extracted concepts and the prior
knowledge. Despite their promise, it was recently shown that - whenever the
concepts are not supervised directly - NeSy models can be affected by Reasoning
Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the
concepts incorrectly. RSs can compromise the interpretability of the model's
explanations, performance in out-of-distribution scenarios, and therefore
reliability. At the same time, RSs are difficult to detect and prevent unless
concept supervision is available, which is typically not the case. However, the
literature on RSs is scattered, making it difficult for researchers and
practitioners to understand and tackle this challenging problem. This overview
addresses this issue by providing a gentle introduction to RSs, discussing
their causes and consequences in intuitive terms. It also reviews and
elucidates existing theoretical characterizations of this phenomenon. Finally,
it details methods for dealing with RSs, including mitigation and awareness
strategies, and maps their benefits and limitations. By reformulating advanced
material in a digestible form, this overview aims to provide a unifying
perspective on RSs to lower the bar to entry for tackling them. Ultimately, we
hope this overview contributes to the development of reliable NeSy and
trustworthy AI models.

</details>


### [30] [LLM Agents Beyond Utility: An Open-Ended Perspective](https://arxiv.org/abs/2510.14548)
*Asen Nachkov,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 本文研究了预训练LLM代理在开放式环境中的能力，发现其能够生成自己的任务、积累知识并解决复杂多步骤指令，但在提示设计敏感性、重复任务生成和自我表征方面仍存在局限。


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理是否能从智能问题解决工具发展为具有自主规划、任务设计和模糊目标推理能力的独立实体。

Method: 采用开放式实验设置，增强预训练LLM代理的能力，使其能够生成自己的任务、积累知识并与环境广泛互动，并进行定性研究。

Result: 代理能够可靠地遵循复杂多步骤指令，跨运行存储和重用信息，提出并解决自己的任务，但对提示设计敏感，容易产生重复任务，无法形成自我表征。

Conclusion: 研究显示了将预训练LLM适应开放式环境的潜力和当前限制，为未来训练代理管理记忆、有效探索和追求抽象长期目标指明了方向。

Abstract: Recent LLM agents have made great use of chain of thought reasoning and
function calling. As their capabilities grow, an important question arises: can
this software represent not only a smart problem-solving tool, but an entity in
its own right, that can plan, design immediate tasks, and reason toward
broader, more ambiguous goals? To study this question, we adopt an open-ended
experimental setting where we augment a pretrained LLM agent with the ability
to generate its own tasks, accumulate knowledge, and interact extensively with
its environment. We study the resulting open-ended agent qualitatively. It can
reliably follow complex multi-step instructions, store and reuse information
across runs, and propose and solve its own tasks, though it remains sensitive
to prompt design, prone to repetitive task generation, and unable to form
self-representations. These findings illustrate both the promise and current
limits of adapting pretrained LLMs toward open-endedness, and point to future
directions for training agents to manage memory, explore productively, and
pursue abstract long-term goals.

</details>


### [31] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song,Heyuan Huang,Qiqiang Lin,Yin Zhao,Xiangmou Qu,Jun Wang,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang,Zhaoxiang Wang*

Main category: cs.AI

TL;DR: 本文提出了ColorBench，一个基于图结构的移动代理评估框架，用于解决复杂长时程任务的评估问题，支持多有效解验证和准动态交互。


<details>
  <summary>Details</summary>
Motivation: 当前移动代理评估方法存在局限：离线静态基准只能验证单一预设路径，而在线动态测试受限于真实设备的复杂性和不可重复性，无法全面评估代理能力。

Method: 通过建模真实设备交互中的有限状态，实现动态行为的静态模拟，开发了ColorBench基准，包含175个任务（74个单应用，101个跨应用），每个任务平均超过13步，包含至少两条正确路径和典型错误路径。

Result: 通过在不同基线模型上评估ColorBench，发现了现有模型的局限性，并基于实验结果提出了改进方向和可行的技术路径。

Conclusion: ColorBench框架能够有效评估移动代理在复杂长时程任务中的表现，填补了离线与在线评估之间的空白，提高了测试稳定性。

Abstract: The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.

</details>


### [32] [Beyond Hallucinations: The Illusion of Understanding in Large Language Models](https://arxiv.org/abs/2510.14665)
*Rikard Rosenbacke,Carl Rosenbacke,Victor Rosenbacke,Martin McKee*

Main category: cs.AI

TL;DR: 本文提出了Rose-Frame三维框架，用于诊断人机交互中的认知和认识论漂移问题，强调LLMs本质上是系统1认知的大规模操作，需要人类理性进行认知治理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然流畅且情感共鸣强，但基于统计预测而非有根据的推理，存在幻觉风险，即听起来有说服力但缺乏事实有效性。它们继承了语言本身的模糊性、偏见和缺乏直接访问真相的特性。

Method: 引入Rose-Frame三维诊断框架：(i)地图与领土：区分现实表征与现实本身；(ii)直觉与理性：基于双过程理论分离快速情感判断与慢速反思思维；(iii)冲突与确认：检验观点是通过批判性测试还是简单相互验证。

Result: Rose-Frame不试图用更多数据或规则修复LLMs，而是提供反思工具，使模型局限性和用户假设可见，实现更透明和批判性意识的AI部署。

Conclusion: 将对齐重新定义为认知治理：无论是人类还是人工智能的直觉，都必须由人类理性来治理。只有通过嵌入反思性、可证伪的监督，才能将机器的流畅性与人类理解对齐。

Abstract: Large language models (LLMs) are becoming deeply embedded in human
communication and decision-making, yet they inherit the ambiguity, bias, and
lack of direct access to truth inherent in language itself. While their outputs
are fluent, emotionally resonant, and coherent, they are generated through
statistical prediction rather than grounded reasoning. This creates the risk of
hallucination, responses that sound convincing but lack factual validity.
Building on Geoffrey Hinton's observation that AI mirrors human intuition
rather than reasoning, this paper argues that LLMs operationalize System 1
cognition at scale: fast, associative, and persuasive, but without reflection
or falsification. To address this, we introduce the Rose-Frame, a
three-dimensional framework for diagnosing cognitive and epistemic drift in
human-AI interaction. The three axes are: (i) Map vs. Territory, which
distinguishes representations of reality (epistemology) from reality itself
(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to
separate fast, emotional judgments from slow, reflective thinking; and (iii)
Conflict vs. Confirmation, which examines whether ideas are critically tested
through disagreement or simply reinforced through mutual validation. Each
dimension captures a distinct failure mode, and their combination amplifies
misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.
Instead, it offers a reflective tool that makes both the model's limitations
and the user's assumptions visible, enabling more transparent and critically
aware AI deployment. It reframes alignment as cognitive governance: intuition,
whether human or artificial, must remain governed by human reason. Only by
embedding reflective, falsifiable oversight can we align machine fluency with
human understanding.

</details>


### [33] [Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review](https://arxiv.org/abs/2510.14669)
*Sara Altamirano,Arjan Vreeken,Sennay Ghebreab*

Main category: cs.AI

TL;DR: 该论文系统评估了2021-2025年荷兰公共卫生机器学习研究中算法偏见的识别、讨论和报告情况，开发了RABAT评估工具，并提出了ACAR四阶段框架来帮助研究人员在ML生命周期中解决公平性问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习在公共卫生领域有巨大潜力，但如果不系统关注算法偏见，可能会无意中加剧现有的健康不平等。需要评估当前研究中对算法偏见的处理情况。

Method: 开发了RABAT评估工具，整合了Cochrane偏倚风险、PROBAST和微软负责任AI检查表的元素，并应用于35篇同行评审研究进行系统文献综述。

Result: 分析发现普遍存在差距：虽然数据采样和缺失数据处理记录良好，但大多数研究忽略了明确的公平性框架、亚组分析以及对潜在危害的透明讨论。

Conclusion: 提出了ACAR四阶段公平性导向框架和具体建议，帮助公共卫生ML从业者持续考虑算法偏见，促进透明度，确保算法创新促进而非损害健康公平。

Abstract: Machine learning (ML) promises to revolutionize public health through
improved surveillance, risk stratification, and resource allocation. However,
without systematic attention to algorithmic bias, ML may inadvertently
reinforce existing health disparities. We present a systematic literature
review of algorithmic bias identification, discussion, and reporting in Dutch
public health ML research from 2021 to 2025. To this end, we developed the Risk
of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from
established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible
AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals
pervasive gaps: although data sampling and missing data practices are well
documented, most studies omit explicit fairness framing, subgroup analyses, and
transparent discussion of potential harms. In response, we introduce a
four-stage fairness-oriented framework called ACAR (Awareness,
Conceptualization, Application, Reporting), with guiding questions derived from
our systematic literature review to help researchers address fairness across
the ML lifecycle. We conclude with actionable recommendations for public health
ML practitioners to consistently consider algorithmic bias and foster
transparency, ensuring that algorithmic innovations advance health equity
rather than undermine it.

</details>


### [34] [TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence](https://arxiv.org/abs/2510.14670)
*Marco Simoni,Aleksandar Fontana,Andrea Saracino,Paolo Mori*

Main category: cs.AI

TL;DR: TITAN是一个连接自然语言网络安全威胁查询与结构化知识图谱推理的框架，包含路径规划模型和图执行器，支持在MITRE威胁图谱上进行双向推理。


<details>
  <summary>Details</summary>
Motivation: 传统检索系统无法在威胁情报领域进行清晰的推理，需要将自然语言查询与可执行推理路径相结合。

Method: 集成路径规划模型预测逻辑关系链，图执行器遍历TITAN本体图谱检索答案和证据，基于MITRE构建双向类型化图谱。

Result: 创建了包含88209个示例的TITAN数据集，实证评估显示模型能生成语法有效、语义连贯的可执行推理路径。

Conclusion: TITAN框架成功实现了自然语言威胁查询与结构化知识图谱推理的有效连接，支持确定性执行推理路径。

Abstract: TITAN (Threat Intelligence Through Automated Navigation) is a framework that
connects natural-language cyber threat queries with executable reasoning over a
structured knowledge graph. It integrates a path planner model, which predicts
logical relation chains from text, and a graph executor that traverses the
TITAN Ontology to retrieve factual answers and supporting evidence. Unlike
traditional retrieval systems, TITAN operates on a typed, bidirectional graph
derived from MITRE, allowing reasoning to move clearly and reversibly between
threats, behaviors, and defenses. To support training and evaluation, we
introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:
13951) pairing natural language questions with executable reasoning paths and
step by step Chain of Thought explanations. Empirical evaluations show that
TITAN enables models to generate syntactically valid and semantically coherent
reasoning paths that can be deterministically executed on the underlying graph.

</details>


### [35] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: NAEL是一个基于主动推理和符号推理的非人类中心主义AI伦理框架，将伦理行为定义为智能系统在动态多智能体环境中最小化全局期望自由能量的涌现属性。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI伦理模型局限于人类中心主义视角的问题，让智能体能够在不确定环境中发展出情境敏感、自适应和关系性的伦理行为，而不预设人类道德直觉。

Method: 提出神经符号架构，结合主动推理和符号推理，使智能体能够评估在不确定环境中的行为伦理后果。

Result: 通过伦理资源分配的案例研究，展示了NAEL能够动态平衡自我保存、认知学习和集体福利。

Conclusion: NAEL为人工智能伦理提供了一种新的非人类中心主义框架，能够产生适应性的伦理行为，克服了传统人类中心伦理模型的局限性。

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [36] [Practical, Utilitarian Algorithm Configuration](https://arxiv.org/abs/2510.14683)
*Devon Graham,Kevin Leyton-Brown*

Main category: cs.AI

TL;DR: COUP是一个基于效用理论的算法配置方法，本文通过一系列改进使其在实际性能上达到与广泛使用的启发式配置方法相当的水平，同时保持理论保证。


<details>
  <summary>Details</summary>
Motivation: COUP方法主要关注理论保证而忽略了实际性能，本文旨在弥补这一差距，使基于效用理论的算法配置方法在实际应用中具有竞争力。

Method: 提出了一系列对COUP的改进措施，在不降低理论保证的前提下提升其经验性能，并通过实验验证这些改进的效果。

Result: 改进后的COUP在实证性能上能够与广泛使用的启发式配置方法竞争，同时保持理论保证。

Conclusion: 通过改进COUP，成功地将基于效用理论的算法配置方法提升到实用水平，同时展示了如何探索算法选择问题解决方案对效用函数变化的鲁棒性。

Abstract: Utilitarian algorithm configuration identifies a parameter setting for a
given algorithm that maximizes a user's utility. Utility functions offer a
theoretically well-grounded approach to optimizing decision-making under
uncertainty and are flexible enough to capture a user's preferences over
algorithm runtimes (e.g., they can describe a sharp cutoff after which a
solution is no longer required, a per-hour cost for compute, or diminishing
returns from algorithms that take longer to run). COUP is a recently-introduced
utilitarian algorithm configuration procedure which was designed mainly to
offer strong theoretical guarantees about the quality of the configuration it
returns, with less attention paid to its practical performance. This paper
closes that gap, bringing theoretically-grounded, utilitarian algorithm
configuration to the point where it is competitive with widely used, heuristic
configuration procedures that offer no performance guarantees. We present a
series of improvements to COUP that improve its empirical performance without
degrading its theoretical guarantees and demonstrate their benefit
experimentally. Using a case study, we also illustrate ways of exploring the
robustness of a given solution to the algorithm selection problem to variations
in the utility function.

</details>


### [37] [Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging](https://arxiv.org/abs/2510.14697)
*Bang An,Yibo Yang,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 提出PAVE方法，通过在知识感知子空间中净化任务向量来改进模型融合性能，消除任务无关冗余并减少性能下降


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法中，任务向量包含任务无关冗余导致冲突和性能下降，现有随机丢弃参数的方法缺乏知识感知

Method: 使用训练样本获取协方差矩阵，进行上下文导向的奇异值分解，在知识感知子空间中分离任务相关和冗余组件，通过谱秩分配策略进行公平剪枝

Result: PAVE作为即插即用方案可提升各种基于任务向量的融合方法性能，在多种融合方法、任务和模型架构上验证了有效性

Conclusion: PAVE通过知识感知的任务向量净化能有效提升模型融合性能，减少冗余导致的冲突

Abstract: Model merging aims to integrate task-specific abilities from individually
fine-tuned models into a single model without extra training. In recent model
merging methods, task vector has become a fundamental building block, as it can
encapsulate the residual information from finetuning. However, the merged model
often suffers from notable performance degradation due to the conflicts caused
by task-irrelevant redundancy in task vectors. Existing efforts in overcoming
redundancy by randomly dropping elements in the parameter space involves
randomness and lacks knowledge awareness. To address these challenges, in this
study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.
Concretely, we sample some training examples from each task, and feed them into
their corresponding fine-tuned models to acquire the covariance matrices before
linear layers. We then perform a context-oriented singular value decomposition,
which accentuates the weight components most relevant to the target knowledge.
As a result, we can split fine-tuned model weights into task-relevant and
redundant components in the knowledge-aware subspace, and purify the task
vector by pruning the redundant components. To induce fair pruning efforts
across models, we further introduce a spectral rank allocation strategy by
optimizing a normalized activated pruning error. The task vector purification
by our method as a plug-and-play scheme is applicable across various task
vector-based merging methods to improve their performance. In experiments, we
demonstrate the effectiveness of PAVE across a diverse set of merging methods,
tasks, and model architectures.

</details>


### [38] [Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction](https://arxiv.org/abs/2510.14702)
*Penglong Zhai,Jie Li,Fanyi Di,Yue Liu,Yifang Yuan,Jie Huang,Peng Wu,Sicong Wang,Mingyang Yin,Tingting Hu,Yao Xu,Xin Li*

Main category: cs.AI

TL;DR: CoAST是一个认知对齐的时空大语言模型框架，通过自然语言接口整合世界知识、时空轨迹模式和用户信息，用于下一个兴趣点推荐。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要基于非结构化文本预训练，缺乏对结构化地理实体和序列移动模式的理解，且未充分利用世界知识和人类认知对齐来提升推荐性能。

Method: 采用两阶段方法：1) 在去敏感化的用户时空轨迹数据上进行持续预训练获取推荐知识；2) 通过监督微调和强化学习实现认知判断与人类偏好的对齐。

Result: 在多个真实数据集上的离线实验和在AMAP App首页"猜你想去"的在线实验证明了CoAST的有效性。

Conclusion: CoAST框架能够有效整合世界知识和认知对齐，提升下一个兴趣点推荐的性能。

Abstract: The next point-of-interest (POI) recommendation task aims to predict the
users' immediate next destinations based on their preferences and historical
check-ins, holding significant value in location-based services. Recently,
large language models (LLMs) have shown great potential in recommender systems,
which treat the next POI prediction in a generative manner. However, these
LLMs, pretrained primarily on vast corpora of unstructured text, lack the
native understanding of structured geographical entities and sequential
mobility patterns required for next POI prediction tasks. Moreover, in
industrial-scale POI prediction applications, incorporating world knowledge and
alignment of human cognition, such as seasons, weather conditions, holidays,
and users' profiles (such as habits, occupation, and preferences), can enhance
the user experience while improving recommendation performance. To address
these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a
framework employing natural language as an interface, allowing for the
incorporation of world knowledge, spatio-temporal trajectory patterns,
profiles, and situational information. Specifically, CoAST mainly comprises of
2 stages: (1) Recommendation Knowledge Acquisition through continued
pretraining on the enriched spatial-temporal trajectory data of the
desensitized users; (2) Cognitive Alignment to align cognitive judgments with
human preferences using enriched training data through Supervised Fine-Tuning
(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline
experiments on various real-world datasets and online experiments deployed in
"Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of
CoAST.

</details>


### [39] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: 提出了一个结合细粒度波束搜索和过程奖励模型ToolPRM的推理扩展框架，用于提升LLM在结构化输出（如函数调用）任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理扩展研究主要关注非结构化输出生成任务，而在结构化输出（如函数调用）方面的应用研究不足。

Method: 构建了首个细粒度的函数调用过程监督数据集，使用函数掩码技术自动标注步骤级奖励，训练ToolPRM过程奖励模型，并结合细粒度波束搜索进行推理扩展。

Result: ToolPRM在预测准确性上优于粗粒度和结果奖励模型，配备ToolPRM的推理扩展技术显著提升了骨干模型在各种函数调用任务和基准测试中的性能。

Conclusion: 揭示了将推理扩展技术应用于结构化输出的关键原则："多探索少保留"，这是由于结构化函数调用生成的不可恢复特性决定的。

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [40] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: 提出SimKO方法解决RLVR中的过度集中问题，通过非对称设计提升探索能力，改善pass@K性能


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在系统性偏向利用而非探索的问题，表现为pass@1提升但pass@K下降，需要理解并解决这种训练动态问题

Method: 分析RLVR训练动态，提出SimKO方法：对已验证正确响应提升top-K候选概率，对已验证错误响应惩罚top-1候选，特别在高熵token上应用

Result: 在多个数学和逻辑推理基准测试中，SimKO能持续提高各种K值的pass@K性能

Conclusion: SimKO通过缓解概率过度集中问题有效促进探索，为改进RLVR的探索能力提供了简单有效的方法

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [41] [Agentic NL2SQL to Reduce Computational Costs](https://arxiv.org/abs/2510.14808)
*Dominik Jehle,Lennart Purucker,Frank Hutter*

Main category: cs.AI

TL;DR: Datalake Agent是一个基于LLM的代理系统，通过交互式循环选择性请求必要信息，大幅减少NL2SQL任务中的token使用量，在保持性能的同时降低成本。


<details>
  <summary>Details</summary>
Motivation: 传统NL2SQL方法需要处理大量数据库元信息，导致提示过长、token使用量大、处理成本高。

Method: 采用代理式交互循环，在推理框架中让LLM选择性请求必要信息来解决表问答任务，而不是一次性使用所有元信息。

Result: 在23个数据库的100个表问答任务上评估，Datalake Agent将LLM使用的token减少高达87%，同时保持竞争力性能。

Conclusion: Datalake Agent通过选择性信息请求机制，在NL2SQL任务中实现了显著的成本节约和效率提升。

Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.

</details>


### [42] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: 提出了RoboGPT-R1，一个用于具身规划的两阶段微调框架，通过监督训练获取基础知识，再通过强化学习解决视觉空间理解和推理的不足，在EmbodiedBench基准上显著优于GPT-4o-mini和其他模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型和视觉语言模型在规划任务中取得了成功，但在复杂真实环境中执行长视距操作任务时仍面临挑战，主要原因是常识和推理能力受限。

Method: 采用两阶段微调框架：监督训练通过专家序列获取基础知识，然后使用强化学习解决视觉空间理解和推理的不足，设计了考虑长视距性能和动作约束的基于规则的奖励函数。

Result: 在Qwen2.5-VL-3B上训练得到的推理模型，在EmbodiedBench基准上比GPT-4o-mini高出21.33%，比在Qwen2.5-VL-7B上训练的其他工作高出20.33%。

Conclusion: RoboGPT-R1框架通过结合监督训练和强化学习，有效提升了具身代理在复杂操作任务中的推理能力，实现了显著的性能提升。

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


### [43] [Boosting Instruction Following at Scale](https://arxiv.org/abs/2510.14842)
*Ben Elder,Evelyn Duesterwald,Vinod Muthusamy*

Main category: cs.AI

TL;DR: Instruction Boosting是一种后生成方法，通过提高LLM对提示指令的遵循率，解决多指令情况下性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 开发者通常通过修改提示来影响LLM行为，但仅添加更多指令无法保证它们会被遵循，需要提高指令遵循的可靠性。

Method: 提出Instruction Boosting作为后生成方法，并引入SCALEDIF基准测试，包含最多十条指令的数据样本。

Result: Instruction Boosting将指令遵循率提高了最多7个百分点（两条指令）和4个百分点（十条指令），并开发了量化冲突评分工具。

Conclusion: 指令数量增加导致性能下降的主要原因是指令间的紧张和冲突，Instruction Boosting能有效提高指令遵循可靠性。

Abstract: A typical approach developers follow to influence an LLM's behavior in an
application is through careful manipulation of the prompt, such as by adding or
modifying instructions. However, merely adding more instructions provides
little assurance that they will actually be followed. We introduce Instruction
Boosting as a post-generation method to increase the reliability of LLM prompt
instructions. We show that Instruction Boosting improves the instruction
following rate by up to 7 points for two instructions and up to 4 points for
ten instructions. To demonstrate these results we introduce SCALEDIF, a
benchmark with a scaled instruction volume of up to ten instructions per data
sample. We also present an analysis of the commonly observed trend that
performance degrades as more instructions are added. We show that an important
factor contributing to this trend is the degree of tension and conflict that
arises as the number of instructions is increased. We contribute a quantitative
conflict scoring tool that explains the observed performance trends and
provides feedback to developers on the impact that additional prompt
instructions have on a model's performance.

</details>


### [44] [Where to Search: Measure the Prior-Structured Search Space of LLM Agents](https://arxiv.org/abs/2510.14846)
*Zhuo-Yang Song*

Main category: cs.AI

TL;DR: 本文提出了一个紧凑的形式理论来描述和衡量基于领域先验指导的LLM辅助迭代搜索，通过模糊关系算子表示智能体，使用覆盖生成函数衡量可达性难度，并提供可测试的推断。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的生成-过滤-精炼迭代范式在推理、编程和科学发现中取得进展，但搜索效果依赖于如何将领域先验编码为操作化结构化的假设空间。

Method: 将智能体表示为输入输出的模糊关系算子，约束在固定安全包络内；通过单次延续参数加权所有可达路径并求和得到覆盖生成函数，从而衡量可达性难度；提供几何解释和多数投票实例化验证。

Result: 提出了一个可操作的语言和工具来衡量智能体及其搜索空间，为LLM构建的迭代搜索提供了系统化的形式描述。

Conclusion: 该理论为LLM辅助的迭代搜索提供了工作语言和操作工具，能够系统化地形式化描述此类搜索过程。

Abstract: The generate-filter-refine (iterative paradigm) based on large language
models (LLMs) has achieved progress in reasoning, programming, and program
discovery in AI+Science. However, the effectiveness of search depends on where
to search, namely, how to encode the domain prior into an operationally
structured hypothesis space. To this end, this paper proposes a compact formal
theory that describes and measures LLM-assisted iterative search guided by
domain priors. We represent an agent as a fuzzy relation operator on inputs and
outputs to capture feasible transitions; the agent is thereby constrained by a
fixed safety envelope. To describe multi-step reasoning/search, we weight all
reachable paths by a single continuation parameter and sum them to obtain a
coverage generating function; this induces a measure of reachability
difficulty; and it provides a geometric interpretation of search on the graph
induced by the safety envelope. We further provide the simplest testable
inferences and validate them via a majority-vote instantiation. This theory
offers a workable language and operational tools to measure agents and their
search spaces, proposing a systematic formal description of iterative search
constructed by LLMs.

</details>


### [45] [LabOS: The AI-XR Co-Scientist That Sees and Works With Humans](https://arxiv.org/abs/2510.14861)
*Le Cong,Zaixi Zhang,Xiaotong Wang,Yin Di,Ruofan Jin,Michal Gerasimiuk,Yinkai Wang,Ravi K. Dinesh,David Smerkous,Alex Smerkous,Xuekun Wu,Shilong Liu,Peishan Li,Yi Zhu,Simran Serrao,Ning Zhao,Imran A. Mohammad,John B. Sunwoo,Joseph C. Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: LabOS是首个将计算推理与物理实验结合的AI科学家助手，通过多模态感知、自进化代理和XR技术实现人机协作，使AI能够参与实验室实际工作。


<details>
  <summary>Details</summary>
Motivation: 现代科学发展需要思想与行动的结合，传统AI主要停留在计算设计阶段，无法直接参与物理实验。LabOS旨在让AI能够真正进入实验室环境，与科学家协作进行实验。

Method: 通过连接多模型AI代理、智能眼镜和人机协作系统，使AI能够感知科学家所见、理解实验背景，并实时协助实验执行。采用多模态感知、自进化代理和XR技术实现人机协作。

Result: 在癌症免疫治疗靶点发现和干细胞工程等应用中，LabOS证明了AI能够超越计算设计，直接参与实验过程，将实验室转变为智能协作环境。

Conclusion: LabOS展示了AI与人类科学家共同进化的可能性，通过人机协作将实验室转变为智能环境，推动科学发现的发展。

Abstract: Modern science advances fastest when thought meets action. LabOS represents
the first AI co-scientist that unites computational reasoning with physical
experimentation through multimodal perception, self-evolving agents, and
Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model
AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see
what scientists see, understand experimental context, and assist in real-time
execution. Across applications--from cancer immunotherapy target discovery to
stem-cell engineering -- LabOS shows that AI can move beyond computational
design to participation, turning the laboratory into an intelligent,
collaborative environment where human and machine discovery evolve together.

</details>


### [46] [The Gatekeeper Knows Enough](https://arxiv.org/abs/2510.14881)
*Fikresilase Wondmeneh Abebayew*

Main category: cs.AI

TL;DR: 提出了Gatekeeper协议来解决LLM作为自主代理时的上下文窗口限制和状态不同步问题，通过使用简化的潜在状态表示和按需请求高保真上下文的方法，提高代理的可靠性和计算效率。


<details>
  <summary>Details</summary>
Motivation: LLM作为自主代理部署时面临上下文窗口有限和状态不同步的挑战，导致输出不可靠、行为不可预测和资源使用效率低下，特别是在与大型结构化知识系统交互时。

Method: 引入Gatekeeper协议框架，要求代理先在简化的低保真潜在状态表示上进行操作和推理，然后按需策略性地请求高保真上下文，所有交互通过统一的JSON格式进行协调。

Result: 在软件开发的Sage实现中，该方法显著提高了代理可靠性，通过最小化token消耗改善了计算效率，并实现了与复杂系统的可扩展交互。

Conclusion: Gatekeeper协议为在任何结构化知识领域构建更稳健、可预测和基于现实的AI代理提供了基础方法论。

Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents,
yet their practical utility is fundamentally constrained by a limited context
window and state desynchronization resulting from the LLMs' stateless nature
and inefficient context management. These limitations lead to unreliable
output, unpredictable behavior, and inefficient resource usage, particularly
when interacting with large, structured, and sensitive knowledge systems such
as codebases and documents. To address these challenges, we introduce the
Gatekeeper Protocol, a novel, domain-agnostic framework that governs
agent-system interactions. Our protocol mandates that the agent first operate
and reason on a minimalist, low-fidelity "latent state" representation of the
system to strategically request high-fidelity context on demand. All
interactions are mediated through a unified JSON format that serves as a
declarative, state-synchronized protocol, ensuring the agent's model of the
system remains verifiably grounded in the system's reality. We demonstrate the
efficacy of this protocol with Sage, a reference implementation of the
Gatekeeper Protocol for software development. Our results show that this
approach significantly increases agent reliability, improves computational
efficiency by minimizing token consumption, and enables scalable interaction
with complex systems, creating a foundational methodology for building more
robust, predictable, and grounded AI agents for any structured knowledge
domain.

</details>


### [47] [Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates](https://arxiv.org/abs/2510.14900)
*Wen-Kwang Tsao,Yao-Ching Yu,Chien-Ming Huang*

Main category: cs.AI

TL;DR: 提出了一种无需标注数据或模型权重更新的强化学习代理，通过生成针对性网络搜索查询来收集外部证据，迭代优化企业日志的模式映射准确性。


<details>
  <summary>Details</summary>
Motivation: 企业智能平台需要整合多个第三方供应商的日志，但供应商文档在测试时往往不可用、不匹配、格式差或不完整，导致模式映射困难。

Method: 使用强化学习代理：1)识别模糊的字段映射尝试；2)生成针对性网络搜索查询收集外部证据；3)应用基于置信度的奖励迭代优化映射。

Result: 将Microsoft Defender for Endpoint日志转换为通用模式，映射准确率从56.4%(仅LLM)提升到72.73%(RAG)，再到93.94%(100次迭代)，同时将需要专家审查的低置信度映射减少了85%。

Conclusion: 该方法提供了一种证据驱动、透明的解决方案，为更稳健、可问责、可扩展、高效、灵活、适应性强和协作的行业问题解决铺平了道路。

Abstract: The Enterprise Intelligence Platform must integrate logs from numerous
third-party vendors in order to perform various downstream tasks. However,
vendor documentation is often unavailable at test time. It is either misplaced,
mismatched, poorly formatted, or incomplete, which makes schema mapping
challenging. We introduce a reinforcement learning agent that can self-improve
without labeled examples or model weight updates. During inference, the agent:
1) Identifies ambiguous field-mapping attempts. 2) Generates targeted
web-search queries to gather external evidence. 3) Applies a confidence-based
reward to iteratively refine its mappings. To demonstrate this concept, we
converted Microsoft Defender for Endpoint logs into a common schema. Our method
increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\%
over 100 iterations using GPT-4o. At the same time, it reduced the number of
low-confidence mappings requiring expert review by 85\%. This new approach
provides an evidence-driven, transparent method for solving future industry
problems, paving the way for more robust, accountable, scalable, efficient,
flexible, adaptable, and collaborative solutions.

</details>


### [48] [Budget-aware Test-time Scaling via Discriminative Verification](https://arxiv.org/abs/2510.14913)
*Kyle Montgomery,Sijun Tan,Yuqi Chen,Siyuan Zhuang,Tianjun Zhang,Raluca Ada Popa,Chenguang Wang*

Main category: cs.AI

TL;DR: 提出了一种基于判别式验证器的混合测试时扩展方法，在固定计算预算下显著优于生成式验证，在AIME2025上准确率提升达15.3%


<details>
  <summary>Details</summary>
Motivation: 现有的生成式验证器方法虽然性能强大，但计算成本过高，限制了实际应用。需要寻找更高效且预算友好的验证策略

Method: 采用判别式验证器与自一致性结合的混合方法，在固定计算预算下进行测试时扩展

Result: 在AIME2025上实现了比最先进生成式验证方法高15.3%的准确率，证明该方法更高效且有效

Conclusion: 对于实际应用，基于判别式验证器的预算感知扩展不仅是自一致性的免费升级，也是昂贵生成技术的更有效替代方案

Abstract: Test-time scaling is a powerful strategy for boosting the performance of
large language models on complex reasoning tasks. While state-of-the-art
approaches often employ generative verifiers to select the best solution from a
pool of candidates, this method incurs prohibitive computational costs,
limiting its practicality. In this work, we shift the focus to a more
budget-aware paradigm: discriminative verification. We conduct a thorough
empirical analysis and demonstrate that while discriminative verifiers may
underperform in isolation, combining them with self-consistency in a hybrid
approach creates a powerful and efficient test-time scaling mechanism. Notably,
under a fixed compute budget, this hybrid approach surpasses state-of-the-art
generative verification by a significant margin: achieving up to 15.3\% higher
accuracy on AIME2025. Our findings establish that for practical, real-world
applications, budget-aware scaling with discriminative verifiers is not only a
"free" upgrade over self-consistency, but also a more effective and efficient
alternative to costly generative techniques. Code is available at
https://github.com/wang-research-lab/verification.

</details>


### [49] [TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG](https://arxiv.org/abs/2510.14922)
*Annisaa Fitri Nurfidausi,Eleonora Mancini,Paolo Torroni*

Main category: cs.AI

TL;DR: 本文系统研究多模态抑郁检测，比较EEG、语音和文本特征，发现三模态组合、预训练嵌入和精心设计的模型能实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁检测研究范围有限，缺乏系统特征比较和一致评估标准，需要填补这些空白。

Method: 系统探索EEG、语音和文本的特征表示和建模策略，评估手工特征vs预训练嵌入，比较单模态、双模态和三模态配置，分析融合策略。

Result: EEG、语音和文本三模态组合增强检测性能，预训练嵌入优于手工特征，精心设计的三模态模型达到最先进水平。

Conclusion: 为多模态抑郁检测的未来研究奠定基础，证明了多模态方法的有效性。

Abstract: Depression is a widespread mental health disorder, yet its automatic
detection remains challenging. Prior work has explored unimodal and multimodal
approaches, with multimodal systems showing promise by leveraging complementary
signals. However, existing studies are limited in scope, lack systematic
comparisons of features, and suffer from inconsistent evaluation protocols. We
address these gaps by systematically exploring feature representations and
modelling strategies across EEG, together with speech and text. We evaluate
handcrafted features versus pre-trained embeddings, assess the effectiveness of
different neural encoders, compare unimodal, bimodal, and trimodal
configurations, and analyse fusion strategies with attention to the role of
EEG. Consistent subject-independent splits are applied to ensure robust,
reproducible benchmarking. Our results show that (i) the combination of EEG,
speech and text modalities enhances multimodal detection, (ii) pretrained
embeddings outperform handcrafted features, and (iii) carefully designed
trimodal models achieve state-of-the-art performance. Our work lays the
groundwork for future research in multimodal depression detection.

</details>


### [50] [Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models](https://arxiv.org/abs/2510.14925)
*Akira Okutomi*

Main category: cs.AI

TL;DR: 将康德的《纯粹理性批判》重新解释为反馈稳定性理论，提出复合不稳定性指数H-Risk来诊断推理系统的过度自信问题，并在LLMs中发现脆弱内部动态与校准错误和幻觉相关。


<details>
  <summary>Details</summary>
Motivation: 将康德的理性自我限制理论与反馈控制理论联系起来，为诊断和减少推理系统中的过度自信提供原则性框架。

Method: 提出复合不稳定性指数H-Risk（结合谱边界、条件数、时间敏感性和创新放大），在线性高斯模拟和大型语言模型中进行验证。

Result: 在线性高斯模拟中，更高的H-Risk即使在形式稳定性下也能预测过度自信错误；在LLMs中，脆弱内部动态与校准错误和幻觉相关，批判式提示对校准和幻觉的影响不一。

Conclusion: 在康德自我限制和反馈控制之间建立了结构性桥梁，为诊断和选择性减少推理系统中的过度自信提供了原则性视角。

Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback
stability, viewing reason as a regulator that keeps inference within the bounds
of possible experience. We formalize this intuition via a composite instability
index (H-Risk) combining spectral margin, conditioning, temporal sensitivity,
and innovation amplification. In linear-Gaussian simulations, higher H-Risk
predicts overconfident errors even under formal stability, revealing a gap
between nominal and epistemic stability. Extending to large language models
(LLMs), we find that fragile internal dynamics correlate with miscalibration
and hallucination, while critique-style prompts show mixed effects on
calibration and hallucination. These results suggest a structural bridge
between Kantian self-limitation and feedback control, offering a principled
lens for diagnosing -- and selectively reducing -- overconfidence in reasoning
systems. This is a preliminary version; supplementary experiments and broader
replication will be reported in a future revision.

</details>


### [51] [GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning](https://arxiv.org/abs/2510.14942)
*Yao Zhang,Yu Wu,Haowei Zhang,Weiguo Li,Haokun Chen,Jingpei Wu,Guohao Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: GroundedPRM是一个基于蒙特卡洛树搜索和外部工具验证的自动过程监督框架，通过减少奖励噪声、提高事实保真度和对齐步骤级推理目标，显著提升了大型语言模型的多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型面临三大挑战：嘈杂的奖励信号、低事实保真度以及与步骤级推理目标的不对齐。这些问题的根源在于缺乏可扩展的高质量注释，以及现有方法（如人工标注、LLM自评估和蒙特卡洛估计）存在的局限性。

Method: 1. 使用蒙特卡洛树搜索构建结构化推理路径，实现细粒度信用分配；2. 通过外部工具验证中间步骤，提供执行基础的正确性信号；3. 设计混合奖励聚合机制，结合工具验证和MCTS反馈；4. 将奖励信号格式化为增强推理的生成结构，提高可解释性。

Result: 仅使用4万个自动标注样本（仅为最佳性能PRM所用数据的10%），在ProcessBench上实现了高达26%的相对性能提升。在奖励引导的贪婪搜索中，甚至超越了使用人工标注监督训练的PRM。

Conclusion: GroundedPRM提供了一个可扩展且可验证的路径，能够实现高质量的过程级推理，解决了现有PRM方法的核心局限性，为多步推理任务提供了有效的自动监督解决方案。

Abstract: Process Reward Models (PRMs) aim to improve multi-step reasoning in Large
Language Models (LLMs) by supervising intermediate steps and identifying
errors. However, building effective PRMs remains challenging due to the lack of
scalable, high-quality annotations. Existing approaches rely on costly human
labeling, LLM-based self-evaluation that is prone to hallucination, or Monte
Carlo (MC) estimation, which infers step quality solely from rollout outcomes
and often introduces noisy, misaligned supervision due to credit
misattribution. These issues result in three core limitations: noisy rewards,
low factual fidelity, and misalignment with step-level reasoning objectives. To
address these challenges, we introduce GroundedPRM, a tree-guided and
fidelity-aware framework for automatic process supervision. To reduce reward
noise and enable fine-grained credit assignment, we construct structured
reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated
supervision, we validate each intermediate step using an external tool,
providing execution-grounded correctness signals. To combine both step-level
validation and global outcome assessment, we design a hybrid reward aggregation
mechanism that fuses tool-based verification with MCTS-derived feedback.
Finally, we format the reward signal into a rationale-enhanced, generative
structure to promote interpretability and compatibility with instruction-tuned
LLMs. GroundedPRM is trained on only 40K automatically labeled samples,
amounting to just 10% of the data used by the best-performing PRM trained with
auto-labeled supervision. Nevertheless, it achieves up to a 26% relative
improvement in average performance on ProcessBench. When used for reward-guided
greedy search, GroundedPRM outperforms even PRMs trained with human-labeled
supervision, offering a scalable and verifiable path toward high-quality
process-level reasoning.

</details>


### [52] [Agentic Design of Compositional Machines](https://arxiv.org/abs/2510.14980)
*Wenqian Zhang,Weiyang Liu,Zhen Liu*

Main category: cs.AI

TL;DR: 该论文研究大型语言模型能否学习复杂机器设计，通过构建BesiegeField测试平台评估LLMs在组合式机器设计中的能力，发现当前开源模型存在不足，并探索通过强化学习改进的方法。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否能够学习创造复杂机器，这是人类智能的重要标志和工程实践的基础。

Method: 引入BesiegeField测试平台，基于Besiege游戏构建，支持基于部件的构造、物理模拟和奖励驱动评估。对最先进的LLMs进行基准测试，并探索强化学习微调方法。

Result: 当前开源模型在空间推理、策略性组装和指令遵循等关键能力方面表现不足，但通过强化学习微调可以改进性能。

Conclusion: 机器设计任务处于语言、机器设计和物理推理的交叉领域，存在开放挑战，强化学习是改进模型性能的有前景路径。

Abstract: The design of complex machines stands as both a marker of human intelligence
and a foundation of engineering practice. Given recent advances in large
language models (LLMs), we ask whether they, too, can learn to create. We
approach this question through the lens of compositional machine design: a task
in which machines are assembled from standardized components to meet functional
demands like locomotion or manipulation in a simulated physical environment. To
support this investigation, we introduce BesiegeField, a testbed built on the
machine-building game Besiege, which enables part-based construction, physical
simulation and reward-driven evaluation. Using BesiegeField, we benchmark
state-of-the-art LLMs with agentic workflows and identify key capabilities
required for success, including spatial reasoning, strategic assembly, and
instruction-following. As current open-source models fall short, we explore
reinforcement learning (RL) as a path to improvement: we curate a cold-start
dataset, conduct RL finetuning experiments, and highlight open challenges at
the intersection of language, machine design, and physical reasoning.

</details>
