<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 46]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI Agents for the Dhumbal Card Game: A Comparative Study](https://arxiv.org/abs/2510.11736)
*Sahaj Raj Malla*

Main category: cs.AI

TL;DR: 本文系统评估了Dhumbal纸牌游戏中多种AI代理的性能，包括基于规则、搜索和学习的方法，发现基于规则的激进策略在1024轮模拟中获得了88.3%的最高胜率。


<details>
  <summary>Details</summary>
Motivation: 评估AI代理在具有文化意义的不完全信息多人纸牌游戏Dhumbal中的表现，为文化游戏的数字保存提供支持并推进AI研究。

Method: 形式化Dhumbal游戏机制，实现多种代理策略：基于启发式（激进、保守、平衡、机会主义）、基于搜索（MCTS、ISMCTS）和强化学习（DQN、PPO）方法，通过锦标赛进行评估。

Result: 在1024轮模拟中，基于规则的激进代理获得最高胜率（88.3%），显著优于ISMCTS（9.0%）和PPO（1.5%），主要通过有效利用Jhyap声明获胜。

Conclusion: 研究贡献了可复现的AI框架、启发式方法在不完全信息下的有效性见解以及开源代码，推动了AI研究并支持文化游戏的数字保存。

Abstract: This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a
culturally significant multiplayer card game with imperfect information,
through a systematic comparison of rule-based, search-based, and learning-based
strategies. We formalize Dhumbal's mechanics and implement diverse agents,
including heuristic approaches (Aggressive, Conservative, Balanced,
Opportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and
Information Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning
approaches including Deep Q-Network (DQN) and Proximal Policy Optimization
(PPO), and a random baseline. Evaluation involves within-category tournaments
followed by a cross-category championship. Performance is measured via win
rate, economic outcome, Jhyap success, cards discarded per round, risk
assessment, and decision efficiency. Statistical significance is assessed using
Welch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95%
confidence intervals (CI). Across 1024 simulated rounds, the rule-based
Aggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]),
outperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of
Jhyap declarations. The study contributes a reproducible AI framework, insights
into heuristic efficacy under partial information, and open-source code,
thereby advancing AI research and supporting digital preservation of cultural
games.

</details>


### [2] [Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations](https://arxiv.org/abs/2510.11822)
*Suryaansh Jain,Umair Z. Ahmed,Shubham Sahai,Ben Leong*

Main category: cs.AI

TL;DR: 论文发现LLM作为评估器存在严重正向偏差，提出少数否决策略和回归框架来缓解这一问题，在代码反馈任务上显著提升评估精度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法存在系统性正向偏差，虽然能准确识别有效输出，但难以识别无效输出，导致可靠性评分虚高。

Method: 提出少数否决策略和基于回归的框架，利用少量人工标注数据直接建模验证器偏差。

Result: 在366个高中Python程序的代码反馈任务中，回归方法将最大绝对误差降至1.2%，比14个最先进LLM的最佳集成效果提升2倍。

Conclusion: 所提出的方法能有效缓解LLM评估器的正向偏差问题，显著提升评估精度，为模型选择提供更可靠的依据。

Abstract: New Large Language Models (LLMs) become available every few weeks, and modern
application developers confronted with the unenviable task of having to decide
if they should switch to a new model. While human evaluation remains the gold
standard, it is costly and unscalable. The state-of-the-art approach is to use
LLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw:
LLMs exhibit a strong positive bias. We provide empirical evidence showing that
while LLMs can identify valid outputs with high accuracy (i.e., True Positive
Rate 96%), they are remarkably poor at identifying invalid ones (i.e., True
Negative Rate <25%). This systematic bias, coupled with class imbalance, often
leads to inflated reliability scores.
  While ensemble-based methods like majority voting can help, we show that they
are not good enough. We introduce an optimal minority-veto strategy that is
resilient to missing data and mitigates this bias to a large extent. For
scenarios requiring even higher precision, we propose a novel regression-based
framework that directly models the validator bias using a small set of
human-annotated ground truth data. On a challenging code feedback task over 366
high-school Python programs, our regression approach reduces the maximum
absolute error to just 1.2%, achieving a 2x improvement over the
best-performing ensemble of 14 state-of-the-art LLMs.

</details>


### [3] [Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation](https://arxiv.org/abs/2510.11977)
*Sayash Kapoor,Benedikt Stroebl,Peter Kirgis,Nitya Nadgir,Zachary S Siegel,Boyi Wei,Tianci Xue,Ziru Chen,Felix Chen,Saiteja Utpala,Franck Ndzomga,Dheeraj Oruganty,Sophie Luskin,Kangheng Liu,Botao Yu,Amit Arora,Dongyoon Hahm,Harsh Trivedi,Huan Sun,Juyong Lee,Tengjun Jin,Yifan Mai,Yifei Zhou,Yuxuan Zhu,Rishi Bommasani,Daniel Kang,Dawn Song,Peter Henderson,Yu Su,Percy Liang,Arvind Narayanan*

Main category: cs.AI

TL;DR: 提出了HAL（整体智能体排行榜）来解决AI智能体评估中的挑战，包括标准化评估框架、三维分析和LLM辅助日志检查，旨在推动从基准测试优秀到现实世界可靠的智能体发展。


<details>
  <summary>Details</summary>
Motivation: AI智能体在复杂现实任务中应用广泛，但现有评估存在诸多挑战，影响对智能体真实性能的理解，需要更全面可靠的评估方法。

Method: 开发标准化评估框架，在数百个虚拟机上进行并行评估；进行涵盖模型、支架和基准的三维分析；使用LLM辅助日志检查来发现未报告的行为。

Result: 完成了21,730次智能体运行，涵盖9个模型和9个基准，成本约4万美元；发现反直觉现象（更高推理努力反而降低准确性）；揭示了未报告行为（如搜索基准而非解决问题、滥用信用卡等）。

Conclusion: 通过标准化智能体评估方法并解决常见陷阱，HAL有助于将研究重点从基准测试表现转向现实世界可靠性，并公开2.5B token的智能体日志以促进进一步研究。

Abstract: AI agents have been developed for complex real-world tasks from coding to
customer service. But AI agent evaluations suffer from many challenges that
undermine our understanding of how well agents really work. We introduce the
Holistic Agent Leaderboard (HAL) to address these challenges. We make three
main contributions. First, we provide a standardized evaluation harness that
orchestrates parallel evaluations across hundreds of VMs, reducing evaluation
time from weeks to hours while eliminating common implementation bugs. Second,
we conduct three-dimensional analysis spanning models, scaffolds, and
benchmarks. We validate the harness by conducting 21,730 agent rollouts across
9 models and 9 benchmarks in coding, web navigation, science, and customer
service with a total cost of about $40,000. Our analysis reveals surprising
insights, such as higher reasoning effort reducing accuracy in the majority of
runs. Third, we use LLM-aided log inspection to uncover previously unreported
behaviors, such as searching for the benchmark on HuggingFace instead of
solving a task, or misusing credit cards in flight booking tasks. We share all
agent logs, comprising 2.5B tokens of language model calls, to incentivize
further research into agent behavior. By standardizing how the field evaluates
agents and addressing common pitfalls in agent evaluation, we hope to shift the
focus from agents that ace benchmarks to agents that work reliably in the real
world.

</details>


### [4] [CGBench: Benchmarking Language Model Scientific Reasoning for Clinical Genetics Research](https://arxiv.org/abs/2510.11985)
*Owen Queen,Harrison G. Zhang,James Zou*

Main category: cs.AI

TL;DR: CGBench是一个用于评估语言模型在科学文献解释能力的新基准，基于临床遗传学专家标注数据构建，测试模型提取实验结果、评估证据强度、分类实验成果的能力。


<details>
  <summary>Details</summary>
Motivation: 传统变异和基因解释方法耗时费力，生成式语言模型可以加速这一过程，但现有基准过于狭窄，无法反映真实研究需求。

Method: 基于ClinGen专家标注的临床遗传学文献构建CGBench基准，测试8种不同语言模型在提取实验结果、评估证据强度、分类实验成果三个方面的能力。

Result: 模型在文献解释方面表现出潜力但存在显著差距，推理模型在细粒度任务中表现更好，非推理模型在高层次解释中更优，模型经常产生幻觉或误解结果。

Conclusion: CGBench揭示了语言模型在科学文献精确解释方面的优势和弱点，为AI在临床遗传学和更广泛科学领域的未来研究开辟了新途径。

Abstract: Variant and gene interpretation are fundamental to personalized medicine and
translational biomedicine. However, traditional approaches are manual and
labor-intensive. Generative language models (LMs) can facilitate this process,
accelerating the translation of fundamental research into clinically-actionable
insights. While existing benchmarks have attempted to quantify the capabilities
of LMs for interpreting scientific data, these studies focus on narrow tasks
that do not translate to real-world research. To meet these challenges, we
introduce CGBench, a robust benchmark that tests reasoning capabilities of LMs
on scientific publications. CGBench is built from ClinGen, a resource of
expert-curated literature interpretations in clinical genetics. CGBench
measures the ability to 1) extract relevant experimental results following
precise protocols and guidelines, 2) judge the strength of evidence, and 3)
categorize and describe the relevant outcome of experiments. We test 8
different LMs and find that while models show promise, substantial gaps exist
in literature interpretation, especially on fine-grained instructions.
Reasoning models excel in fine-grained tasks but non-reasoning models are
better at high-level interpretations. Finally, we measure LM explanations
against human explanations with an LM judge approach, revealing that models
often hallucinate or misinterpret results even when correctly classifying
evidence. CGBench reveals strengths and weaknesses of LMs for precise
interpretation of scientific publications, opening avenues for future research
in AI for clinical genetics and science more broadly.

</details>


### [5] [Asking Clarifying Questions for Preference Elicitation With Large Language Models](https://arxiv.org/abs/2510.12015)
*Ali Montazeralghaem,Guy Tennenholtz,Craig Boutilier,Ofer Meshi*

Main category: cs.AI

TL;DR: 提出一种基于扩散模型思想的两阶段方法，训练LLMs生成有效的顺序澄清问题来揭示用户偏好，提升推荐系统的个性化能力。


<details>
  <summary>Details</summary>
Motivation: 在用户历史有限的情况下，如何通过生成有效的顺序澄清问题来获取用户偏好信息，以个性化LLM推荐系统的响应。

Method: 采用两阶段过程：前向过程从用户档案生成澄清问题并逐步移除答案作为"噪声"；反向过程训练模型通过提问来"去噪"用户档案。

Result: 该方法显著提高了LLM在提出漏斗式问题和有效获取用户偏好方面的能力。

Conclusion: 基于扩散模型思想的训练方法能够有效提升LLMs生成顺序澄清问题的能力，从而更好地获取用户偏好信息。

Abstract: Large Language Models (LLMs) have made it possible for recommendation systems
to interact with users in open-ended conversational interfaces. In order to
personalize LLM responses, it is crucial to elicit user preferences, especially
when there is limited user history. One way to get more information is to
present clarifying questions to the user. However, generating effective
sequential clarifying questions across various domains remains a challenge. To
address this, we introduce a novel approach for training LLMs to ask sequential
questions that reveal user preferences. Our method follows a two-stage process
inspired by diffusion models. Starting from a user profile, the forward process
generates clarifying questions to obtain answers and then removes those answers
step by step, serving as a way to add ``noise'' to the user profile. The
reverse process involves training a model to ``denoise'' the user profile by
learning to ask effective clarifying questions. Our results show that our
method significantly improves the LLM's proficiency in asking funnel questions
and eliciting user preferences effectively.

</details>


### [6] [CausalTrace: A Neurosymbolic Causal Analysis Agent for Smart Manufacturing](https://arxiv.org/abs/2510.12033)
*Chathurangi Shyalika,Aryaman Sharma,Fadi El Kalach,Utkarshani Jaimini,Cory Henson,Ramy Harik,Amit Sheth*

Main category: cs.AI

TL;DR: CausalTrace是一个集成在工业CoPilot中的神经符号因果分析模块，通过数据驱动的因果分析结合工业本体和知识图谱，提供因果发现、反事实推理和根本原因分析等功能，实现透明可解释的决策支持。


<details>
  <summary>Details</summary>
Motivation: 现代制造环境需要准确预测和可解释的异常分析，现有AI系统往往作为孤立的黑盒运行，缺乏预测、解释和因果推理的无缝集成，限制了在高风险工业环境中的可信度和实用性。

Method: 开发CausalTrace神经符号因果分析模块，集成到SmartPilot工业CoPilot中，利用数据驱动的因果分析结合工业本体和知识图谱，支持因果发现、反事实推理和根本原因分析，提供实时操作员交互。

Result: 在学术火箭装配测试中，CausalTrace与领域专家达成高度一致（ROUGE-1: 0.91），RCA性能优异（MAP@3: 94%，PR@2: 97%，MRR: 0.92，Jaccard: 0.92），在C3AN评估中获得4.59/5分，展现了部署的精确性和可靠性。

Conclusion: CausalTrace成功地将神经符号因果分析集成到工业CoPilot中，提供了透明可解释的决策支持，在多个评估指标上表现出色，适合实时工业部署。

Abstract: Modern manufacturing environments demand not only accurate predictions but
also interpretable insights to process anomalies, root causes, and potential
interventions. Existing AI systems often function as isolated black boxes,
lacking the seamless integration of prediction, explanation, and causal
reasoning required for a unified decision-support solution. This fragmentation
limits their trustworthiness and practical utility in high-stakes industrial
environments. In this work, we present CausalTrace, a neurosymbolic causal
analysis module integrated into the SmartPilot industrial CoPilot. CausalTrace
performs data-driven causal analysis enriched by industrial ontologies and
knowledge graphs, including advanced functions such as causal discovery,
counterfactual reasoning, and root cause analysis (RCA). It supports real-time
operator interaction and is designed to complement existing agents by offering
transparent, explainable decision support. We conducted a comprehensive
evaluation of CausalTrace using multiple causal assessment methods and the C3AN
framework (i.e. Custom, Compact, Composite AI with Neurosymbolic Integration),
which spans principles of robustness, intelligence, and trustworthiness. In an
academic rocket assembly testbed, CausalTrace achieved substantial agreement
with domain experts (ROUGE-1: 0.91 in ontology QA) and strong RCA performance
(MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92). It also attained 4.59/5 in
the C3AN evaluation, demonstrating precision and reliability for live
deployment.

</details>


### [7] [Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation](https://arxiv.org/abs/2510.12047)
*Soohan Lim,Joonghyuk Hahn,Hyunwoo Park,Sang-Ki Ko,Yo-Sub Han*

Main category: cs.AI

TL;DR: PACT是一个评估LLM生成代码合同遵循性的框架，通过扩展HumanEval+和MBPP+基准，引入合同违反测试用例来系统评估代码的鲁棒性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准主要评估功能正确性，但忽略了真实软件中合同遵循性（处理异常输入的能力），导致模型无法生成真正鲁棒的代码。

Method: 扩展HumanEval+和MBPP+基准，创建专注于合同违反的测试套件，通过不同提示条件分析代码生成，并引入新的合同遵循性量化指标。

Result: 在提示中加入合同违反测试用例能显著提升模型对合同的遵循能力，相比仅使用合同描述效果更好。

Conclusion: PACT提供了严格的、可解释的指标来评估LLM生成代码在功能和合同遵循性方面的鲁棒性，揭示了传统基准忽略的关键错误。

Abstract: Prevailing code generation benchmarks, such as HumanEval+ and MBPP+,
primarily evaluate large language models (LLMs) with pass@k on functional
correctness using well-formed inputs. However, they ignore a crucial aspect of
real-world software: adherence to contracts-the preconditions and validity
constraints that dictate how ill-formed inputs must be rejected. This critical
oversight means that existing benchmarks fail to measure, and models
consequently fail to generate, truly robust and reliable code snippets. We
introduce PACT, a program assessment and contract-adherence evaluation
framework, to bridge this gap. PACT is the first framework designed to
systematically evaluate and enhance contract-adherence in LLM-generated code
snippets alongside functional correctness. PACT's contributions are threefold:
First, it provides a comprehensive test-suite corpus focused on contract
violations, extending HumanEval+ and MBPP+. Second, it enables a systematic
analysis of code generation under varied prompting conditions. This analysis
demonstrates that augmenting prompts with contract-violating test cases
significantly enhance a model's ability to respect contracts compared to using
contract description alone. Finally, it introduces novel metrics to rigorously
quantify contract adherence in both test generation and code generation. By
revealing critical errors that conventional benchmarks overlook, PACT provides
the rigorous and interpretable metrics to evaluate the robustness of
LLM-generated code snippets in both functionality and contract-adherence.Our
code and data are available at https://github.com/suhanmen/PACT.

</details>


### [8] [Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response](https://arxiv.org/abs/2510.12061)
*Yiheng Chen,Lingyao Li,Zihui Ma,Qikai Hu,Yilun Zhu,Min Deng,Runlong Yu*

Main category: cs.AI

TL;DR: 提出了一种地理空间感知层(GAL)，通过整合地理数据增强LLM在灾害响应中的能力，使其能够生成基于证据的资源分配建议。


<details>
  <summary>Details</summary>
Motivation: 现有统计方法缺乏语义上下文，跨事件泛化能力差，可解释性有限；而LLM虽然具有少样本泛化能力，但受限于文本且缺乏地理感知。

Method: 开发GAL层，从原始野火检测数据自动检索并整合基础设施、人口、地形和天气等地理数据库信息，生成带单位标注的感知脚本。

Result: 在真实野火场景中评估，地理空间接地的智能体表现优于基线模型，能够产生基于证据的资源分配建议。

Conclusion: 该框架可推广到洪水、飓风等其他灾害类型，为灾害响应提供了更智能、可解释的解决方案。

Abstract: Effective disaster response is essential for safeguarding lives and property.
Existing statistical approaches often lack semantic context, generalize poorly
across events, and offer limited interpretability. While Large language models
(LLMs) provide few-shot generalization, they remain text-bound and blind to
geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL)
that grounds LLM agents in structured earth data. Starting from raw wildfire
detections, GAL automatically retrieves and integrates infrastructure,
demographic, terrain, and weather information from external geodatabases,
assembling them into a concise, unit-annotated perception script. This enriched
context enables agents to produce evidence-based resource-allocation
recommendations (e.g., personnel assignments, budget allocations), further
reinforced by historical analogs and daily change signals for incremental
updates. We evaluate the framework in real wildfire scenarios across multiple
LLM models, showing that geospatially grounded agents can outperform baselines.
The proposed framework can generalize to other hazards such as floods and
hurricanes.

</details>


### [9] [ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization](https://arxiv.org/abs/2510.12063)
*Sunzhu Li,Zhiyu Lin,Shuling Yang,Jiale Zhao,Wei Chen*

Main category: cs.AI

TL;DR: ThinkPilot是一个无需训练即可自动优化大型推理模型推理能力的框架，通过进化过程生成think-prefixes指令来引导模型实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然强大，但存在推理效率低下和偏离目标的问题。现有的免训练方法要么基于僵硬的启发式规则，要么提供描述性但不可操作的分析，无法有效优化模型推理。

Method: 使用进化过程生成think-prefixes指令，这些指令在推理行为分类学的驱动下演化，引导模型朝着更优性能发展。

Result: 实验表明ThinkPilot显著改善了准确率-长度权衡，大幅提升安全性（如将DeepSeek-R1-Distill-Qwen-32B的StrongREJECT分数从27.0%降至0.7），并增强了指令跟随能力，还能与现有基于训练的方法协同工作。

Conclusion: think-prefixes能够可靠地控制大型推理模型的推理行为，不同任务对特定行为分布有强烈偏好。通过自动识别和激发这些行为，ThinkPilot提供了一个可泛化的框架来使模型推理与任务需求对齐。

Abstract: Large Reasoning Models (LRMs) are powerful, but they still suffer from
inefficient and off-target reasoning. Currently, training-free methods are
limited to either rigid heuristics or descriptive, non-actionable analyses. In
this paper, we introduce ThinkPilot, a training-free framework that
automatically optimizes LRMs reasoning. It uses an evolutionary process to
generate think-prefixes, which are instructions that evolve driven by a
taxonomy of reasoning behaviors to guide models toward superior performance.
Extensive experiments demonstrate ThinkPilot's broad effectiveness: it
significantly improves the accuracy-length trade-off for efficient reasoning,
drastically improves safety (for example, cutting the StrongREJECT score of
DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction
following. It also synergizes with existing training-based methods. Our
analysis reveals that think-prefixes can reliably control LRMs' reasoning
behaviors, and that different tasks have strong preferences for specific
behavioral distributions. By automatically identifying and eliciting these
behaviors, ThinkPilot provides a generalizable framework for aligning LRMs
reasoning with task demands. Data and code are available at
https://github.com/teqkilla/ThinkPilot

</details>


### [10] [AI Agents as Universal Task Solvers](https://arxiv.org/abs/2510.12066)
*Alessandro Achille,Stefano Soatto*

Main category: cs.AI

TL;DR: 本文重新诠释了AI智能体中的学习角色，将其视为具有计算能力的随机动态系统，强调时间在学习推理中的关键作用，并提出从归纳学习转向转导学习，以算法结构捕获来减少解决新任务的时间。


<details>
  <summary>Details</summary>
Motivation: 探讨AI推理智能体是否能成为通用计算器，链式思维推理是否能解决任何可计算任务，以及AI智能体如何学习推理——是模型规模还是训练数据集规模的问题。

Method: 将AI智能体重新解释为计算能力的随机动态系统，提出转导学习框架，关注算法结构捕获而非数据分布近似，分析信息在减少时间而非重建误差中的作用。

Result: 理论推导了推理时间与训练时间之间的幂律缩放关系，证明大模型在无限空间和时间下可能成为仅能暴力求解任务的"专家"而非真正智能体。

Conclusion: 优化推理模型的关键量是时间而非模型规模，时间在学习中的关键作用此前仅被间接考虑，转导学习应成为主要关注点。

Abstract: AI reasoning agents are already able to solve a variety of tasks by deploying
tools, simulating outcomes of multiple hypotheses and reflecting on them. In
doing so, they perform computation, although not in the classical sense --
there is no program being executed. Still, if they perform computation, can AI
agents be universal? Can chain-of-thought reasoning solve any computable task?
How does an AI Agent learn to reason? Is it a matter of model size? Or training
dataset size?
  In this work, we reinterpret the role of learning in the context of AI
Agents, viewing them as compute-capable stochastic dynamical systems, and
highlight the role of time in a foundational principle for learning to reason.
In doing so, we propose a shift from classical inductive learning to
transductive learning -- where the objective is not to approximate the
distribution of past data, but to capture their algorithmic structure to reduce
the time needed to find solutions to new tasks.
  Transductive learning suggests that, counter to Shannon's theory, a key role
of information in learning is about reduction of time rather than
reconstruction error. In particular, we show that the optimal speed-up that a
universal solver can achieve using past data is tightly related to their
algorithmic information. Using this, we show a theoretical derivation for the
observed power-law scaling of inference time versus training time. We then show
that scaling model size can lead to behaviors that, while improving accuracy on
benchmarks, fail any reasonable test of intelligence, let alone
super-intelligence: In the limit of infinite space and time, large models can
behave as savants, able to brute-force through any task without any insight.
Instead, we argue that the key quantity to optimize when scaling reasoning
models is time, whose critical role in learning has so far only been indirectly
considered.

</details>


### [11] [HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory](https://arxiv.org/abs/2510.12067)
*Junyi Xie,Yuankun Jiao,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: HiCoTraj是一个利用大语言模型进行零样本人口属性推理的框架，通过将轨迹转化为语义丰富的自然语言表示，并使用分层思维链推理来推断年龄、性别、收入等人口属性。


<details>
  <summary>Details</summary>
Motivation: 现有基于移动轨迹的人口属性推断方法严重依赖带标签的大规模轨迹数据，导致可解释性差且在不同数据集和用户群体间泛化能力不足。

Method: 将轨迹转化为详细活动记录和多尺度访问摘要的自然语言表示，然后使用分层思维链推理（事实特征提取、行为模式分析、结构化输出的人口属性推断）来指导LLMs。

Result: 在真实世界轨迹数据上的实验表明，HiCoTraj在零样本场景下对多个人口属性实现了具有竞争力的性能。

Conclusion: HiCoTraj解决了带标签人口数据稀缺的挑战，同时提供了透明的推理链，在零样本人口属性推断方面表现出色。

Abstract: Inferring demographic attributes such as age, sex, or income level from human
mobility patterns enables critical applications such as targeted public health
interventions, equitable urban planning, and personalized transportation
services. Existing mobility-based demographic inference studies heavily rely on
large-scale trajectory data with demographic labels, leading to limited
interpretability and poor generalizability across different datasets and user
groups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical
Chain-of-Thought Prompting from Trajectory), a framework that leverages LLMs'
zero-shot learning and semantic understanding capabilities to perform
demographic inference without labeled training data. HiCoTraj transforms
trajectories into semantically rich, natural language representations by
creating detailed activity chronicles and multi-scale visiting summaries. Then
HiCoTraj uses a novel hierarchical chain of thought reasoning to systematically
guide LLMs through three cognitive stages: factual feature extraction,
behavioral pattern analysis, and demographic inference with structured output.
This approach addresses the scarcity challenge of labeled demographic data
while providing transparent reasoning chains. Experimental evaluation on
real-world trajectory data demonstrates that HiCoTraj achieves competitive
performance across multiple demographic attributes in zero-shot scenarios.

</details>


### [12] [EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making](https://arxiv.org/abs/2510.12072)
*Zixing Lei,Sheng Yin,Yichen Xiong,Yuanzhuo Ding,Wenhao Huang,Yuxi Wei,Qingyao Xu,Yiming Li,Weixin Li,Yunhong Wang,Siheng Chen*

Main category: cs.AI

TL;DR: EmboMatrix是首个为LLM提供真实具身决策技能的训练场，通过大规模任务模拟和交互学习，培养出EmboBrain模型，在具身决策基准上超越671B参数的DeepSeek-R1模型9.5%。


<details>
  <summary>Details</summary>
Motivation: 解决LLM仅基于语言训练而缺乏物理环境暴露的问题，需要让LLM获得真正的具身理解能力，实现从高级目标到可执行动作的具身决策。

Method: 构建EmboMatrix训练场，包含多智能体数据引擎生成大规模任务场景、分布式异构硬件系统实现可扩展模拟、多级奖励架构提供精确监督。

Result: 基于EmboMatrix训练的EmboBrain-7B模型在两个具身决策基准上超越671B参数的DeepSeek-R1基线9.5%，证明了交互式环境学习的力量。

Conclusion: 通过交互式、环境基础的学习方法，可以构建真正智能的具身智能体，EmboMatrix为LLM获得具身决策能力提供了有效解决方案。

Abstract: Embodied decision-making enables agents to translate high-level goals into
executable actions through continuous interactions within the physical world,
forming a cornerstone of general-purpose embodied intelligence. Large language
models (LLMs), with their general decision-making capabilities, offer a
promising path to realize this potential; however, LLMs trained solely on
language lack exposure to physical environments, limiting their true embodied
understanding. To bridge this gap, we propose the concept of a training ground:
a comprehensive infrastructure that provides task and scene simulation,
embodied interaction, and feedback signals, offering a one-stop solution for
LLM acquire genuine embodied decision-making skills. In this work, we present
EmboMatrix, the first training ground of its kind, providing massive and
diverse tasks with efficient simulation and precise rewards. EmboMatrix
incorporates a series of novel techniques: a multi-agent data engine for
large-scale task and scene generation, a distributed heterogeneous-hardware
system for scalable simulation, and a multi-level reward architecture for
precise supervision. Leveraging EmboMatrix, we cultivate EmboBrain, an LLM
whose embodied decision-making abilities emerge from extensive embodied
interactions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1
baseline by 9.5\% on two challenging embodied decision-making benchmarks,
demonstrating the power of interactive, environment-grounded learning for
building truly intelligent embodied agents.

</details>


### [13] [BeSTAD: Behavior-Aware Spatio-Temporal Anomaly Detection for Human Mobility Data](https://arxiv.org/abs/2510.12076)
*Junyi Xie,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: BeSTAD是一个无监督框架，通过联合建模空间上下文和时间动态来检测人类移动数据中的个体级异常，能够在大规模人群中捕捉个体化行为特征并发现细粒度异常。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测主要关注轨迹级分析，但在大规模数据集中检测个体级异常（即个人相对于自身历史模式的不寻常偏差）仍然是一个重大挑战。

Method: BeSTAD学习语义丰富的移动表示，整合位置含义和时间模式，采用行为聚类感知建模机制构建个性化行为档案，并通过跨时期行为比较识别异常。

Result: 该方法能够检测行为转变和与既定常规的偏差，并在大规模移动数据集中识别表现出此类变化的个体。

Conclusion: 通过直接从无标签数据学习个体行为，BeSTAD将异常检测推向个性化和可解释的移动分析。

Abstract: Traditional anomaly detection in human mobility has primarily focused on
trajectory-level analysis, identifying statistical outliers or spatiotemporal
inconsistencies across aggregated movement traces. However, detecting
individual-level anomalies, i.e., unusual deviations in a person's mobility
behavior relative to their own historical patterns, within datasets
encompassing large populations remains a significant challenge. In this paper,
we present BeSTAD (Behavior-aware Spatio-Temporal Anomaly Detection for Human
Mobility Data), an unsupervised framework that captures individualized
behavioral signatures across large populations and uncovers fine-grained
anomalies by jointly modeling spatial context and temporal dynamics. BeSTAD
learns semantically enriched mobility representations that integrate location
meaning and temporal patterns, enabling the detection of subtle deviations in
individual movement behavior. BeSTAD further employs a behavior-cluster-aware
modeling mechanism that builds personalized behavioral profiles from normal
activity and identifies anomalies through cross-period behavioral comparison
with consistent semantic alignment. Building on prior work in mobility behavior
clustering, this approach enables not only the detection of behavioral shifts
and deviations from established routines but also the identification of
individuals exhibiting such changes within large-scale mobility datasets. By
learning individual behaviors directly from unlabeled data, BeSTAD advances
anomaly detection toward personalized and interpretable mobility analysis.

</details>


### [14] [Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models](https://arxiv.org/abs/2510.12080)
*Rabimba Karanjai,Yang Lu,Ranjith Chodavarapu,Lei Xu,Weidong Shi*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型处理随机性任务的能力，发现虽然LLM能产生一定随机性的输出，但表现不稳定且与预期行为存在显著偏差。


<details>
  <summary>Details</summary>
Motivation: 随着LLM技术的快速发展，许多应用需要随机性，但LLM在处理随机数生成和使用方面的能力尚不清楚。

Method: 设计了一系列实验，考虑外部工具可访问性、任务类型、模型状态和提示策略等因素，涵盖随机数生成、密码生成、项目洗牌等任务，并使用熵和NIST随机性测试套件评估随机性质量。

Result: LLM能够生成具有一定随机性的输出，但性能不一致，经常显著偏离预期行为。

Conclusion: 实验结果表明LLM在处理涉及随机性的任务时存在关键局限性，需要改进才能有效处理这类任务。

Abstract: The rapid advancement of large language model (LLM) technology has led to
diverse applications, many of which inherently require randomness, such as
stochastic decision-making, gaming, scheduling, AI agents, and
cryptography-related tasks. However, the capabilities of LLMs in handling
randomness, particularly in generating and utilizing random numbers
effectively, remain unclear. This paper investigates the capacity of LLMs for
handling tasks that involve randomness through a series of experiments. We
designed a set of experiments that consider various factors that can influence
an LLM's performance in tasks involving randomness, such as accessibility to
external tools, types of tasks, model states (fresh vs. non-fresh), and
prompting strategies. The experiments cover a range of tasks, including
generating random numbers, generating random strings such as passwords,
shuffling items, and evaluating the quality of randomness using entropy and the
NIST randomness test-suite. Our findings reveal that while LLMs can generate
outputs that exhibit some degree of randomness, their performance is
inconsistent and often deviates significantly from the expected behavior. The
analysis of the experimental results highlights key limitations and areas where
improvement is needed for the LLMs to effectively handle tasks involving
randomness

</details>


### [15] [One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration](https://arxiv.org/abs/2510.12088)
*Zaid Khan,Archiki Prasad,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.AI

TL;DR: OneLife框架通过条件激活的程序化法则在概率编程框架中建模世界动态，能够在复杂随机环境中从有限的非指导性交互中学习关键环境动态。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂、随机环境中，智能体只有"一次生命"来探索敌对环境且无人指导的更具挑战性的现实场景。

Method: 使用条件激活的程序化法则，每个法则通过前提条件-效果结构在相关世界状态中激活，创建动态计算图，仅通过相关法则路由推理和优化。

Result: 在23个测试场景中的16个场景上优于强基线，能够从最小化、非指导性交互中成功学习关键环境动态。

Conclusion: 为自主构建未知复杂环境的程序化世界模型奠定了基础。

Abstract: Symbolic world modeling requires inferring and representing an environment's
transitional dynamics as an executable program. Prior work has focused on
largely deterministic environments with abundant interaction data, simple
mechanics, and human guidance. We address a more realistic and challenging
setting, learning in a complex, stochastic environment where the agent has only
"one life" to explore a hostile environment without human guidance. We
introduce OneLife, a framework that models world dynamics through
conditionally-activated programmatic laws within a probabilistic programming
framework. Each law operates through a precondition-effect structure,
activating in relevant world states. This creates a dynamic computation graph
that routes inference and optimization only through relevant laws, avoiding
scaling challenges when all laws contribute to predictions about a complex,
hierarchical state, and enabling the learning of stochastic dynamics even with
sparse rule activation. To evaluate our approach under these demanding
constraints, we introduce a new evaluation protocol that measures (a) state
ranking, the ability to distinguish plausible future states from implausible
ones, and (b) state fidelity, the ability to generate future states that
closely resemble reality. We develop and evaluate our framework on Crafter-OO,
our reimplementation of the Crafter environment that exposes a structured,
object-oriented symbolic state and a pure transition function that operates on
that state alone. OneLife can successfully learn key environment dynamics from
minimal, unguided interaction, outperforming a strong baseline on 16 out of 23
scenarios tested. We also test OneLife's planning ability, with simulated
rollouts successfully identifying superior strategies. Our work establishes a
foundation for autonomously constructing programmatic world models of unknown,
complex environments.

</details>


### [16] [ToPolyAgent: AI Agents for Coarse-Grained Topological Polymer Simulations](https://arxiv.org/abs/2510.12091)
*Lijie Ding,Jan-Michael Carrillo,Changwoo Do*

Main category: cs.AI

TL;DR: ToPolyAgent是一个多智能体AI框架，通过自然语言指令执行拓扑聚合物的粗粒度分子动力学模拟，集成了大语言模型和领域专用计算工具。


<details>
  <summary>Details</summary>
Motivation: 降低复杂计算工作流的门槛，推进聚合物科学中AI驱动的材料发现，为自主和可扩展的多智能体科学研究生态系统奠定基础。

Method: 系统包含四个LLM驱动的智能体：配置智能体生成初始聚合物-溶剂配置，模拟智能体执行LAMMPS分子动力学模拟和构象分析，报告智能体编译markdown报告，工作流智能体实现流线化自主操作。支持交互式和自主两种模式。

Result: 通过案例研究展示了系统在多种聚合物架构、溶剂条件、恒温器和模拟时长下的多功能性，并成功研究了相互作用参数对线性聚合物构象的影响以及接枝密度对刷状聚合物持久长度的影响。

Conclusion: 通过将自然语言界面与严格的模拟工具相结合，ToPolyAgent为聚合物科学中的AI驱动材料发现提供了有力工具，并为自主可扩展的多智能体科学研究生态系统奠定了基础。

Abstract: We introduce ToPolyAgent, a multi-agent AI framework for performing
coarse-grained molecular dynamics (MD) simulations of topological polymers
through natural language instructions. By integrating large language models
(LLMs) with domain-specific computational tools, ToPolyAgent supports both
interactive and autonomous simulation workflows across diverse polymer
architectures, including linear, ring, brush, and star polymers, as well as
dendrimers. The system consists of four LLM-powered agents: a Config Agent for
generating initial polymer-solvent configurations, a Simulation Agent for
executing LAMMPS-based MD simulations and conformational analyses, a Report
Agent for compiling markdown reports, and a Workflow Agent for streamlined
autonomous operations. Interactive mode incorporates user feedback loops for
iterative refinements, while autonomous mode enables end-to-end task execution
from detailed prompts. We demonstrate ToPolyAgent's versatility through case
studies involving diverse polymer architectures under varying solvent
condition, thermostats, and simulation lengths. Furthermore, we highlight its
potential as a research assistant by directing it to investigate the effect of
interaction parameters on the linear polymer conformation, and the influence of
grafting density on the persistence length of the brush polymer. By coupling
natural language interfaces with rigorous simulation tools, ToPolyAgent lowers
barriers to complex computational workflows and advances AI-driven materials
discovery in polymer science. It lays the foundation for autonomous and
extensible multi-agent scientific research ecosystems.

</details>


### [17] [Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing](https://arxiv.org/abs/2510.12121)
*Rongzhi Zhang,Liqin Ye,Yuzhao Heng,Xiang Chen,Tong Yu,Lingkai Kong,Sudheer Chava,Chao Zhang*

Main category: cs.AI

TL;DR: 提出了一种精确控制LLM输出属性强度的方法，通过目标达成问题重构、轻量级价值函数训练和基于梯度的隐表示干预，实现了对属性强度的细粒度连续控制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法只能提供方向性或开放式指导，无法可靠实现精确的属性强度控制，限制了AI系统适应不同用户期望的能力。

Method: 1) 将精确属性强度控制重新定义为目标达成问题；2) 通过时序差分学习训练轻量级价值函数来预测部分生成内容的最终属性强度；3) 使用基于梯度的隐表示干预来精确导航模型达到特定属性强度目标。

Result: 在LLaMA-3.2-3b和Phi-4-mini上的实验证实了该方法能够以高精度将文本生成引导至用户指定的属性强度。

Conclusion: 该方法超越了简单的方向对齐，实现了对属性强度的精细控制，并在偏好数据合成、帕累托前沿近似优化和无干预推理的对齐行为蒸馏等下游任务中展示了效率提升。

Abstract: Precise attribute intensity control--generating Large Language Model (LLM)
outputs with specific, user-defined attribute intensities--is crucial for AI
systems adaptable to diverse user expectations. Current LLM alignment methods,
however, typically provide only directional or open-ended guidance, failing to
reliably achieve exact attribute intensities. We address this limitation with
three key designs: (1) reformulating precise attribute intensity control as a
target-reaching problem, rather than simple maximization; (2) training a
lightweight value function via temporal-difference learning to predict final
attribute intensity scores from partial generations, thereby steering LLM
outputs; and (3) employing gradient-based interventions on hidden
representations to navigate the model precisely towards specific attribute
intensity targets. Our method enables fine-grained, continuous control over
attribute intensities, moving beyond simple directional alignment. Experiments
on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text
generation to user-specified attribute intensities with high accuracy. Finally,
we demonstrate efficiency enhancements across three downstream tasks:
preference data synthesis, Pareto frontier approximation and optimization, and
distillation of aligned behaviors for intervention-free inference. Our code is
available on https://github.com/Pre-Control/pre-control

</details>


### [18] [MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science](https://arxiv.org/abs/2510.12171)
*Junkai Zhang,Jingru Gan,Xiaoxuan Wang,Zian Jia,Changquan Gu,Jianpeng Chen,Yanqiao Zhu,Mingyu Derek Ma,Dawei Zhou,Ling Li,Wei Wang*

Main category: cs.AI

TL;DR: MatSciBench是一个包含1340个大学水平材料科学问题的综合基准，涵盖6个主要领域和31个子领域，具有三级难度分类和多模态推理能力。评估显示即使最佳模型准确率也不到80%，不同推理策略在不同场景下表现各异。


<details>
  <summary>Details</summary>
Motivation: 填补大型语言模型在材料科学推理能力评估方面的空白，建立全面的基准来推动LLMs在科学推理能力上的改进。

Method: 创建MatSciBench基准，包含1340个结构化分类的材料科学问题，采用三级难度分类，整合多模态推理，并评估不同推理策略（基础思维链、工具增强、自我纠正）的效果。

Result: 最佳模型Gemini-2.5-Pro在材料科学问题上准确率低于80%，不同推理策略没有单一方法在所有场景中表现一致，多模态推理任务存在挑战。

Conclusion: MatSciBench为评估和提升LLMs在材料科学领域的科学推理能力建立了全面可靠的基准，揭示了当前模型在该领域的局限性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities in
scientific reasoning, yet their reasoning capabilities in materials science
remain underexplored. To fill this gap, we introduce MatSciBench, a
comprehensive college-level benchmark comprising 1,340 problems that span the
essential subdisciplines of materials science. MatSciBench features a
structured and fine-grained taxonomy that categorizes materials science
questions into 6 primary fields and 31 sub-fields, and includes a three-tier
difficulty classification based on the reasoning length required to solve each
question. MatSciBench provides detailed reference solutions enabling precise
error analysis and incorporates multimodal reasoning through visual contexts in
numerous questions. Evaluations of leading models reveal that even the
highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on
college-level materials science questions, highlighting the complexity of
MatSciBench. Our systematic analysis of different reasoning strategie--basic
chain-of-thought, tool augmentation, and self-correction--demonstrates that no
single method consistently excels across all scenarios. We further analyze
performance by difficulty level, examine trade-offs between efficiency and
accuracy, highlight the challenges inherent in multimodal reasoning tasks,
analyze failure modes across LLMs and reasoning methods, and evaluate the
influence of retrieval-augmented generation. MatSciBench thus establishes a
comprehensive and solid benchmark for assessing and driving improvements in the
scientific reasoning capabilities of LLMs within the materials science domain.

</details>


### [19] [Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey](https://arxiv.org/abs/2510.12178)
*Abdulhady Abas Abdullah,Arkaitz Zubiaga,Seyedali Mirjalili,Amir H. Gandomi,Fatemeh Daneshfar,Mohammadsadra Amini,Alan Salam Mohammed,Hadi Veisi*

Main category: cs.AI

TL;DR: 本文综述了Meta AI的LLaMA系列模型从LLaMA 1到LLaMA 4的演进，以及为这些模型开发的参数高效微调(PEFT)方法，包括LoRA、LLaMA-Adapter V1/V2、LLaMA-Excitor和QLoRA等。


<details>
  <summary>Details</summary>
Motivation: 随着LLaMA系列模型的快速发展，需要系统梳理这些基础模型及其高效微调方法，为研究者和实践者提供一站式资源。

Method: 首先描述LLaMA系列基础模型(7B-65B到288B参数)的架构和性能特征，然后介绍PEFT概念，详细分析五种应用于LLaMA的PEFT方法及其机制、参数节省和应用案例。

Result: 提供了模型和适配器架构、参数数量和基准测试结果的系统分析，展示了微调后的LLaMA模型在某些情况下优于更大基线模型的例子，并探讨了在医疗、法律等领域的实际应用案例。

Conclusion: LLaMA模型和PEFT方法在现实应用中取得了成功，但仍面临扩展到更大上下文和提升鲁棒性等挑战，为未来研究指明了方向。

Abstract: This review surveys the rapid evolution of Meta AI's LLaMA (Large Language
Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized
parameter-efficient fine-tuning (PEFT) methods developed for these models. We
first describe the LLaMA family of foundation models (7B-65B to 288B
parameters), their architectures (including native multimodal and
Mixtureof-Experts variants), and key performance characteristics. We then
describe and discuss the concept of PEFT, which adapts large pre-trained models
by updating only a small subset of parameters, and review five PEFT methods
that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1
and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's
mechanism, parameter savings, and example application to LLaMA (e.g.,
instruction tuning, multimodal tasks). We provide structured discussion and
analysis of model and adapter architectures, parameter counts, and benchmark
results (including examples where fine-tuned LLaMA models outperform larger
baselines). Finally, we examine real-world use cases where LLaMA-based models
and PEFT have been successfully applied (e.g., legal and medical domains), and
we discuss ongoing challenges and future research directions (such as scaling
to even larger contexts and improving robustness). This survey paper provides a
one-stop resource for ML researchers and practitioners interested in LLaMA
models and efficient fine-tuning strategies.

</details>


### [20] [ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents](https://arxiv.org/abs/2510.12194)
*Linyi Yang,Yixuan Weng*

Main category: cs.AI

TL;DR: ResearStudio是一个开源框架，通过"协作工作坊"设计实现实时人类控制的深度研究代理，支持AI主导、人类辅助和人类主导、AI辅助等多种模式切换。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理运行在"发射后不管"模式，用户无法在执行过程中修正错误或添加专家知识，需要实现实时人类控制。

Method: 采用分层规划器-执行器架构，将每个步骤写入实时"计划即文档"，通过快速通信层将每个动作、文件变更和工具调用流式传输到Web界面，用户可随时暂停、编辑计划或代码、运行自定义命令并恢复。

Result: 在完全自主模式下，ResearStudio在GAIA基准测试中达到最先进水平，超越了OpenAI的DeepResearch和Manus等系统。

Conclusion: 强大的自动化性能和细粒度人类控制可以共存，为安全可控的研究代理开发提供了新方向。

Abstract: Current deep-research agents run in a ''fire-and-forget'' mode: once started,
they give users no way to fix errors or add expert knowledge during execution.
We present ResearStudio, the first open-source framework that places real-time
human control at its core. The system follows a Collaborative Workshop design.
A hierarchical Planner-Executor writes every step to a live
''plan-as-document,'' a fast communication layer streams each action, file
change, and tool call to a web interface. At any moment, the user can pause the
run, edit the plan or code, run custom commands, and resume -- switching
smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In
fully autonomous mode, ResearStudio achieves state-of-the-art results on the
GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These
results show that strong automated performance and fine-grained human control
can coexist. The full code, protocol, and evaluation scripts are available at
https://github.com/ResearAI/ResearStudio. We will continue to update the
repository to encourage further work on safe and controllable research agents.
Our live demo is publicly accessible at http://ai-researcher.net:3000/. We
support the development of DeepScientist, which can be accessed at
https://github.com/ResearAI/DeepScientist.

</details>


### [21] [On the Design and Evaluation of Human-centered Explainable AI Systems: A Systematic Review and Taxonomy](https://arxiv.org/abs/2510.12201)
*Aline Mangold,Juliane Zietz,Susanne Weinhold,Sebastian Pannasch*

Main category: cs.AI

TL;DR: 该论文对65个评估可解释AI(XAI)系统的用户研究进行了全面综述，提出了以人为中心的XAI系统设计目标和评估框架，并根据用户AI专业水平(新手vs专家)进行了差异化设计。


<details>
  <summary>Details</summary>
Motivation: 随着AI在日常生活中的普及，对既高性能又可理解的智能系统需求增加。目前XAI系统的评估过于技术化，缺乏对人本需求的关注，需要用户研究来指导XAI开发。

Method: 对65个XAI用户研究进行系统性综述分析，提出人本设计目标和评估指标框架，并根据用户AI专业水平进行差异化设计扩展。

Result: 区分了核心系统与XAI解释组件，将评估指标分为情感、认知、可用性、可解释性和解释指标五类，为AI新手和数据专家分别提出了不同的设计目标。

Conclusion: 该研究扩展了现有的XAI评估和设计框架，强调需要根据用户特征进行差异化设计，为XAI开发者提供了人本导向的系统设计和评估指南。

Abstract: As AI becomes more common in everyday living, there is an increasing demand
for intelligent systems that are both performant and understandable.
Explainable AI (XAI) systems aim to provide comprehensible explanations of
decisions and predictions. At present, however, evaluation processes are rather
technical and not sufficiently focused on the needs of human users.
Consequently, evaluation studies involving human users can serve as a valuable
guide for conducting user studies. This paper presents a comprehensive review
of 65 user studies evaluating XAI systems across different domains and
application contexts. As a guideline for XAI developers, we provide a holistic
overview of the properties of XAI systems and evaluation metrics focused on
human users (human-centered). We propose objectives for the human-centered
design (design goals) of XAI systems. To incorporate users' specific
characteristics, design goals are adapted to users with different levels of AI
expertise (AI novices and data experts). In this regard, we provide an
extension to existing XAI evaluation and design frameworks. The first part of
our results includes the analysis of XAI system characteristics. An important
finding is the distinction between the core system and the XAI explanation,
which together form the whole system. Further results include the distinction
of evaluation metrics into affection towards the system, cognition, usability,
interpretability, and explanation metrics. Furthermore, the users, along with
their specific characteristics and behavior, can be assessed. For AI novices,
the relevant extended design goals include responsible use, acceptance, and
usability. For data experts, the focus is performance-oriented and includes
human-AI collaboration and system and user task performance.

</details>


### [22] [GOAT: A Training Framework for Goal-Oriented Agent with Tools](https://arxiv.org/abs/2510.12218)
*Hyunji Min,Sangwon Jung,Junyoung Sung,Dosung Lee,Leekyeung Han,Paul Hongsuck Seo*

Main category: cs.AI

TL;DR: GOAT是一个无需人工标注的训练框架，能够从API文档自动构建合成数据集，用于微调LLM代理处理目标导向的API执行任务。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在处理需要分解高级目标为多个相互依赖API调用的目标导向查询时能力有限，且缺乏训练数据。开源模型在复杂工具使用方面表现不佳。

Method: 提出GOAT框架，从API文档自动构建目标导向API执行任务的合成数据集，训练模型进行相互依赖调用的推理并生成连贯响应。

Result: GOAT训练的代理在多个现有目标导向基准测试中达到最先进性能，并在新提出的GOATBench基准测试中表现出色。

Conclusion: GOAT为构建能够进行复杂推理和工具使用的稳健开源LLM代理提供了一条实用路径。

Abstract: Large language models (LLMs) have recently been extended beyond traditional
text generation to serve as interactive agents capable of using external tools
based on user intent. However, current LLM agents still show limited ability to
handle goal-oriented queries, which require decomposing a high-level objective
into multiple interdependent API calls with correct planning and execution.
Current approaches mainly rely on zero-shot evaluation due to the absence of
training data. While proprietary closed-source models such as GPT-4 demonstrate
strong reasoning abilities, smaller open-source models struggle to perform
complex tool use effectively. Thus, we propose a novel training framework GOAT,
which enables fine-tuning of LLM agents in a human annotation-free setting.
GOAT automatically constructs synthetic datasets of goal-oriented API execution
tasks directly from given API documents, equipping models with the ability to
reason over interdependent calls and generate coherent responses. Through
extensive experiments, we show that GOAT-trained agents achieve
state-of-the-art performance across multiple existing goal-oriented benchmarks.
In addition, we introduce GOATBench, a new goal-oriented API execution
benchmark, and demonstrate that agents trained with GOAT also excel in this
setting. These results highlight GOAT as a practical path toward building
robust open-source LLM agents capable of complex reasoning and tool use.

</details>


### [23] [MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs](https://arxiv.org/abs/2510.12224)
*Yuechun Yu,Han Ying,Haoan Jin,Wenjian Jiang,Dong Xian,Binghao Wang,Zhou Yang,Mengyue Wu*

Main category: cs.AI

TL;DR: 提出了MedKGEval，一个基于结构化医学知识的多轮临床LLM评估框架，通过知识图谱驱动的患者模拟和实时轮次评估来改进医学对话评估。


<details>
  <summary>Details</summary>
Motivation: 现有医学LLM评估方法主要依赖事后回顾完整对话记录，忽略了医学对话的动态性和上下文敏感性，无法捕捉真实临床环境中复杂的多轮医患互动。

Method: 1) 知识图谱驱动的患者模拟机制，从精心构建的知识图谱中检索医学事实；2) 实时轮次评估框架，由法官代理在对话过程中评估每个模型响应的临床适当性、事实准确性和安全性；3) 构建多轮基准测试八个最先进的LLM。

Result: MedKGEval能够识别传统评估流程经常忽略的细微行为缺陷和安全风险，展示了其在医学LLM评估中的有效性。

Conclusion: MedKGEval为医学LLM提供了更可靠的多轮评估框架，虽然最初针对中英文医学应用设计，但可通过切换知识图谱轻松扩展到其他语言，确保双语支持和领域特定适用性。

Abstract: The reliable evaluation of large language models (LLMs) in medical
applications remains an open challenge, particularly in capturing the
complexity of multi-turn doctor-patient interactions that unfold in real
clinical environments. Existing evaluation methods typically rely on post hoc
review of full conversation transcripts, thereby neglecting the dynamic,
context-sensitive nature of medical dialogues and the evolving informational
needs of patients. In this work, we present MedKGEval, a novel multi-turn
evaluation framework for clinical LLMs grounded in structured medical
knowledge. Our approach introduces three key contributions: (1) a knowledge
graph-driven patient simulation mechanism, where a dedicated control module
retrieves relevant medical facts from a curated knowledge graph, thereby
endowing the patient agent with human-like and realistic conversational
behavior. This knowledge graph is constructed by integrating open-source
resources with additional triples extracted from expert-annotated datasets; (2)
an in-situ, turn-level evaluation framework, where each model response is
assessed by a Judge Agent for clinical appropriateness, factual correctness,
and safety as the dialogue progresses using a suite of fine-grained,
task-specific metrics; (3) a comprehensive multi-turn benchmark of eight
state-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle
behavioral flaws and safety risks that are often overlooked by conventional
evaluation pipelines. Although initially designed for Chinese and English
medical applications, our framework can be readily extended to additional
languages by switching the input knowledge graphs, ensuring seamless bilingual
support and domain-specific applicability.

</details>


### [24] [PromptFlow: Training Prompts Like Neural Networks](https://arxiv.org/abs/2510.12246)
*Jingyi Wang,Hongyuan Zhu,Ye Niu,Yunhui Deng*

Main category: cs.AI

TL;DR: 提出了PromptFlow框架，通过模块化设计和强化学习来优化大语言模型的提示工程，实现自动化的提示优化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业领域部署时需要领域适配，手动提示设计费时费力，现有自动提示工程方法缺乏动态策略选择和细粒度编辑能力。

Method: 提出PromptFlow模块化训练框架，集成元提示、操作符、优化器和评估器，使用基于梯度的元学习和强化学习来回收经验。

Result: 在多个数据集上的实验证明了PromptFlow的有效性。

Conclusion: PromptFlow框架能够自动探索最优提示优化轨迹，显著提升大语言模型在专业任务上的表现。

Abstract: Large Language Models (LLMs) have demonstrated profound impact on Natural
Language Processing (NLP) tasks. However, their effective deployment across
diverse domains often require domain-specific adaptation strategies, as generic
models may underperform when faced with specialized data distributions. Recent
advances in prompt engineering (PE) offer a promising alternative to extensive
retraining by refining input instructions to align LLM outputs with task
objectives. This paradigm has emerged as a rapid and versatile approach for
model fine-tuning. Despite its potential, manual prompt design remains
labor-intensive and heavily depends on specialized expertise, often requiring
iterative human effort to achieve optimal formulations. To address this
limitation, automated prompt engineering methodologies have been developed to
systematically generate task-specific prompts. However, current implementations
predominantly employ static update rules and lack mechanisms for dynamic
strategy selection, resulting in suboptimal adaptation to varying NLP task
requirements. Furthermore, most methods treat and update the whole prompts at
each step, without considering editing prompt sections at a finer granularity.
At last, in particular, the problem of how to recycle experience in LLM is
still underexplored. To this end, we propose the PromptFlow, a modular training
framework inspired by TensorFlow, which integrates meta-prompts, operators,
optimization, and evaluator. Our framework can be equipped with the latest
optimization methods and autonomously explores optimal prompt refinement
trajectories through gradient-based meta-learning, requiring minimal
task-specific training data. Specifically, we devise a reinforcement learning
method to recycle experience for LLM in the PE process. Finally, we conduct
extensive experiments on various datasets, and demonstrate the effectiveness of
PromptFlow.

</details>


### [25] [$\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning](https://arxiv.org/abs/2510.12264)
*Deyu Zou,Yongqiang Chen,Jianxiang Wang,Haochen Yang,Mufei Li,James Cheng,Pan Li,Yu Gong*

Main category: cs.AI

TL;DR: 提出T³方法，通过检测信念偏差并截断训练轨迹，解决LLM智能体在主动推理中的信念偏差问题，提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在主动推理中容易产生信念偏差，导致问题状态跟踪失败和重复无效行为，而强化学习训练无法正确归因关键探索步骤。

Method: 开发T³方法，跟踪模型信念偏差，检测过度的信念偏差并在训练时截断轨迹，移除无信息尾部，保留信息前缀的信用。

Result: 在5个挑战性任务中，T³持续提升训练稳定性、令牌效率和最终性能，获得高达30%的性能增益，同时减少约25%的滚动令牌。

Conclusion: 信念控制是开发鲁棒和可泛化LLM主动推理器的关键原则。

Abstract: Active reasoning requires large language models (LLMs) to interact with
external sources and strategically gather information to solve problems.
Central to this process is belief tracking: maintaining a coherent
understanding of the problem state and the missing information toward the
solution. However, due to limited reasoning capabilities, LLM-based agents
often suffer from belief deviation: they struggle to correctly model beliefs,
lose track of problem states, and fall into uninformative or repetitive
actions. Once this happens, errors compound and reinforcement learning (RL)
training fails to properly credit the crucial exploratory steps. To address
this issue, we propose to track the deviation of model beliefs and develop
$\mathbf{T^3}$, a simple yet effective method that detects excessive belief
deviation and truncates trajectories during training to remove uninformative
tails. By preserving credit for informative prefixes, $\mathbf{T^3}$
systematically improves policy optimization. Across 5 challenging tasks,
$\mathbf{T^3}$ consistently enhances training stability, token efficiency, and
final performance, achieving up to 30% gains while cutting rollout tokens by
roughly 25%. These results highlight belief control as a key principle for
developing robust and generalizable LLM-based active reasoners.

</details>


### [26] [Tensor Logic: The Language of AI](https://arxiv.org/abs/2510.12269)
*Pedro Domingos*

Main category: cs.AI

TL;DR: 提出张量逻辑语言，统一神经和符号AI，通过张量方程作为唯一构造，将逻辑规则与爱因斯坦求和统一，实现可扩展的推理和学习。


<details>
  <summary>Details</summary>
Motivation: 现有AI工具如PyTorch缺乏自动推理和知识获取支持，而AI语言如LISP缺乏可扩展性和学习支持，需要统一解决方案。

Method: 基于张量方程构建张量逻辑语言，将逻辑规则与爱因斯坦求和视为相同操作，实现神经、符号和统计AI的统一表达。

Result: 成功实现了变换器、形式推理、核机器和图模型等关键AI形式的优雅实现，并支持嵌入空间中的可靠推理。

Conclusion: 张量逻辑结合了神经网络的可扩展性和符号推理的可靠性，为AI的广泛采用提供了潜在基础。

Abstract: Progress in AI is hindered by the lack of a programming language with all the
requisite features. Libraries like PyTorch and TensorFlow provide automatic
differentiation and efficient GPU implementation, but are additions to Python,
which was never intended for AI. Their lack of support for automated reasoning
and knowledge acquisition has led to a long and costly series of hacky attempts
to tack them on. On the other hand, AI languages like LISP an Prolog lack
scalability and support for learning. This paper proposes tensor logic, a
language that solves these problems by unifying neural and symbolic AI at a
fundamental level. The sole construct in tensor logic is the tensor equation,
based on the observation that logical rules and Einstein summation are
essentially the same operation, and all else can be reduced to them. I show how
to elegantly implement key forms of neural, symbolic and statistical AI in
tensor logic, including transformers, formal reasoning, kernel machines and
graphical models. Most importantly, tensor logic makes new directions possible,
such as sound reasoning in embedding space. This combines the scalability and
learnability of neural networks with the reliability and transparency of
symbolic reasoning, and is potentially a basis for the wider adoption of AI.

</details>


### [27] [RAG-Anything: All-in-One RAG Framework](https://arxiv.org/abs/2510.12323)
*Zirui Guo,Xubin Ren,Lingrui Xu,Jiahao Zhang,Chao Huang*

Main category: cs.AI

TL;DR: RAG-Anything是一个统一的多模态检索增强生成框架，能够处理文本、图像、表格和数学表达式等多种模态内容，解决了传统RAG仅限于文本的问题。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统仅限于文本内容，而现实世界中的知识库本质上是多模态的，包含丰富的文本、视觉元素、结构化表格和数学表达式组合。现有RAG框架与真实信息环境存在严重不匹配。

Method: 将多模态内容重新概念化为相互连接的知识实体，引入双图构建来捕获跨模态关系和文本语义，开发结合结构知识导航和语义匹配的跨模态混合检索方法。

Result: 在具有挑战性的多模态基准测试中表现出优越性能，相比最先进方法有显著提升，特别是在传统方法失败的长文档处理上表现尤为突出。

Conclusion: RAG-Anything为多模态知识访问建立了新范式，消除了当前系统的架构碎片化限制，实现了跨异构内容的有效推理。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm
for expanding Large Language Models beyond their static training limitations.
However, a critical misalignment exists between current RAG capabilities and
real-world information environments. Modern knowledge repositories are
inherently multimodal, containing rich combinations of textual content, visual
elements, structured tables, and mathematical expressions. Yet existing RAG
frameworks are limited to textual content, creating fundamental gaps when
processing multimodal documents. We present RAG-Anything, a unified framework
that enables comprehensive knowledge retrieval across all modalities. Our
approach reconceptualizes multimodal content as interconnected knowledge
entities rather than isolated data types. The framework introduces dual-graph
construction to capture both cross-modal relationships and textual semantics
within a unified representation. We develop cross-modal hybrid retrieval that
combines structural knowledge navigation with semantic matching. This enables
effective reasoning over heterogeneous content where relevant evidence spans
multiple modalities. RAG-Anything demonstrates superior performance on
challenging multimodal benchmarks, achieving significant improvements over
state-of-the-art methods. Performance gains become particularly pronounced on
long documents where traditional approaches fail. Our framework establishes a
new paradigm for multimodal knowledge access, eliminating the architectural
fragmentation that constrains current systems. Our framework is open-sourced
at: https://github.com/HKUDS/RAG-Anything.

</details>


### [28] [O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis](https://arxiv.org/abs/2510.12350)
*Ayush Khaitan,Vijay Ganesh*

Main category: cs.AI

TL;DR: 提出LLM+CAS框架和O-Forge工具，将前沿大语言模型与计算机代数系统结合，通过符号反馈循环产生既有创造性又经过符号验证的证明，特别针对渐近不等式证明问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在数学研究中验证困难的问题，证明可能看起来合理但缺乏严格验证，需要结合符号验证系统来增强可信度。

Method: 使用LLM+CAS框架，通过上下文符号反馈循环，让LLM提出域分解建议，CAS（如Mathematica）对每个部分进行公理化验证。

Result: 该框架在提出适当域分解方面表现出色，成功回答了Terence Tao关于LLM与验证器结合能否证明复杂渐近不等式的问题。

Conclusion: LLM+CAS框架展示了AI如何超越竞赛数学，成为专业数学家的研究级工具，推动AI在数学研究中的应用。

Abstract: Large language models have recently demonstrated advanced capabilities in
solving IMO and Putnam problems; yet their role in research mathematics has
remained fairly limited. The key difficulty is verification: suggested proofs
may look plausible, but cannot be trusted without rigorous checking. We present
a framework, called LLM+CAS, and an associated tool, O-Forge, that couples
frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic
Feedback loop to produce proofs that are both creative and symbolically
verified. Our focus is on asymptotic inequalities, a topic that often involves
difficult proofs and appropriate decomposition of the domain into the "right"
subdomains. Many mathematicians, including Terry Tao, have suggested that using
AI tools to find the right decompositions can be very useful for research-level
asymptotic analysis. In this paper, we show that our framework LLM+CAS turns
out to be remarkably effective at proposing such decompositions via a
combination of a frontier LLM and a CAS. More precisely, we use an LLM to
suggest domain decomposition, and a CAS (such as Mathematica) that provides a
verification of each piece axiomatically. Using this loop, we answer a question
posed by Terence Tao: whether LLMs coupled with a verifier can be used to help
prove intricate asymptotic inequalities. More broadly, we show how AI can move
beyond contest math towards research-level tools for professional
mathematicians.

</details>


### [29] [A Survey of Vibe Coding with Large Language Models](https://arxiv.org/abs/2510.12399)
*Yuyao Ge,Lingrui Mei,Zenghao Duan,Tianhao Li,Yujia Zheng,Yiwei Wang,Lexin Wang,Jiayu Yao,Tianyu Liu,Yujun Cai,Baolong Bi,Fangda Guo,Jiafeng Guo,Shenghua Liu,Xueqi Cheng*

Main category: cs.AI

TL;DR: 这篇论文对基于大语言模型的Vibe Coding范式进行了首次系统性综述，建立了理论框架并分析了五种开发模式，揭示了成功Vibe Coding的关键因素。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，从代码生成辅助转向自主编码代理，催生了'Vibe Coding'新开发范式，但其有效性尚未充分探索，存在生产力损失和人类-AI协作挑战。

Method: 通过系统分析1000多篇研究论文，建立了约束马尔可夫决策过程理论框架，将Vibe Coding形式化，并综合出五种开发模式：无约束自动化、迭代对话协作、规划驱动、测试驱动和上下文增强模型。

Result: 分析表明成功的Vibe Coding不仅依赖代理能力，更需要系统性的上下文工程、完善的开发环境和人机协作开发模式。

Conclusion: Vibe Coding作为一个正式学科，其成功实施需要理论框架、系统分类和关键基础设施组件的综合支持，为这一变革性开发方法提供了理论基础和实践指导。

Abstract: The advancement of large language models (LLMs) has catalyzed a paradigm
shift from code generation assistance to autonomous coding agents, enabling a
novel development methodology termed "Vibe Coding" where developers validate
AI-generated implementations through outcome observation rather than
line-by-line code comprehension. Despite its transformative potential, the
effectiveness of this emergent paradigm remains under-explored, with empirical
evidence revealing unexpected productivity losses and fundamental challenges in
human-AI collaboration. To address this gap, this survey provides the first
comprehensive and systematic review of Vibe Coding with large language models,
establishing both theoretical foundations and practical frameworks for this
transformative development approach. Drawing from systematic analysis of over
1000 research papers, we survey the entire vibe coding ecosystem, examining
critical infrastructure components including LLMs for coding, LLM-based coding
agent, development environment of coding agent, and feedback mechanisms. We
first introduce Vibe Coding as a formal discipline by formalizing it through a
Constrained Markov Decision Process that captures the dynamic triadic
relationship among human developers, software projects, and coding agents.
Building upon this theoretical foundation, we then synthesize existing
practices into five distinct development models: Unconstrained Automation,
Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and
Context-Enhanced Models, thus providing the first comprehensive taxonomy in
this domain. Critically, our analysis reveals that successful Vibe Coding
depends not merely on agent capabilities but on systematic context engineering,
well-established development environments, and human-agent collaborative
development models.

</details>


### [30] [PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks](https://arxiv.org/abs/2510.12409)
*Yunuo Liu,Dawei Zhu,Zena Al-Khalili,Dai Cheng,Yanjun Chen,Dietrich Klakow,Wei Zhang,Xiaoyu Shen*

Main category: cs.AI

TL;DR: PricingLogic是首个评估大语言模型在旅游定价场景中可靠性的基准测试，包含300个基于真实定价政策的问题，测试显示LLMs在复杂定价规则下存在系统性失败。


<details>
  <summary>Details</summary>
Motivation: 旅游机构希望将易出错的定价任务自动化给AI系统，但未经验证的LLM部署可能导致重大财务损失和客户信任危机。

Method: 构建包含300个自然语言问题的基准测试，基于42个真实定价政策，分为两个难度级别：基础客户类型定价和涉及交互折扣的捆绑旅游计算。

Result: 对一系列LLMs的评估显示，在较难层级上性能急剧下降，暴露出规则解释和算术推理的系统性失败。

Conclusion: 尽管具备通用能力，但当今的LLMs在收入关键应用中仍不可靠，需要进一步的安全保障或领域适应。

Abstract: We present PricingLogic, the first benchmark that probes whether Large
Language Models(LLMs) can reliably automate tourism-related prices when
multiple, overlapping fare rules apply. Travel agencies are eager to offload
this error-prone task onto AI systems; however, deploying LLMs without verified
reliability could result in significant financial losses and erode customer
trust. PricingLogic comprises 300 natural-language questions based on booking
requests derived from 42 real-world pricing policies, spanning two levels of
difficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations
involving interacting discounts. Evaluations of a line of LLMs reveal a steep
performance drop on the harder tier,exposing systematic failures in rule
interpretation and arithmetic reasoning.These results highlight that, despite
their general capabilities, today's LLMs remain unreliable in revenue-critical
applications without further safeguards or domain adaptation. Our code and
dataset are available at https://github.com/EIT-NLP/PricingLogic.

</details>


### [31] [MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for Exploring Echo Chamber Dynamics](https://arxiv.org/abs/2510.12423)
*Dingyi Zuo,Hongjie Zhang,Jie Ou,Chaosheng Feng,Shuwan Liu*

Main category: cs.AI

TL;DR: 提出了MTOS框架，将多主题情境与LLMs结合，通过记忆机制、交互策略和信念衰减来模拟跨主题观点演化，相比传统数值模型能更真实地捕捉人类推理和语言特征。


<details>
  <summary>Details</summary>
Motivation: 现实社交网络中信息常涉及多个相关主题，现有研究多关注单一主题，无法捕捉跨领域认知转移；传统数值模型将复杂语言态度简化为离散值，缺乏解释性和行为一致性。

Method: MTOS框架整合LLMs与短期/长期记忆，采用多用户选择交互机制、动态主题选择策略和信念衰减机制，支持跨主题视角更新。

Result: 多主题设置显著改变极化趋势：正相关主题强化回音室，负相关主题抑制回音室，无关主题通过资源竞争缓解回音室效应；相比数值模型，LLM代理能更真实模拟动态观点变化，再现新闻文本语言特征。

Conclusion: MTOS框架通过多主题情境和LLMs的结合，提高了模拟的解释性和系统稳定性，能更好地捕捉复杂的人类推理过程。

Abstract: The polarization of opinions, information segregation, and cognitive biases
on social media have attracted significant academic attention. In real-world
networks, information often spans multiple interrelated topics, posing
challenges for opinion evolution and highlighting the need for frameworks that
simulate interactions among topics. Existing studies based on large language
models (LLMs) focus largely on single topics, limiting the capture of cognitive
transfer in multi-topic, cross-domain contexts. Traditional numerical models,
meanwhile, simplify complex linguistic attitudes into discrete values, lacking
interpretability, behavioral consistency, and the ability to integrate multiple
topics. To address these issues, we propose Multi-topic Opinion Simulation
(MTOS), a social simulation framework integrating multi-topic contexts with
LLMs. MTOS leverages LLMs alongside short-term and long-term memory,
incorporates multiple user-selection interaction mechanisms and dynamic
topic-selection strategies, and employs a belief decay mechanism to enable
perspective updates across topics. We conduct extensive experiments on MTOS,
varying topic numbers, correlation types, and performing ablation studies to
assess features such as group polarization and local consistency. Results show
that multi-topic settings significantly alter polarization trends: positively
correlated topics amplify echo chambers, negatively correlated topics inhibit
them, and irrelevant topics also mitigate echo chamber effects through resource
competition. Compared with numerical models, LLM-based agents realistically
simulate dynamic opinion changes, reproduce linguistic features of news texts,
and capture complex human reasoning, improving simulation interpretability and
system stability.

</details>


### [32] [Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized Intersections](https://arxiv.org/abs/2510.12428)
*Chengyang Dong,Nan Guo*

Main category: cs.AI

TL;DR: 提出一种基于深度强化学习的无信号交叉口决策框架，结合偏置注意力机制构建交通风险预测器，通过SAC算法实现主动安全控制。


<details>
  <summary>Details</summary>
Motivation: 无信号交叉口自动驾驶决策面临复杂动态交互和高冲突风险挑战，需要实现主动安全控制。

Method: 基于Soft Actor-Critic算法构建DRL框架，使用偏置注意力机制构建交通风险预测器，将碰撞风险转化为密集奖励信号指导决策。

Result: 仿真结果表明该方法有效提高了交叉口的交通效率和车辆安全性。

Conclusion: 该智能决策框架在复杂场景中具有有效性，能实现安全高效的自动驾驶决策。

Abstract: Autonomous driving decision-making at unsignalized intersections is highly
challenging due to complex dynamic interactions and high conflict risks. To
achieve proactive safety control, this paper proposes a deep reinforcement
learning (DRL) decision-making framework integrated with a biased attention
mechanism. The framework is built upon the Soft Actor-Critic (SAC) algorithm.
Its core innovation lies in the use of biased attention to construct a traffic
risk predictor. This predictor assesses the long-term risk of collision for a
vehicle entering the intersection and transforms this risk into a dense reward
signal to guide the SAC agent in making safe and efficient driving decisions.
Finally, the simulation results demonstrate that the proposed method
effectively improves both traffic efficiency and vehicle safety at the
intersection, thereby proving the effectiveness of the intelligent
decision-making framework in complex scenarios. The code of our work is
available at https://github.com/hank111525/SAC-RWB.

</details>


### [33] [Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems](https://arxiv.org/abs/2510.12462)
*Jiaxin Gao,Chen Chen,Yanwen Jia,Xueluan Gong,Kwok-Yan Lam,Qian Wang*

Main category: cs.AI

TL;DR: 本文系统研究了LLM作为评判员时的偏见问题，发现现有模型对偏见输入具有鲁棒性，但微调于偏见数据会显著降低性能，并提出四种缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被越来越多地用于自主评估通信系统内容质量，其评判的公正性存在风险，任何偏见都可能影响结果并损害用户信任。

Method: 在点式评分设置下系统调查了两种LLM评判模型（GPT-Judge和JudgeLM）的11种偏见类型，涵盖隐性和显性形式。

Result: 发现最先进的LLM评判员对偏见输入具有鲁棒性，通常给偏见样本分配较低分数；提供详细评分标准能增强鲁棒性；在偏见数据上微调会显著降低性能；评判分数与任务难度相关。

Conclusion: 提出了四种潜在缓解策略，以确保在实际通信场景中实现公平可靠的AI评判。

Abstract: Large Language Models (LLMs) are increasingly being used to autonomously
evaluate the quality of content in communication systems, e.g., to assess
responses in telecom customer support chatbots. However, the impartiality of
these AI "judges" is not guaranteed, and any biases in their evaluation
criteria could skew outcomes and undermine user trust. In this paper, we
systematically investigate judgment biases in two LLM-as-a-judge models (i.e.,
GPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11
types of biases that cover both implicit and explicit forms. We observed that
state-of-the-art LLM judges demonstrate robustness to biased inputs, generally
assigning them lower scores than the corresponding clean samples. Providing a
detailed scoring rubric further enhances this robustness. We further found that
fine-tuning an LLM on high-scoring yet biased responses can significantly
degrade its performance, highlighting the risk of training on biased data. We
also discovered that the judged scores correlate with task difficulty: a
challenging dataset like GPQA yields lower average scores, whereas an
open-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.
Finally, we proposed four potential mitigation strategies to ensure fair and
reliable AI judging in practical communication scenarios.

</details>


### [34] [Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical Interviews](https://arxiv.org/abs/2510.12490)
*Rui Reis,Pedro Rangel Henriques,João Ferreira-Coimbra,Eva Oliveira,Nuno F. Rodrigues*

Main category: cs.AI

TL;DR: 开发了一个基于有向无环图(DAG)的任务导向对话框架，用于医疗问诊，包含冷启动机制、自适应分支回溯、终止逻辑和结构化报告生成等功能。


<details>
  <summary>Details</summary>
Motivation: 将医疗算法和指南转化为临床问题语料库，设计能够适应患者回答的智能问诊系统，减少认知负担并支持高效报告生成。

Method: 使用DAG结构组织医疗问题，集成系统化管道将指南转化为问题库，采用分层聚类的冷启动机制，实现自适应分支和回溯的扩展剪枝机制，以及终止逻辑和结构化报告自动合成。

Result: 患者应用获得低认知负荷(15.6)、高可用性(86)和强满意度(8.1/9)；医生应用获得中等认知负荷(26)、优异可用性(88.5)和高满意度(8.3/9)。

Conclusion: 该系统能有效集成到临床工作流程中，降低认知需求并支持高效报告生成，但存在系统延迟和评估样本小等局限性。

Abstract: We developed a task-oriented dialogue framework structured as a Directed
Acyclic Graph (DAG) of medical questions. The system integrates: (1) a
systematic pipeline for transforming medical algorithms and guidelines into a
clinical question corpus; (2) a cold-start mechanism based on hierarchical
clustering to generate efficient initial questioning without prior patient
information; (3) an expand-and-prune mechanism enabling adaptive branching and
backtracking based on patient responses; (4) a termination logic to ensure
interviews end once sufficient information is gathered; and (5) automated
synthesis of doctor-friendly structured reports aligned with clinical
workflows. Human-computer interaction principles guided the design of both the
patient and physician applications. Preliminary evaluation involved five
physicians using standardized instruments: NASA-TLX (cognitive workload), the
System Usability Scale (SUS), and the Questionnaire for User Interface
Satisfaction (QUIS). The patient application achieved low workload scores
(NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS =
8.1/9), with particularly high ratings for ease of learning and interface
design. The physician application yielded moderate workload (NASA-TLX = 26) and
excellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both
applications demonstrated effective integration into clinical workflows,
reducing cognitive demand and supporting efficient report generation.
Limitations included occasional system latency and a small, non-diverse
evaluation sample.

</details>


### [35] [Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation](https://arxiv.org/abs/2510.12498)
*Chengpeng Hu,Calvin Yu-Chian Chen*

Main category: cs.AI

TL;DR: 提出了细胞状态潜在（CSL）视角，通过操作符语法组织学习过程，强调跨模态、尺度、情境和干预的决策对齐评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有AI虚拟细胞模型在跨实验室、平台的可迁移性有限，存在数据泄漏和覆盖偏差问题，且剂量、时间和组合效应缺乏系统处理。跨尺度耦合受限，与科学或临床读数的对齐不一致。

Method: 采用模型无关的CSL视角，通过测量、提升/投影（跨尺度耦合）和干预（剂量调度）的操作符语法组织学习。强调操作符感知的数据设计、抗泄漏分区和透明校准报告。

Result: 提出了决策对齐的评估蓝图，关注功能空间读数如通路活性、空间邻域和临床相关终点。

Conclusion: CSL框架和操作符语法能够实现可重复的同类比较，促进AI虚拟细胞模型在跨模态、尺度、情境和干预方面的稳健发展。

Abstract: Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable,
decision-relevant models of cell state from multimodal, multiscale
measurements. Recent studies have introduced single-cell and spatial foundation
models, improved cross-modality alignment, scaled perturbation atlases, and
explored pathway-level readouts. Nevertheless, although held-out validation is
standard practice, evaluations remain predominantly within single datasets and
settings; evidence indicates that transport across laboratories and platforms
is often limited, that some data splits are vulnerable to leakage and coverage
bias, and that dose, time and combination effects are not yet systematically
handled. Cross-scale coupling also remains constrained, as anchors linking
molecular, cellular and tissue levels are sparse, and alignment to scientific
or clinical readouts varies across studies. We propose a model-agnostic
Cell-State Latent (CSL) perspective that organizes learning via an operator
grammar: measurement, lift/project for cross-scale coupling, and intervention
for dosing and scheduling. This view motivates a decision-aligned evaluation
blueprint across modality, scale, context and intervention, and emphasizes
function-space readouts such as pathway activity, spatial neighborhoods and
clinically relevant endpoints. We recommend operator-aware data design,
leakage-resistant partitions, and transparent calibration and reporting to
enable reproducible, like-for-like comparisons.

</details>


### [36] [ProtoSiTex: Learning Semi-Interpretable Prototypes for Multi-label Text Classification](https://arxiv.org/abs/2510.12534)
*Utsav Kumar Nareti,Suraj Kumar,Soumya Pandey,Soumi Chattopadhyay,Chandranath Adak*

Main category: cs.AI

TL;DR: ProtoSiTex是一个用于细粒度多标签文本分类的半可解释框架，采用双阶段交替训练策略，在保持高性能的同时提供忠实的人类对齐解释。


<details>
  <summary>Details</summary>
Motivation: 用户生成评论的激增需要可解释模型来提供细粒度洞察。现有基于原型的模型通常在粗粒度（句子或文档级别）运行，无法处理现实世界文本分类的多标签性质。

Method: 采用双阶段交替训练策略：无监督原型发现阶段学习语义连贯且多样化的原型，监督分类阶段将这些原型映射到类别标签。使用分层损失函数在子句、句子和文档级别强制执行一致性。

Result: 在酒店评论基准数据集和两个公共基准测试（二分类和多分类）上的实验表明，ProtoSiTex实现了最先进的性能，同时提供忠实的人类对齐解释。

Conclusion: ProtoSiTex是半可解释多标签文本分类的稳健解决方案，通过自适应原型和多头注意力捕获重叠和冲突语义，在性能和可解释性方面都表现出色。

Abstract: The surge in user-generated reviews has amplified the need for interpretable
models that can provide fine-grained insights. Existing prototype-based models
offer intuitive explanations but typically operate at coarse granularity
(sentence or document level) and fail to address the multi-label nature of
real-world text classification. We propose ProtoSiTex, a semi-interpretable
framework designed for fine-grained multi-label text classification. ProtoSiTex
employs a dual-phase alternating training strategy: an unsupervised prototype
discovery phase that learns semantically coherent and diverse prototypes, and a
supervised classification phase that maps these prototypes to class labels. A
hierarchical loss function enforces consistency across sub-sentence, sentence,
and document levels, enhancing interpretability and alignment. Unlike prior
approaches, ProtoSiTex captures overlapping and conflicting semantics using
adaptive prototypes and multi-head attention. We also introduce a benchmark
dataset of hotel reviews annotated at the sub-sentence level with multiple
labels. Experiments on this dataset and two public benchmarks (binary and
multi-class) show that ProtoSiTex achieves state-of-the-art performance while
delivering faithful, human-aligned explanations, establishing it as a robust
solution for semi-interpretable multi-label text classification.

</details>


### [37] [Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings](https://arxiv.org/abs/2510.12555)
*Andries Rosseau,Raphaël Avalos,Ann Nowé*

Main category: cs.AI

TL;DR: 提出基于包容性适应度的多智能体强化学习框架，通过基因型分配和遗传共享，在囚徒困境网络游戏中模拟自然选择的社会动态，验证了汉密尔顿法则等生物学原理。


<details>
  <summary>Details</summary>
Motivation: 受自然选择中竞争与合作力量驱动智力进化的启发，旨在创建更接近生物进化过程的多智能体学习环境，超越传统二元团队结构。

Method: 为每个智能体分配基因型，设计基于包容性适应度的奖励函数，允许基因共享，在囚徒困境网络游戏中研究社会动态。

Result: 实验结果与汉密尔顿法则等生物学原理一致，展示了基于遗传相似性的合作谱系，产生了独特的非团队社会动态。

Conclusion: 基于包容性适应度的奖励机制为更高级战略和社会智能智能体的涌现提供了基础，可扩展到具有时空结构、有限资源和进化种群的开放环境。

Abstract: The competitive and cooperative forces of natural selection have driven the
evolution of intelligence for millions of years, culminating in nature's vast
biodiversity and the complexity of human minds. Inspired by this process, we
propose a novel multi-agent reinforcement learning framework where each agent
is assigned a genotype and where reward functions are modelled after the
concept of inclusive fitness. An agent's genetic material may be shared with
other agents, and our inclusive reward function naturally accounts for this. We
study the resulting social dynamics in two types of network games with
prisoner's dilemmas and find that our results align with well-established
principles from biology, such as Hamilton's rule. Furthermore, we outline how
this framework can extend to more open-ended environments with spatial and
temporal structure, finite resources, and evolving populations. We hypothesize
the emergence of an arms race of strategies, where each new strategy is a
gradual improvement over earlier adaptations of other agents, effectively
producing a multi-agent autocurriculum analogous to biological evolution. In
contrast to the binary team-based structures prevalent in earlier research, our
gene-based reward structure introduces a spectrum of cooperation ranging from
full adversity to full cooperativeness based on genetic similarity, enabling
unique non team-based social dynamics. For example, one agent having a mutual
cooperative relationship with two other agents, while the two other agents
behave adversarially towards each other. We argue that incorporating inclusive
fitness in agents provides a foundation for the emergence of more strategically
advanced and socially intelligent agents.

</details>


### [38] [HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle Games](https://arxiv.org/abs/2510.12563)
*Jingcong Liang,Shijun Wan,Xuehai Wu,Siyuan Wang,Yitong Li,Qianglong Chen,Duyu Tang,Zhongyu Wei*

Main category: cs.AI

TL;DR: 提出了HardcoreLogic基准测试，包含5000多个谜题，通过增加复杂性、非常见元素和不可解谜题三个维度来测试大型推理模型的鲁棒性，揭示现有模型严重依赖记忆而非真正推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在复杂任务上表现优异，但面对非标准游戏变体时能否灵活应用规则仍存疑问。现有语料库主要关注9x9数独等流行谜题，容易导致模型过拟合和记忆解决方案模式，掩盖了理解新规则和适应新变体的缺陷。

Method: 构建HardcoreLogic基准测试，包含10个游戏的5000多个谜题，通过三个维度系统化转换标准谜题：增加复杂性(IC)、非常见元素(UE)和不可解谜题(UP)，减少对捷径记忆的依赖。

Result: 评估显示各种大型推理模型性能显著下降，即使在现有基准测试中得分最高的模型也表现不佳，表明它们严重依赖记忆的刻板模式。增加复杂性是主要困难来源，但模型在处理不增加谜题难度的细微规则变化时也遇到困难。

Conclusion: HardcoreLogic暴露了当前大型推理模型的局限性，为推进高级逻辑推理建立了基准测试，系统化错误分析进一步突显了真正推理能力的差距。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance on
complex tasks, including logical puzzle games that require deriving solutions
satisfying all constraints. However, whether they can flexibly apply
appropriate rules to varying conditions, particularly when faced with
non-canonical game variants, remains an open question. Existing corpora focus
on popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats
and memorization of solution patterns, which can mask deficiencies in
understanding novel rules or adapting strategies to new variants. To address
this, we introduce HardcoreLogic, a challenging benchmark of over 5,000 puzzles
across 10 games, designed to test the robustness of LRMs on the "long-tail" of
logical games. HardcoreLogic systematically transforms canonical puzzles
through three dimensions: Increased Complexity (IC), Uncommon Elements (UE),
and Unsolvable Puzzles (UP), reducing reliance on shortcut memorization.
Evaluations on a diverse set of LRMs reveal significant performance drops, even
for models achieving top scores on existing benchmarks, indicating heavy
reliance on memorized stereotypes. While increased complexity is the dominant
source of difficulty, models also struggle with subtle rule variations that do
not necessarily increase puzzle difficulty. Our systematic error analysis on
solvable and unsolvable puzzles further highlights gaps in genuine reasoning.
Overall, HardcoreLogic exposes the limitations of current LRMs and establishes
a benchmark for advancing high-level logical reasoning.

</details>


### [39] [Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2510.12635)
*Yuxiang Zhang,Jiangming Shu,Ye Ma,Xueyuan Lin,Shangxi Wu,Jitao Sang*

Main category: cs.AI

TL;DR: 提出了Memory-as-Action框架，将工作内存管理重新定义为可学习的内在能力，通过强化学习联合优化任务推理和内存管理，解决了LLM在长视野任务中内存受限的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长视野智能体任务中面临内存受限的挑战，现有工作内存方法通常依赖与核心策略解耦的外部启发式机制。

Method: 提出Memory-as-Action框架，智能体通过执行显式编辑操作来主动管理工作内存作为统一策略的一部分，并开发Dynamic Context Policy Optimization算法处理轨迹断裂问题。

Result: 端到端联合优化不仅减少了总体计算消耗，还通过针对模型内在能力定制的自适应上下文管理策略提高了任务性能。

Conclusion: 将内存管理作为可学习的内在能力进行端到端优化，能够有效提升LLM在长视野任务中的性能，同时降低计算开销。

Abstract: Large Language Models face challenges in long-horizon agentic tasks as their
constrained memory is easily overwhelmed by distracting or irrelevant context.
Existing working memory methods typically rely on external, heuristic
mechanisms that are decoupled from the agent's core policy. In this work, we
reframe working memory management as a learnable, intrinsic capability. We
propose a novel framework, Memory-as-Action, where an agent actively manages
its working memory by executing explicit editing operations as part of a
unified policy. This formulation allows an agent, trained via reinforcement
learning, to balance memory curation against long-term task objectives under
given resource constraints. However, such memory editing actions break the
standard assumption of a continuously growing prefix in LLM interactions,
leading to what we call trajectory fractures. These non-prefix changes disrupt
the causal continuity required by standard policy gradient methods, making
those methods inapplicable. To address this, we propose a new algorithm,
Dynamic Context Policy Optimization, which enables stable end-to-end
reinforcement learning by segmenting trajectories at memory action points and
applying trajectory-level advantages to the resulting action segments. Our
results demonstrate that jointly optimizing for task reasoning and memory
management in an end-to-end fashion not only reduces overall computational
consumption but also improves task performance, driven by adaptive context
curation strategies tailored to the model's intrinsic capabilities.

</details>


### [40] [ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning](https://arxiv.org/abs/2510.12693)
*Hanyang Chen,Mark Zhao,Rui Yang,Qinwei Ma,Ke Yang,Jiarui Yao,Kangrui Wang,Hao Bai,Zhenhailong Wang,Rui Pan,Mengchao Zhang,Jose Barreiros,Aykut Onol,ChengXiang Zhai,Heng Ji,Manling Li,Huan Zhang,Tong Zhang*

Main category: cs.AI

TL;DR: ERA是一个两阶段框架，通过先验知识学习和在线强化学习，使小型视觉语言模型在具身AI任务中超越大型模型。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型部署成本高，而小型模型又缺乏足够知识和技能的问题，弥合性能与效率之间的差距。

Method: 第一阶段：具身先验学习，从三种数据源蒸馏知识；第二阶段：在线强化学习，包含自我总结、密集奖励塑造和回合级策略优化。

Result: 在EB-ALFRED任务上比GPT-4o提升8.4%，在EB-Manipulation任务上提升19.4%，并在未见任务上表现出强泛化能力。

Conclusion: ERA为可扩展的具身智能提供了实用路径，为未来具身AI系统提供了方法论见解。

Abstract: Recent advances in embodied AI highlight the potential of vision language
models (VLMs) as agents capable of perception, reasoning, and interaction in
complex environments. However, top-performing systems rely on large-scale
models that are costly to deploy, while smaller VLMs lack the necessary
knowledge and skills to succeed. To bridge this gap, we present
\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates
prior knowledge learning and online reinforcement learning (RL). The first
stage, \textit{Embodied Prior Learning}, distills foundational knowledge from
three types of data: (1) Trajectory-Augmented Priors, which enrich existing
trajectory data with structured reasoning generated by stronger models; (2)
Environment-Anchored Priors, which provide in-environment knowledge and
grounding supervision; and (3) External Knowledge Priors, which transfer
general knowledge from out-of-environment datasets. In the second stage, we
develop an online RL pipeline that builds on these priors to further enhance
agent performance. To overcome the inherent challenges in agent RL, including
long horizons, sparse rewards, and training instability, we introduce three key
designs: self-summarization for context management, dense reward shaping, and
turn-level policy optimization. Extensive experiments on both high-level
planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate
that ERA-3B surpasses both prompting-based large models and previous
training-based baselines. Specifically, it achieves overall improvements of
8.4\% on EB-ALFRED and 19.4\% on EB-Manipulation over GPT-4o, and exhibits
strong generalization to unseen tasks. Overall, ERA offers a practical path
toward scalable embodied intelligence, providing methodological insights for
future embodied AI systems.

</details>


### [41] [Multi-Agent Debate for LLM Judges with Adaptive Stability Detection](https://arxiv.org/abs/2510.12697)
*Tianyu Hu,Zhen Tan,Song Wang,Huaizhi Qu,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出多智能体辩论法官框架，通过协作推理和迭代优化提升LLM自动判断任务的准确性，相比多数投票方法更有效。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为法官的方法依赖简单聚合（如多数投票），即使个体智能体给出正确答案也可能失败，需要更有效的协作判断机制。

Method: 多智能体辩论框架，数学形式化辩论过程，引入基于时间变化Beta-Binomial混合的稳定性检测机制，使用Kolmogorov-Smirnov检验进行自适应停止。

Result: 在多个基准测试和模型上的实验表明，该框架在保持计算效率的同时，比多数投票方法提高了判断准确性。

Conclusion: 辩论框架能放大正确性，稳定性检测机制确保效率，为LLM自动判断任务提供了更可靠的解决方案。

Abstract: With advancements in reasoning capabilities, Large Language Models (LLMs) are
increasingly employed for automated judgment tasks. While LLMs-as-Judges offer
promise in automating evaluations, current approaches often rely on simplistic
aggregation methods (e.g., majority voting), which can fail even when
individual agents provide correct answers. To address this, we propose a
multi-agent debate judge framework where agents collaboratively reason and
iteratively refine their responses. We formalize the debate process
mathematically, analyzing agent interactions and proving that debate amplifies
correctness compared to static ensembles. To enhance efficiency, we introduce a
stability detection mechanism that models judge consensus dynamics via a
time-varying Beta-Binomial mixture, with adaptive stopping based on
distributional similarity (Kolmogorov-Smirnov test). This mechanism models the
judges' collective correct rate dynamics using a time-varying mixture of
Beta-Binomial distributions and employs an adaptive stopping criterion based on
distributional similarity (Kolmogorov-Smirnov statistic). Experiments across
multiple benchmarks and models demonstrate that our framework improves judgment
accuracy over majority voting while maintaining computational efficiency.

</details>


### [42] [CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction](https://arxiv.org/abs/2510.12703)
*Mattia Grasselli,Angelo Porrello,Carlo Augusto Grazia*

Main category: cs.AI

TL;DR: 本文研究了使用合作感知消息(CAM)数据进行车辆轨迹预测，开发了CAMNet神经网络模型，并在两个数据集上验证了CAM数据在轨迹预测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶面临安全挑战，现有传感器存在视野受限和视线遮挡问题。车辆间通信可以通过共享信息来提高情境感知能力，特别是当传感器被遮挡时。

Method: 设计并训练了基于合作感知消息的图神经网络(CAMNet)，在一个广泛使用的运动预测数据集上进行训练，然后在自行创建的CAM数据集上进行评估。

Result: 方法显示出有希望的结果，证明CAM数据确实可以支持车辆轨迹预测。同时发现了该方法的一些局限性。

Conclusion: CAM数据在车辆轨迹预测中具有应用潜力，但存在一些局限性，为未来研究提供了机会。

Abstract: Autonomous driving remains a challenging task, particularly due to safety
concerns. Modern vehicles are typically equipped with expensive sensors such as
LiDAR, cameras, and radars to reduce the risk of accidents. However, these
sensors face inherent limitations: their field of view and line of sight can be
obstructed by other vehicles, thereby reducing situational awareness. In this
context, vehicle-to-vehicle communication plays a crucial role, as it enables
cars to share information and remain aware of each other even when sensors are
occluded. One way to achieve this is through the use of Cooperative Awareness
Messages (CAMs). In this paper, we investigate the use of CAM data for vehicle
trajectory prediction. Specifically, we design and train a neural network,
Cooperative Awareness Message-based Graph Neural Network (CAMNet), on a widely
used motion forecasting dataset. We then evaluate the model on a second dataset
that we created from scratch using Cooperative Awareness Messages, in order to
assess whether this type of data can be effectively exploited. Our approach
demonstrates promising results, showing that CAMs can indeed support vehicle
trajectory prediction. At the same time, we discuss several limitations of the
approach, which highlight opportunities for future research.

</details>


### [43] [Towards Robust Artificial Intelligence: Self-Supervised Learning Approach for Out-of-Distribution Detection](https://arxiv.org/abs/2510.12713)
*Wissam Salhab,Darine Ameyed,Hamid Mcheick,Fehmi Jaafar*

Main category: cs.AI

TL;DR: 提出一种无需标注数据即可提升AI系统对分布外样本检测能力的方法，结合自监督学习和图论技术，在AUROC指标上达到0.99的优异表现。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶、交通、医疗等安全关键系统中，AI系统的鲁棒性至关重要。需要提升系统在分布外样本、对抗攻击和环境变化等条件下的可靠性能。

Method: 利用自监督学习原理从无标注数据中学习有用表示，并结合图论技术来更有效地识别和分类分布外样本。

Result: 与现有最先进方法相比，该方法在AUROC指标上达到了0.99的优异性能。

Conclusion: 该方法成功提升了AI系统对分布外样本的检测能力，无需标注数据即可增强系统鲁棒性，在安全关键应用中具有重要价值。

Abstract: Robustness in AI systems refers to their ability to maintain reliable and
accurate performance under various conditions, including out-of-distribution
(OOD) samples, adversarial attacks, and environmental changes. This is crucial
in safety-critical systems, such as autonomous vehicles, transportation, or
healthcare, where malfunctions could have severe consequences. This paper
proposes an approach to improve OOD detection without the need of labeled data,
thereby increasing the AI systems' robustness. The proposed approach leverages
the principles of self-supervised learning, allowing the model to learn useful
representations from unlabeled data. Combined with graph-theoretical
techniques, this enables the more efficient identification and categorization
of OOD samples. Compared to existing state-of-the-art methods, this approach
achieved an Area Under the Receiver Operating Characteristic Curve (AUROC) =
0.99.

</details>


### [44] [Clutch Control: An Attention-based Combinatorial Bandit for Efficient Mutation in JavaScript Engine Fuzzing](https://arxiv.org/abs/2510.12732)
*Myles Foley,Sergio Maffeis,Muhammad Fakhrur Rozi,Takeshi Takahashi*

Main category: cs.AI

TL;DR: CLUTCH是一种基于深度组合多臂老虎机的新方法，用于提高JavaScript模糊测试的效率，通过注意力机制和Concrete Dropout动态适应探索策略。


<details>
  <summary>Details</summary>
Motivation: 现有的JavaScript模糊测试方法使用随机选择来确定突变位置，这限制了测试效率。作者认为突变目标选择问题适合使用具有可变臂数的组合多臂老虎机来解决。

Method: 提出CLUTCH方法，使用深度组合多臂老虎机观察可变长度的JavaScript测试用例表示，采用注意力机制和Concrete Dropout来动态调整探索策略。

Result: 与三种最先进解决方案相比，CLUTCH将有效测试用例数量和每个测试用例的覆盖率分别平均提高了20.3%和8.9%。在可变和组合设置下，CLUTCH的遗憾分别减少了至少78.1%和4.1%。

Conclusion: CLUTCH在JavaScript模糊测试中显著提高了效率，证明了组合多臂老虎机方法在突变目标选择问题上的有效性。

Abstract: JavaScript engines are widely used in web browsers, PDF readers, and
server-side applications. The rise in concern over their security has led to
the development of several targeted fuzzing techniques. However, existing
approaches use random selection to determine where to perform mutations in
JavaScript code. We postulate that the problem of selecting better mutation
targets is suitable for combinatorial bandits with a volatile number of arms.
Thus, we propose CLUTCH, a novel deep combinatorial bandit that can observe
variable length JavaScript test case representations, using an attention
mechanism from deep learning. Furthermore, using Concrete Dropout, CLUTCH can
dynamically adapt its exploration. We show that CLUTCH increases efficiency in
JavaScript fuzzing compared to three state-of-the-art solutions by increasing
the number of valid test cases and coverage-per-testcase by, respectively,
20.3% and 8.9% on average. In volatile and combinatorial settings we show that
CLUTCH outperforms state-of-the-art bandits, achieving at least 78.1% and 4.1%
less regret in volatile and combinatorial settings, respectively.

</details>


### [45] [CTRL-Rec: Controlling Recommender Systems With Natural Language](https://arxiv.org/abs/2510.12742)
*Micah Carroll,Adeline Foote,Kevin Feng,Marcus Williams,Anca Dragan,W. Bradley Knox,Smitha Milli*

Main category: cs.AI

TL;DR: CTRL-Rec是一种允许用户通过自然语言实时控制推荐系统的方法，使用LLM模拟用户对项目的批准判断，并训练嵌入模型来近似这些判断，从而在传统推荐系统中实现细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 当用户对推荐系统的推荐不满意时，他们通常缺乏细粒度的控制手段来改变推荐结果。大语言模型通过自然语言请求为用户提供了解决方案。

Method: 在训练时使用LLM模拟用户基于语言请求对项目的批准判断，训练嵌入模型来近似这些模拟判断，并将基于用户请求的预测集成到传统推荐系统的标准信号加权中。部署时每个用户请求只需一次LLM嵌入计算。

Result: 在MovieLens数据集上的实验表明，该方法在各种请求下都能实现一致的细粒度控制。在19名Letterboxd用户的研究中，CTRL-Rec受到用户积极评价，与传统控制相比显著增强了用户的控制感和满意度。

Conclusion: CTRL-Rec方法成功实现了通过自然语言对传统推荐系统进行实时控制，提高了用户的控制感和满意度，同时保持了计算效率。

Abstract: When users are dissatisfied with recommendations from a recommender system,
they often lack fine-grained controls for changing them. Large language models
(LLMs) offer a solution by allowing users to guide their recommendations
through natural language requests (e.g., "I want to see respectful posts with a
different perspective than mine"). We propose a method, CTRL-Rec, that allows
for natural language control of traditional recommender systems in real-time
with computational efficiency. Specifically, at training time, we use an LLM to
simulate whether users would approve of items based on their language requests,
and we train embedding models that approximate such simulated judgments. We
then integrate these user-request-based predictions into the standard weighting
of signals that traditional recommender systems optimize. At deployment time,
we require only a single LLM embedding computation per user request, allowing
for real-time control of recommendations. In experiments with the MovieLens
dataset, our method consistently allows for fine-grained control across a
diversity of requests. In a study with 19 Letterboxd users, we find that
CTRL-Rec was positively received by users and significantly enhanced users'
sense of control and satisfaction with recommendations compared to traditional
controls.

</details>


### [46] [Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics](https://arxiv.org/abs/2510.12787)
*Marco Del Tredici,Jacob McCarran,Benjamin Breen,Javier Aspuru Mijares,Weichen Winston Yin,Jacob M. Taylor,Frank Koppens,Dirk Englund*

Main category: cs.AI

TL;DR: Ax-Prover是一个基于多智能体系统的自动定理证明器，能够跨多个科学领域解决Lean中的定理证明问题，既可自主运行也可与人类专家协作。


<details>
  <summary>Details</summary>
Motivation: 解决科学问题需要创造性推理和严格的形式化验证，现有专用证明系统难以泛化到不同领域。

Method: 通过模型上下文协议（MCP）为大型语言模型配备Lean工具，结合LLM的知识推理能力和Lean的形式化正确性保证。

Result: 在公共数学基准测试中与最先进证明器竞争，在新引入的抽象代数和量子理论基准测试中显著优于其他模型。

Conclusion: 基于工具的智能体定理证明方法为跨领域形式化验证提供了可泛化的方法论，并在实际密码学定理证明中展示了辅助能力。

Abstract: We present Ax-Prover, a multi-agent system for automated theorem proving in
Lean that can solve problems across diverse scientific domains and operate
either autonomously or collaboratively with human experts. To achieve this,
Ax-Prover approaches scientific problem solving through formal proof
generation, a process that demands both creative reasoning and strict syntactic
rigor. Ax-Prover meets this challenge by equipping Large Language Models
(LLMs), which provide knowledge and reasoning, with Lean tools via the Model
Context Protocol (MCP), which ensure formal correctness. To evaluate its
performance as an autonomous prover, we benchmark our approach against frontier
LLMs and specialized prover models on two public math benchmarks and on two
Lean benchmarks we introduce in the fields of abstract algebra and quantum
theory. On public datasets, Ax-Prover is competitive with state-of-the-art
provers, while it largely outperform them on the new benchmarks. This shows
that, unlike specialized systems that struggle to generalize, our tool-based
agentic theorem prover approach offers a generalizable methodology for formal
verification across diverse scientific domains. Furthermore, we demonstrate
Ax-Prover's assistant capabilities in a practical use case, showing how it
enabled an expert mathematician to formalize the proof of a complex
cryptography theorem.

</details>
