<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 47]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Activation Manifold Projection: Liberating Task-Specific Behaviors from LLM Architectures](https://arxiv.org/abs/2510.17902)
*Al Kari*

Main category: cs.AI

TL;DR: CAST框架通过激活空间映射实现LoRA适配器的跨架构迁移，解决了大语言模型中的架构锁定问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于权重空间对齐的迁移方法脆弱且间接，无法有效解决不同LLM架构间LoRA适配器的迁移问题。

Method: 学习轻量级双向投影头，在激活流层面进行非线性映射，将目标模型激活转换为源模型潜在空间，应用冻结的行为核，再投影回目标模型。

Result: 在Llama-2和Mistral等异构模型间迁移LoRA适配器，性能达到目标模型上完全重新训练LoRA的85-95%，优于现有权重空间迁移技术。

Conclusion: CAST实现了真正的零样本LoRA适配器迁移，为模型互操作性建立了新的技术标准。

Abstract: The proliferation of Large Language Model (LLM) architectures presents a
fundamental challenge: valuable, task-specific behaviors learned through
fine-tuning methods like Low-Rank Adaptation (LoRA) are effectively trapped
within their source model's architecture, herein referred to architectural
lock-in. Existing transfer methods attempt to bridge this gap by aligning the
static weight spaces of models, a brittle and indirect approach that relies on
tenuous correlations between parameter geometries. This paper introduces a
fundamentally different and more direct paradigm: the Cartridge Activation
Space Transfer (CAST), a novel framework that liberates LoRA-encoded behaviors
by learning a direct, nonlinear mapping between the activation manifolds, the
geometric structures formed by the model's internal neuron activations, of two
distinct LLM architectures. CAST treats a pre-trained LoRA as a frozen
"behavioral kernel." It learns a set of lightweight, bidirectional projection
heads that translate the target model's activation stream into the source
model's latent space, apply the frozen kernel, and project the result back.
This process, trained on a general text corpus without any task-specific data,
effectively decouples the learned skill from the source architecture. We
demonstrate that CAST enables true "zero-shot" translation of any standard LoRA
adapter. Our experiments, including transfers between heterogeneous model
families like Llama-2 and Mistral, show that CAST-translated adapters achieve
85-95\% of the performance of a LoRA fully retrained on the target model,
quantitatively outperforming current weight-space transfer techniques and
establishing a new state-of-the-art in model interoperability.

</details>


### [2] [Beyond More Context: Retrieval Diversity Boosts Multi-Turn Intent Understanding](https://arxiv.org/abs/2510.17940)
*Zhiming Lin*

Main category: cs.AI

TL;DR: 提出了一种多样性感知检索框架，在固定token预算下通过选择覆盖意图多样性和语言多样性的示例来提升LLM的多轮意图理解性能，在MultiWOZ 2.4和SGD数据集上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 任务型聊天机器人的多轮意图理解面临token预算限制和噪声上下文问题，现有检索方法过于关注相关性而忽略了集合层面的多样性和混杂因素（如上下文长度、示例顺序）。

Method: 开发了多样性感知检索框架，选择上下文示例时平衡意图覆盖和语言多样性，并与标准LLM解码器集成；评估时强制执行预算匹配提示和随机位置，并进行敏感性分析。

Result: 在MultiWOZ 2.4和SGD数据集上，该方法在相同token预算下实现了Joint Goal Accuracy的显著提升，超越了强LLM/DST基线，在K=4到7范围内表现一致，延迟适中。

Conclusion: 研究分离并验证了检索中内容多样性的影响，为构建准确、预算受限的多轮意图系统提供了一个简单可部署的选择原则。

Abstract: Multi turn intent understanding is central to task oriented chatbots, yet
real deployments face tight token budgets and noisy contexts, and most
retrieval pipelines emphasize relevance while overlooking set level diversity
and confounds such as more context or exemplar order. We ask whether retrieval
diversity, rather than longer prompts, systematically improves LLM intent
understanding under fixed budgets. We present a diversity aware retrieval
framework that selects in context exemplars to balance intent coverage and
linguistic variety, and integrates this selection with standard LLM decoders;
the evaluation enforces budget matched prompts and randomized positions, and
includes sensitivity analyses over exemplar count, diversity strength, and
backbone size. On MultiWOZ 2.4 and SGD, the approach achieves strong gains in
Joint Goal Accuracy under equal token budgets, surpassing strong LLM/DST
baselines, with consistent improvements across K from 4 to 7 and moderate
latency. Overall, the study isolates and validates the impact of content
diversity in retrieval and offers a simple, deployable selection principle for
building accurate, budget constrained multi turn intent systems.

</details>


### [3] [FABRIC: Framework for Agent-Based Realistic Intelligence Creation](https://arxiv.org/abs/2510.17995)
*Abhigya Verma,Seganrasan Subramanian,Nandhakumar Kandasamy,Naman Gupta*

Main category: cs.AI

TL;DR: 提出了一个仅使用LLM合成智能体数据的统一框架，无需人工监督即可生成包含任务规范、工具定义、策略伪代码、自然语言交互和执行轨迹的完整交互记录。


<details>
  <summary>Details</summary>
Motivation: 收集智能体数据成本高昂且难以扩展，需要一种可扩展的方法来生成结构化交互记录以支持智能体LLM的开发。

Method: 使用模块化流水线生成符合严格语法和语义约束的交互记录，支持多任务和多轮交互，集成约束生成格式、JSON模式验证和基于评判器的过滤。

Result: 框架能够生成机器可解析且忠实对齐输入、输出和工具调用的高质量合成数据，支持构建反映完整工具使用能力的数据集。

Conclusion: 该框架为手动收集提供了可重现的LLM替代方案，推动了能够进行稳健工具使用的智能体LLM的发展。

Abstract: Large language models (LLMs) are increasingly deployed as agents, expected to
decompose goals, invoke tools, and verify results in dynamic environments.
Realizing these capabilities requires access to agentic data- structured
interaction records that couple user intents with tool specifications,
argument-grounded calls, and verifiable execution traces. However, collecting
such data from human annotators is costly, time-consuming, and difficult to
scale.
  We present a unified framework for synthesizing agentic data using only LLMs,
without any human-in-the-loop supervision. This framework decomposes generation
into modular pipelines that produce complete interaction records spanning task
specifications, tool definitions, policy pseudocode, natural language
exchanges, and execution traces. Records conform to strict syntactic and
semantic constraints, ensuring machine-parseability and faithful alignment
across inputs, outputs, and tool calls.
  Beyond single tasks, there is support for both multi-task and multi-turn
agent interactions, enabling the construction of datasets that reflect the full
spectrum of tool-use competencies. To ensure quality and consistency, the
framework integrates constrained generation formats, JSON-schema validation,
and judge-based filtering.
  This paper formalizes the schema for agentic records, details the prompt
design principles that guide generation, and introduces scalable pipelines for
high-quality synthetic data. By providing a reproducible, LLM-only alternative
to manual collection, hence advancing the development of agentic LLMs capable
of robust tool use.

</details>


### [4] [OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal Reinforcement Learning for Enhanced Reasoning](https://arxiv.org/abs/2510.18032)
*Zhenyu Bi,Meng Lu,Yang Li,Swastik Roy,Weijie Guan,Morteza Ziyadi,Xuan Wang*

Main category: cs.AI

TL;DR: 提出了一种多智能体口头强化学习算法，通过动态构建和优化协作结构来提升多智能体推理能力，在多种推理任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统要么采用预定义结构，要么依赖多数投票或圆桌辩论，这会压制正确但非主导的智能体贡献。现有图网络方法只优化智能体性能而忽视交互质量。

Method: 提出多智能体口头强化学习算法，定义动作空间和反馈机制来评估沟通的鲁棒性和连贯性，通过多数投票达成最终决策。

Result: 在数学推理、创意写作、科学推理和数值排序等任务上，该方法显著优于单智能体提示方法和最先进的多智能体框架。

Conclusion: 有效的智能体沟通对多智能体推理至关重要，辩论质量在其中发挥重要作用，动态优化的协作结构能显著提升推理性能。

Abstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities in
mathematical and scientific tasks. To enhance complex reasoning, multi-agent
systems have been proposed to harness the collective intelligence of LLM
agents. However, existing collaboration structures are either predefined or
rely on majority voting or round-table debates, which can suppress correct but
less dominant agent contributions. Recent approaches model multi-agent systems
as graph networks but optimize purely for agent performance, neglecting the
quality of interactions. We hypothesize that effective agent communication is
crucial for multi-agent reasoning and that debating quality plays a significant
role. To address this, we propose $\ours$, a multi-agent verbal reinforcement
learning algorithm that dynamically constructs and refines multi-agent
collaboration structures. Our method defines action spaces and a feedback
mechanism that evaluates communication robustness and coherence throughout the
debate. The final decision is achieved through a majority vote over all the
agents. We assess $\ours$ on various reasoning tasks, including mathematical
reasoning, creative writing, scientific reasoning, and numerical sorting.
Results demonstrate that our approach significantly outperforms single-agent
prompting methods and state-of-the-art multi-agent frameworks on diverse tasks.

</details>


### [5] [Subject-Event Ontology Without Global Time: Foundations and Execution Semantics](https://arxiv.org/abs/2510.18040)
*Alexander Boldachev*

Main category: cs.AI

TL;DR: 提出了一种基于主体-事件的本体论形式化方法，用于建模复杂动态系统，不依赖全局时间。核心原则包括事件作为固定行为、因果顺序通过happens-before定义、可执行本体论、模型作为认知过滤器、以及真值假定。


<details>
  <summary>Details</summary>
Motivation: 为复杂动态系统提供不依赖全局时间的建模方法，特别适用于分布式系统、微服务架构、DLT平台和多视角场景，解决不同主体间事实冲突的问题。

Method: 提出九条公理(A1-A9)确保可执行本体论的正确性：历史单调性(I1)、因果无环性(I2)、可追溯性(I3)。采用基于模型的方法(A9)，通过模式进行事件验证、参与者授权，自动构建因果链(W3)。

Result: 在boldsea系统中实现了理论构建，这是一个用于可执行本体论的工作流引擎，使用BSL语言实现。展示了在分布式系统、微服务架构等场景的实用性。

Conclusion: 该形式化方法为复杂动态系统提供了有效的建模框架，特别在不依赖全局时间的情况下处理多主体视角和因果关系的挑战，具有广泛的应用前景。

Abstract: A formalization of a subject-event ontology is proposed for modeling complex
dynamic systems without reliance on global time. Key principles: (1) event as
an act of fixation - a subject discerns and fixes changes according to models
(conceptual templates) available to them; (2) causal order via happens-before -
the order of events is defined by explicit dependencies, not timestamps; (3)
making the ontology executable via a declarative dataflow mechanism, ensuring
determinism; (4) models as epistemic filters - a subject can only fix what
falls under its known concepts and properties; (5) presumption of truth - the
declarative content of an event is available for computation from the moment of
fixation, without external verification. The formalization includes nine axioms
(A1-A9), ensuring the correctness of executable ontologies: monotonicity of
history (I1), acyclicity of causality (I2), traceability (I3). Special
attention is given to the model-based approach (A9): event validation via
schemas, actor authorization, automatic construction of causal chains (W3)
without global time. Practical applicability is demonstrated on the boldsea
system - a workflow engine for executable ontologies, where the theoretical
constructs are implemented in BSL (Boldsea Semantic Language). The
formalization is applicable to distributed systems, microservice architectures,
DLT platforms, and multiperspectivity scenarios (conflicting facts from
different subjects).

</details>


### [6] [CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows](https://arxiv.org/abs/2510.18043)
*Joong Ho Choi,Jiayang Zhao,Jeel Shah,Ritvika Sonawane,Vedant Singh,Avani Appalla,Will Flanagan,Filipe Condessa*

Main category: cs.AI

TL;DR: CompactPrompt是一个端到端管道，通过硬提示压缩和轻量级文件级数据压缩相结合，将LLM代理工作流中的总令牌使用量和推理成本降低高达60%，同时保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代理工作流中运行时会产生高昂成本，因为需要链接长提示和处理丰富数据流。

Method: 使用自信息评分和基于依赖的短语分组修剪提示中的低信息令牌，同时对附加文档应用n-gram缩写和数值列的统一量化。

Result: 在TAT-QA和FinQA等基准数据集上，总令牌使用量和推理成本降低高达60%，输出质量保持（Claude-3.5-Sonnet和GPT-4.1-Mini的准确率下降小于5%）。

Conclusion: CompactPrompt为更精简的生成AI管道奠定了基础，能够可视化实时压缩决策并量化成本性能权衡。

Abstract: Large Language Models (LLMs) deliver powerful reasoning and generation
capabilities but incur substantial run-time costs when operating in agentic
workflows that chain together lengthy prompts and process rich data streams. We
introduce CompactPrompt, an end-to-end pipeline that merges hard prompt
compression with lightweight file-level data compression. CompactPrompt first
prunes low-information tokens from prompts using self-information scoring and
dependency-based phrase grouping. In parallel, it applies n-gram abbreviation
to recurrent textual patterns in attached documents and uniform quantization to
numerical columns, yielding compact yet semantically faithful representations.
Integrated into standard LLM agents, CompactPrompt reduces total token usage
and inference cost by up to 60% on benchmark dataset like TAT-QA and FinQA,
while preserving output quality (Results in less than 5% accuracy drop for
Claude-3.5-Sonnet, and GPT-4.1-Mini) CompactPrompt helps visualize real-time
compression decisions and quantify cost-performance trade-offs, laying the
groundwork for leaner generative AI pipelines.

</details>


### [7] [Planned Diffusion](https://arxiv.org/abs/2510.18087)
*Daniel Israel,Tian Jin,Ellie Cheng,Guy Van den Broeck,Aditya Grover,Suvinay Subramanian,Michael Carbin*

Main category: cs.AI

TL;DR: 提出了一种结合自回归和扩散模型的混合方法——计划扩散，通过两阶段生成实现更快的文本生成速度，同时保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型推理中生成速度与输出质量之间的权衡问题，自回归模型质量高但速度慢，扩散模型可并行生成但需要多次迭代才能达到相同质量。

Method: 两阶段方法：首先自回归地创建短计划将输出分解为独立片段，然后使用扩散模型同时生成这些片段。

Result: 在AlpacaEval评估中实现了帕累托最优权衡，相比自回归生成获得1.27x到1.81x加速，胜率仅下降0.87%到5.4%。

Conclusion: 计划扩散扩展了速度-质量的帕累托前沿，为更快、高质量的文本生成提供了实用路径，其规划机制简洁可靠且具有灵活的质量-延迟权衡控制。

Abstract: A central challenge in large language model inference is the trade-off
between generation speed and output quality. Autoregressive models produce
high-quality text but generate tokens sequentially. Diffusion models can
generate tokens in parallel but often need many iterations to match the same
quality. We propose planned diffusion, a hybrid method that combines the
strengths of both paradigms. Planned diffusion works in two stages: first, the
model creates a short autoregressive plan that breaks the output into smaller,
independent spans. Second, the model generates these spans simultaneously using
diffusion. This approach expands the speed-quality Pareto frontier and provides
a practical path to faster, high-quality text generation. On AlpacaEval, a
suite of 805 instruction-following prompts, planned diffusion achieves
Pareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x
speedup over autoregressive generation with only 0.87\% to 5.4\% drop in win
rate, respectively. Our sensitivity analysis shows that the planning mechanism
of planned diffusion is minimal and reliable, and simple runtime knobs exist to
provide flexible control of the quality-latency trade-off.

</details>


### [8] [SMaRT: Select, Mix, and ReinvenT - A Strategy Fusion Framework for LLM-Driven Reasoning and Planning](https://arxiv.org/abs/2510.18095)
*Nikhil Verma,Manasa Bharadwaj,Wonjun Jang,Harmanpreet Singh,Yixiao Wang,Homa Fashandi,Chul Lee*

Main category: cs.AI

TL;DR: SMaRT框架通过融合多种推理策略来提升LLM的性能和鲁棒性，超越单一策略方法


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一策略提示，缺乏不同推理方法的协同效应，需要融合多种策略来最大化性能和确保鲁棒性

Method: 提出SMaRT（选择、混合和重塑）框架，使用LLM作为智能集成器而非评估器，无缝整合多样推理策略

Result: 在推理、规划和顺序决策等基准测试中，SMaRT在解决方案质量、约束遵循和性能指标方面持续优于最先进基线

Conclusion: 这项工作通过开创跨策略校准新范式，重新定义了LLM驱动的决策制定，为推理系统解锁了更优结果并推进了自精炼方法的边界

Abstract: Large Language Models (LLMs) have redefined complex task automation with
exceptional generalization capabilities. Despite these advancements,
state-of-the-art methods rely on single-strategy prompting, missing the synergy
of diverse reasoning approaches. No single strategy excels universally,
highlighting the need for frameworks that fuse strategies to maximize
performance and ensure robustness. We introduce the Select, Mix, and ReinvenT
(SMaRT) framework, an innovative strategy fusion approach designed to overcome
this constraint by creating balanced and efficient solutions through the
seamless integration of diverse reasoning strategies. Unlike existing methods,
which employ LLMs merely as evaluators, SMaRT uses them as intelligent
integrators, unlocking the "best of all worlds" across tasks. Extensive
empirical evaluations across benchmarks in reasoning, planning, and sequential
decision-making highlight the robustness and adaptability of SMaRT. The
framework consistently outperforms state-of-the-art baselines in solution
quality, constraint adherence, and performance metrics. This work redefines
LLM-driven decision-making by pioneering a new paradigm in cross-strategy
calibration, unlocking superior outcomes for reasoning systems and advancing
the boundaries of self-refining methodologies.

</details>


### [9] [Measuring Reasoning in LLMs: a New Dialectical Angle](https://arxiv.org/abs/2510.18134)
*Soheil Abbasloo*

Main category: cs.AI

TL;DR: 提出SIEV框架，基于辩证法评估语言模型的推理过程，而不仅仅是答案正确性，发现当前最先进模型在推理过程中存在显著缺陷


<details>
  <summary>Details</summary>
Motivation: 当前评估方法只关注模型答案的正确性，但无法揭示推理过程的质量。推理应该是动态的、思想碰撞和进化的过程

Method: 基于辩证法（thesis, antithesis, synthesis）构建SIEV框架，评估模型在推理过程中解决矛盾、整合观点和进行高阶综合的能力

Result: 在GSM和MMLU等饱和基准测试中，GPT-5-chat等先进模型在SIEV评估下得分下降超过40分（满分100），揭示了显著的推理缺陷

Conclusion: 采用过程导向、哲学基础的方法能够对语言模型推理进行更深入、严谨和区分性的评估

Abstract: What does it truly mean for a language model to "reason"? Most current
evaluations and benchmarks reward models' correct standalone answers--but
correctness alone reveals little about the process that produced them. In this
work, we explore a different perspective: reasoning is not a static chain of
steps, but a dynamic trajectory where ideas interact, clash, and evolve into
deeper insights. To capture this dynamic, we draw on a well-established
philosophical tradition: \textit{dialectics}, where reasoning unfolds through
thesis, antithesis, and synthesis. Building on this, we present SIEV, a
structured framework that evaluates reasoning of LLMs through dialectics.
Unlike conventional evaluations, SIEV assesses not only the conclusion a model
reaches, but how it gets there: its ability to resolve tension, integrate
distinct ideas, and synthesize higher-order reasoning. This lens uncovers
significant reasoning gaps in state-of-the-art models even under saturated
benchmarks like GSM and MMLU. For instance, GPT-5-chat, a recent model, loses
over 40 points (out of 100) when evaluated with SIEV on GSM. Our findings
highlight that adopting a process-oriented, philosophically grounded approach
enables a deeper, more rigorous, and more discriminative assessment of LLM
reasoning.

</details>


### [10] [Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models](https://arxiv.org/abs/2510.18143)
*Huan Song,Deeksha Razdan,Yiyue Qian,Arijit Ghosh Chowdhury,Parth Patwa,Aman Chadha,Shinan Zhang,Sharlina Keshava,Hannah Marlowe*

Main category: cs.AI

TL;DR: PaDA-Agent是一种评估驱动的数据增强方法，通过发现验证数据中的失败模式来制定针对性策略，显著提升了小语言模型在领域特定任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在部署成本和延迟方面具有优势，但在复杂领域特定任务上的准确性往往落后于大模型。虽然监督微调可以弥补性能差距，但需要大量手动数据准备和迭代优化工作。

Method: PaDA-Agent通过评估发现验证数据中的失败模式，制定针对性数据增强策略，直接减少泛化差距。与仅关注模型训练错误的方法不同，它通过协调操作简化SLM的数据增强过程。

Result: 实验结果显示，PaDA-Agent在Llama 3.2 1B Instruct模型微调中，相比最先进的基于LLM的数据增强方法取得了显著改进。

Conclusion: PaDA-Agent提供了一种有效的评估驱动数据增强方法，能够显著提升小语言模型在领域特定任务上的性能，同时简化数据增强过程。

Abstract: Small Language Models (SLMs) offer compelling advantages in deployment cost
and latency, but their accuracy often lags behind larger models, particularly
for complex domain-specific tasks. While supervised fine-tuning can help bridge
this performance gap, it requires substantial manual effort in data preparation
and iterative optimization. We present PaDA-Agent (Pattern-guided Data
Augmentation Agent), an evaluation-driven approach that streamlines the data
augmentation process for SLMs through coordinated operations. Unlike
state-of-the-art approaches that focus on model training errors only and
generating error-correcting samples, PaDA-Agent discovers failure patterns from
the validation data via evaluations and drafts targeted data augmentation
strategies aiming to directly reduce the generalization gap. Our experimental
results demonstrate significant improvements over state-of-the-art LLM-based
data augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.

</details>


### [11] [Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety](https://arxiv.org/abs/2510.18154)
*Antonio-Gabriel Chacón Menke,Phan Xuan Tan,Eiji Kamioka*

Main category: cs.AI

TL;DR: 提出了一个句子级标注数据集，用于在LLM推理过程中基于激活的安全行为监控，填补了现有数据集只能整体标注推理的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本推理步骤的安全监控方法可能遗漏细微的有害模式，且可能被隐藏不安全推理的模型规避，需要更精细的激活级监控技术。

Method: 构建包含推理序列的句子级标注数据集，标注安全行为（如安全担忧表达、用户意图推测），并从中提取用于检测和影响这些行为的引导向量。

Result: 展示了数据集的实用性，提取的表征能够在模型激活中检测和引导安全行为，证明了激活级技术在改进推理安全监督方面的潜力。

Conclusion: 激活级监控技术有望改进AI推理安全监督，该数据集为安全研究填补了关键空白，提供了更精确的行为识别方法。

Abstract: Recent work has highlighted the importance of monitoring chain-of-thought
reasoning for AI safety; however, current approaches that analyze textual
reasoning steps can miss subtle harmful patterns and may be circumvented by
models that hide unsafe reasoning. We present a sentence-level labeled dataset
that enables activation-based monitoring of safety behaviors during LLM
reasoning. Our dataset contains reasoning sequences with sentence-level
annotations of safety behaviors such as expression of safety concerns or
speculation on user intent, which we use to extract steering vectors for
detecting and influencing these behaviors within model activations. The dataset
fills a key gap in safety research: while existing datasets label reasoning
holistically, effective application of steering vectors for safety monitoring
could be improved by identifying precisely when specific behaviors occur within
reasoning chains. We demonstrate the dataset's utility by extracting
representations that both detect and steer safety behaviors in model
activations, showcasing the potential of activation-level techniques for
improving safety oversight on reasoning.
  Content Warning: This paper discusses AI safety in the context of harmful
prompts and may contain references to potentially harmful content.

</details>


### [12] [LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior](https://arxiv.org/abs/2510.18155)
*Man-Lin Chu,Lucian Terhorst,Kadin Reed,Tom Ni,Weiwei Chen,Rongyu Lin*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体模拟框架，用于模拟消费者决策和社交动态，无需预定义规则即可测试营销策略。


<details>
  <summary>Details</summary>
Motivation: 传统的事后分析和基于规则的智能体模型难以捕捉人类行为和社交互动的复杂性，需要更真实的模拟工具来降低营销活动风险。

Method: 利用大语言模型在沙盒环境中构建多智能体模拟框架，智能体能够互动、表达内部推理、形成习惯并做出购买决策。

Result: 在价格折扣营销场景中，系统提供了可操作的策略测试结果，并揭示了传统方法无法发现的涌现社交模式。

Conclusion: 该方法为营销人员提供了一个可扩展、低风险的预实施测试工具，减少了对耗时的事后评估的依赖，降低了营销活动表现不佳的风险。

Abstract: Simulating consumer decision-making is vital for designing and evaluating
marketing strategies before costly real- world deployment. However, post-event
analyses and rule-based agent-based models (ABMs) struggle to capture the
complexity of human behavior and social interaction. We introduce an
LLM-powered multi-agent simulation framework that models consumer decisions and
social dynamics. Building on recent advances in large language model simulation
in a sandbox envi- ronment, our framework enables generative agents to
interact, express internal reasoning, form habits, and make purchasing
decisions without predefined rules. In a price-discount marketing scenario, the
system delivers actionable strategy-testing outcomes and reveals emergent
social patterns beyond the reach of con- ventional methods. This approach
offers marketers a scalable, low-risk tool for pre-implementation testing,
reducing reliance on time-intensive post-event evaluations and lowering the
risk of underperforming campaigns.

</details>


### [13] [Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model](https://arxiv.org/abs/2510.18165)
*Yihong Dong,Zhaoyu Ma,Xue Jiang,Zhiyuan Fan,Jiaru Qian,Yongmin Li,Jianha Xiao,Zhi Jin,Rongyu Cao,Binhua Li,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: 提出了Saber算法，一种无需训练的新型采样方法，用于扩散语言模型在代码生成任务中平衡推理速度与输出质量。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在代码生成中存在推理速度与输出质量的关键权衡问题，减少采样步骤通常导致性能灾难性下降。

Method: Saber算法基于两个关键洞察：1）随着代码上下文建立可以自适应加速；2）需要回溯机制来反转生成的标记。

Result: 在多个主流代码生成基准测试中，Saber比主流DLM采样方法平均提升1.9%的Pass@1准确率，同时实现平均251.4%的推理加速。

Conclusion: 通过利用DLM的固有优势，该工作显著缩小了与自回归模型在代码生成中的性能差距。

Abstract: Diffusion language models (DLMs) are emerging as a powerful and promising
alternative to the dominant autoregressive paradigm, offering inherent
advantages in parallel generation and bidirectional context modeling. However,
the performance of DLMs on code generation tasks, which have stronger
structural constraints, is significantly hampered by the critical trade-off
between inference speed and output quality. We observed that accelerating the
code generation process by reducing the number of sampling steps usually leads
to a catastrophic collapse in performance. In this paper, we introduce
efficient Sampling with Adaptive acceleration and Backtracking Enhanced
Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to
achieve better inference speed and output quality in code generation.
Specifically, Saber is motivated by two key insights in the DLM generation
process: 1) it can be adaptively accelerated as more of the code context is
established; 2) it requires a backtracking mechanism to reverse the generated
tokens. Extensive experiments on multiple mainstream code generation benchmarks
show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over
mainstream DLM sampling methods, meanwhile achieving an average 251.4%
inference speedup. By leveraging the inherent advantages of DLMs, our work
significantly narrows the performance gap with autoregressive models in code
generation.

</details>


### [14] [AgentChangeBench: A Multi-Dimensional Evaluation Framework for Goal-Shift Robustness in Conversational AI](https://arxiv.org/abs/2510.18170)
*Manik Rana,Calissa Man,Anotida Expected Msiiwa,Jeffrey Paine,Kevin Zhu,Sunishchal Dev,Vasu Sharma,Ahan M R*

Main category: cs.AI

TL;DR: AgentChangeBench是一个专门评估工具增强语言模型代理在对话中适应目标变化的基准测试，包含2,835个任务序列和四个评估指标，揭示了高准确率不等于动态目标下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界多轮交互中目标变化是常见特征，但现有代理基准主要评估静态目标或一次性工具使用，缺乏对动态目标适应能力的系统评估。

Method: 构建AgentChangeBench基准，包含三个企业领域的2,835个任务序列和五个用户角色，通过四个互补指标（任务成功率、工具使用效率、工具调用冗余率、目标转移恢复时间）进行系统评估。

Result: 评估显示前沿模型在动态目标适应上存在显著差异：GPT-4o在航空预订任务中达到92.2%恢复率，而Gemini降至48.6%；零售任务参数有效性接近完美但冗余率超过80%，揭示主要效率问题。

Conclusion: 高原始准确率并不保证动态目标下的鲁棒性，明确测量恢复时间和冗余率对于评估代理在现实企业环境中的弹性至关重要，AgentChangeBench为诊断和改进代理韧性提供了可复现的测试平台。

Abstract: Goal changes are a defining feature of real world multi-turn interactions,
yet current agent benchmarks primarily evaluate static objectives or one-shot
tool use. We introduce AgentChangeBench, a benchmark explicitly designed to
measure how tool augmented language model agents adapt to mid dialogue goal
shifts across three enterprise domains. Our framework formalizes evaluation
through four complementary metrics: Task Success Rate (TSR) for effectiveness,
Tool Use Efficiency (TUE) for reliability, Tool Call Redundancy Rate (TCRR) for
wasted effort, and Goal-Shift Recovery Time (GSRT) for adaptation latency.
AgentChangeBench comprises 2,835 task sequences and five user personas, each
designed to trigger realistic shift points in ongoing workflows. Using this
setup, we evaluate several frontier models and uncover sharp contrasts obscured
by traditional $\text{pass}@k$ scores: for example, GPT-4o reaches $92.2\%$
recovery on airline booking shifts while Gemini collapses to $48.6\%$, and
retail tasks show near perfect parameter validity yet redundancy rates above
$80\%$, revealing major inefficiencies. These findings demonstrate that high
raw accuracy does not imply robustness under dynamic goals, and that explicit
measurement of recovery time and redundancy is essential. AgentChangeBench
establishes a reproducible testbed for diagnosing and improving agent
resilience in realistic enterprise settings.

</details>


### [15] [Local Coherence or Global Validity? Investigating RLVR Traces in Math Domains](https://arxiv.org/abs/2510.18176)
*Soumya Rani Samineni,Durgesh Kalwar,Vardaan Gangal,Siddhant Bhambri,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 本文研究了基于可验证奖励的强化学习(RLVR)对大型语言模型后训练的影响，特别关注未直接激励的中间推理步骤。研究发现RL后训练提高了推理轨迹的局部连贯性，但这并不保证最终答案的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法通常对所有token一视同仁，仅基于最终答案正确性评估性能，却声称RL后训练能改善推理轨迹。这促使我们研究RL后训练对未直接激励的中间token的影响。

Method: 使用GRPO算法在Qwen-2.5-0.5B模型和GSM8K数据集上进行实验，引入基于一阶逻辑的轨迹连贯性度量来捕捉推理步骤的一致性，区分轨迹有效性和轨迹连贯性。

Result: RL后训练整体提高了轨迹连贯性，在基础模型失败但RL模型成功的问题上改进最显著。令人惊讶的是，RL增强了局部连贯性，但不一定产生有效或正确的解决方案。

Conclusion: RL改善推理的主张需要谨慎审视，因为基于改进的轨迹连贯性可能无法转化为完全有效的数学证明。改进的推理步骤局部连贯性并不保证最终答案正确性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR)-based post-training of
Large Language Models (LLMs) has been shown to improve accuracy on reasoning
tasks and continues to attract significant attention. Existing RLVR methods,
however, typically treat all tokens uniformly without accounting for
token-level advantages. These methods primarily evaluate performance based on
final answer correctness or Pass@K accuracy, and yet make claims about RL
post-training leading to improved reasoning traces. This motivates our
investigation into the effect of RL post-training on intermediate tokens which
are not directly incentivized. To study this, we design an experimental setup
using the GRPO algorithm with Qwen-2.5-0.5B model on the GSM8K dataset. We
introduce trace coherence, a First-Order Logic (FOL)-based measure to capture
the consistency of reasoning steps by identifying errors in the traces. We
distinguish between trace validity and trace coherence, noting that the former
implies logical soundness while the latter measures local coherence via lack of
errors. Our results show that RL post-training overall improves trace coherence
with the most significant gains on problems where the base model fails but the
RL model succeeds. Surprisingly, RL enhances local coherence without
necessarily producing valid or correct solutions. This highlights a crucial
distinction: improved local coherence in reasoning steps does not guarantee
final answer correctness. We argue that claims of improved reasoning via RL
must be examined with care, as these may be based on improved trace coherence,
which may not translate into fully valid mathematical proofs.

</details>


### [16] [FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo](https://arxiv.org/abs/2510.18193)
*Keivan Shariatmadar,Ahmad Osman,Ramin Ray,Usman Dildar,Kisam Kim*

Main category: cs.AI

TL;DR: FST.ai 2.0是一个可解释的AI生态系统，用于支持跆拳道比赛和训练中的裁判、教练和运动员，通过姿态识别、不确定性建模和可视化解释来提升决策透明度和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决奥林匹克和残奥会格斗运动中公平、透明和可解释决策的挑战，提升裁判决策的可信度和运动员评估的准确性。

Method: 集成基于姿态的动作识别（使用图卷积网络）、认知不确定性建模（通过置信集）和可解释性覆盖层，提供交互式仪表板支持人机协作。

Result: 实验验证显示决策审查时间减少85%，裁判对AI辅助决策的信任度达到93%。

Conclusion: 该框架建立了透明且可扩展的管道，用于可信赖的数据驱动裁判和运动员评估，代表了体育领域公平、负责和以人为本的AI发展方向。

Abstract: Fair, transparent, and explainable decision-making remains a critical
challenge in Olympic and Paralympic combat sports. This paper presents
\emph{FST.ai 2.0}, an explainable AI ecosystem designed to support referees,
coaches, and athletes in real time during Taekwondo competitions and training.
The system integrates {pose-based action recognition} using graph convolutional
networks (GCNs), {epistemic uncertainty modeling} through credal sets, and
{explainability overlays} for visual decision support. A set of {interactive
dashboards} enables human--AI collaboration in referee evaluation, athlete
performance analysis, and Para-Taekwondo classification. Beyond automated
scoring, FST.ai~2.0 incorporates modules for referee training, fairness
monitoring, and policy-level analytics within the World Taekwondo ecosystem.
Experimental validation on competition data demonstrates an {85\% reduction in
decision review time} and {93\% referee trust} in AI-assisted decisions. The
framework thus establishes a transparent and extensible pipeline for
trustworthy, data-driven officiating and athlete assessment. By bridging
real-time perception, explainable inference, and governance-aware design,
FST.ai~2.0 represents a step toward equitable, accountable, and human-aligned
AI in sports.

</details>


### [17] [A Definition of AGI](https://arxiv.org/abs/2510.18212)
*Dan Hendrycks,Dawn Song,Christian Szegedy,Honglak Lee,Yarin Gal,Erik Brynjolfsson,Sharon Li,Andy Zou,Lionel Levine,Bo Han,Jie Fu,Ziwei Liu,Jinwoo Shin,Kimin Lee,Mantas Mazeika,Long Phan,George Ingebretsen,Adam Khoja,Cihang Xie,Olawale Salaudeen,Matthias Hein,Kevin Zhao,Alexander Pan,David Duvenaud,Bo Li,Steve Omohundro,Gabriel Alfour,Max Tegmark,Kevin McGrew,Gary Marcus,Jaan Tallinn,Eric Schmidt,Yoshua Bengio*

Main category: cs.AI

TL;DR: 本文提出了一个可量化的AGI评估框架，基于Cattell-Horn-Carroll人类认知理论，将通用智能分解为10个核心认知领域，并应用人类心理测量工具来评估AI系统。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏对AGI的具体定义，当前专业AI与人类水平认知之间的差距难以衡量。

Method: 基于Cattell-Horn-Carroll理论，将通用智能分解为10个认知领域，并采用人类心理测量工具来评估AI系统。

Result: 应用该框架显示当代AI模型具有高度"锯齿状"认知特征：在知识密集型领域表现良好，但在基础认知机制（特别是长期记忆存储）存在严重缺陷。GPT-4得分为27%，GPT-5为58%。

Conclusion: 该框架量化了AGI的发展进程，既显示了快速进步，也明确了距离实现真正AGI仍有显著差距。

Abstract: The lack of a concrete definition for Artificial General Intelligence (AGI)
obscures the gap between today's specialized AI and human-level cognition. This
paper introduces a quantifiable framework to address this, defining AGI as
matching the cognitive versatility and proficiency of a well-educated adult. To
operationalize this, we ground our methodology in Cattell-Horn-Carroll theory,
the most empirically validated model of human cognition. The framework dissects
general intelligence into ten core cognitive domains-including reasoning,
memory, and perception-and adapts established human psychometric batteries to
evaluate AI systems. Application of this framework reveals a highly "jagged"
cognitive profile in contemporary models. While proficient in
knowledge-intensive domains, current AI systems have critical deficits in
foundational cognitive machinery, particularly long-term memory storage. The
resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify
both rapid progress and the substantial gap remaining before AGI.

</details>


### [18] [ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning](https://arxiv.org/abs/2510.18250)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Cancheng Zhang,Xiangdong Zhang,Mingquan Feng,Jingzhi Wang,Junchi Yan*

Main category: cs.AI

TL;DR: 提出ssToken方法，通过自调制和语义感知的token选择来提升大语言模型的监督微调效果，无需额外参考模型，在多个模型家族和规模上优于全数据微调和现有token级选择方法。


<details>
  <summary>Details</summary>
Motivation: 现有token级数据选择方法需要额外参考模型且仅依赖损失信息，无法很好保留语义重要的token。

Method: 使用历史模型计算当前模型的token损失差异作为自调制信号，并引入基于注意力的语义感知token重要性估计指标。

Result: 自调制选择和语义感知选择单独使用均优于全数据微调，两者结合产生协同增益，超越现有token级选择方法。

Conclusion: ssToken通过自调制和语义感知token选择实现了性能提升并保持训练效率，为数据质量优化提供了有效解决方案。

Abstract: Data quality plays a critical role in enhancing supervised fine-tuning (SFT)
for large language models (LLMs), and token-level data selection has emerged as
a promising direction for its fine-grained nature. Despite their strong
empirical performance, existing token-level selection methods share two key
limitations: (1) requiring training or accessing an additional reference model,
and (2) relying solely on loss information for token selection, which cannot
well preserve semantically important tokens that are not favored by loss-based
metrics. To address these challenges, we propose ssToken, a Self-modulated and
Semantic-aware Token Selection approach. ssToken leverages readily accessible
history models to compute the per-token loss difference with the current model,
which serves as a self-modulated signal that enables the model to adaptively
select tokens along its optimization trajectory, rather than relying on excess
loss from an offline-trained reference model as in prior works. We further
introduce a semantic-aware, attention-based token importance estimation metric,
orthogonal to loss-based selection and providing complementary semantic
information for more effective filtering. Extensive experiments across
different model families and scales demonstrate that both self-modulated
selection and semantic-aware selection alone outperform full-data fine-tuning,
while their integration--ssToken--achieves synergistic gains and further
surpasses prior token-level selection methods, delivering performance
improvements while maintaining training efficiency.

</details>


### [19] [Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning](https://arxiv.org/abs/2510.18254)
*Sion Weatherhead,Flora Salim,Aaron Belbasis*

Main category: cs.AI

TL;DR: 当前大型语言模型的'反思'功能与人类反思推理在功能上并不等效，模型在开放但规则约束的任务中表现不佳，反思后改进有限，且经常重复相同的约束违反错误。


<details>
  <summary>Details</summary>
Motivation: 研究当前LLM的'反思'能力是否真正等同于人类的反思推理，特别是在开放但规则约束的任务中测试其自我修正能力。

Method: 在八个前沿模型上测试一个简单但开放且规则约束的真实世界任务：生成有效的科学测试项目，然后基于自我批评进行修订。

Result: 首次尝试表现很差（通常4个项目中0个有效，平均约1个），反思后只有适度改进（也约1个）。第二次尝试经常重复相同的约束违反，表明'修正收益'主要来自偶然产生有效项目而非错误检测和原则性修复。

Conclusion: 当前LLM的'反思'缺乏人类主动、目标驱动的监控机制的证据，可靠性能需要外部结构来强制执行约束。

Abstract: Humans do not just find mistakes after the fact -- we often catch them
mid-stream because 'reflection' is tied to the goal and its constraints.
Today's large language models produce reasoning tokens and 'reflective' text,
but is it functionally equivalent with human reflective reasoning? Prior work
on closed-ended tasks -- with clear, external 'correctness' signals -- can make
'reflection' look effective while masking limits in self-correction. We
therefore test eight frontier models on a simple, real-world task that is
open-ended yet rule-constrained, with auditable success criteria: to produce
valid scientific test items, then revise after considering their own critique.
First-pass performance is poor (often zero valid items out of 4 required; mean
$\approx$ 1), and reflection yields only modest gains (also $\approx$ 1).
Crucially, the second attempt frequently repeats the same violation of
constraint, indicating 'corrective gains' arise largely from chance production
of a valid item rather than error detection and principled,
constraint-sensitive repair. Performance before and after reflection
deteriorates as open-endedness increases, and models marketed for 'reasoning'
show no advantage. Our results suggest that current LLM 'reflection' lacks
functional evidence of the active, goal-driven monitoring that helps humans
respect constraints even on a first pass. Until such mechanisms are
instantiated in the model itself, reliable performance requires external
structure that enforces constraints.

</details>


### [20] [Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming](https://arxiv.org/abs/2510.18314)
*Zheng Zhang,Jiarui He,Yuchen Cai,Deheng Ye,Peilin Zhao,Ruili Feng,Hao Wang*

Main category: cs.AI

TL;DR: 提出了Genesis框架，通过遗传算法和混合策略表示生成对抗性注入，动态发现并编译有效攻击策略，持续提升对网络代理的攻击效果。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型代理越来越多地自动化复杂网络任务，它们在提高生产力的同时也引入了新的安全风险。现有红队方法主要依赖手动制定的攻击策略或离线训练的静态模型，难以捕捉网络代理的行为模式，无法在不同环境中泛化。

Method: 提出Genesis框架，包含三个模块：攻击者（使用遗传算法和混合策略表示生成对抗性注入）、评分器（评估目标网络代理的响应提供反馈）、策略师（从交互日志中动态发现有效策略并编译到持续增长的战略库中）。

Result: 在各种网络任务上的广泛实验表明，该框架能够发现新颖策略，并始终优于现有的攻击基线方法。

Conclusion: Genesis框架通过动态策略发现和演化机制，有效解决了网络代理攻击中的泛化问题，显著提升了攻击效果。

Abstract: As large language model (LLM) agents increasingly automate complex web tasks,
they boost productivity while simultaneously introducing new security risks.
However, relevant studies on web agent attacks remain limited. Existing
red-teaming approaches mainly rely on manually crafted attack strategies or
static models trained offline. Such methods fail to capture the underlying
behavioral patterns of web agents, making it difficult to generalize across
diverse environments. In web agent attacks, success requires the continuous
discovery and evolution of attack strategies. To this end, we propose Genesis,
a novel agentic framework composed of three modules: Attacker, Scorer, and
Strategist. The Attacker generates adversarial injections by integrating the
genetic algorithm with a hybrid strategy representation. The Scorer evaluates
the target web agent's responses to provide feedback. The Strategist
dynamically uncovers effective strategies from interaction logs and compiles
them into a continuously growing strategy library, which is then re-deployed to
enhance the Attacker's effectiveness. Extensive experiments across various web
tasks show that our framework discovers novel strategies and consistently
outperforms existing attack baselines.

</details>


### [21] [Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning](https://arxiv.org/abs/2510.18318)
*Aaron Bell,Amit Aides,Amr Helmy,Arbaaz Muslim,Aviad Barzilai,Aviv Slobodkin,Bolous Jaber,David Schottlander,George Leifman,Joydeep Paul,Mimi Sun,Nadav Sherman,Natalie Williams,Per Bjornsson,Roy Lee,Ruth Alcantara,Thomas Turnbull,Tomer Shekel,Vered Silverman,Yotam Gigi,Adam Boulanger,Alex Ottenwess,Ali Ahmadalipour,Anna Carter,Charles Elliott,David Andre,Elad Aharoni,Gia Jung,Hassler Thurston,Jacob Bien,Jamie McPike,Juliet Rothenberg,Kartik Hegde,Kel Markert,Kim Philipp Jablonski,Luc Houriez,Monica Bharel,Phing VanLee,Reuven Sayag,Sebastian Pilarski,Shelley Cazares,Shlomi Pasternak,Siduo Jiang,Stone Jiang,Thomas Colthurst,Yang Chen,Yehonathan Refael,Yochai Blau,Yuval Carny,Yael Maguire,Avinatan Hassidim,James Manyika,Tim Thelin,Genady Beryozkin,Gautam Prasad,Luke Barrington,Yossi Matias,Niv Efron,Shravya Shetty*

Main category: cs.AI

TL;DR: 本文介绍了Earth AI，一个结合地理空间AI模型和智能推理的系统，通过三个关键领域的基础模型和Gemini驱动的推理引擎，解决地理数据分析的挑战。


<details>
  <summary>Details</summary>
Motivation: 地理空间数据具有巨大潜力，但其庞大的数量、多样性以及不同的分辨率、时间尺度和稀疏性给深入分析和解释带来了重大挑战。

Method: 构建了三个关键领域的基础模型（行星尺度影像、人口、环境）和Gemini驱动的智能推理引擎，开发了能够联合推理多个基础模型、大型地理空间数据源和工具的Gemini驱动代理。

Result: 在严格基准测试中展示了基础模型的能力，验证了它们在地理空间推理中的互补价值，并在真实世界危机场景的新基准上，代理能够提供关键及时的洞察。

Conclusion: Earth AI系统有效弥合了原始地理空间数据与可操作理解之间的差距，解锁了对地球的新颖深刻洞察。

Abstract: Geospatial data offers immense potential for understanding our planet.
However, the sheer volume and diversity of this data along with its varied
resolutions, timescales, and sparsity pose significant challenges for thorough
analysis and interpretation. This paper introduces Earth AI, a family of
geospatial AI models and agentic reasoning that enables significant advances in
our ability to unlock novel and profound insights into our planet. This
approach is built upon foundation models across three key domains--Planet-scale
Imagery, Population, and Environment--and an intelligent Gemini-powered
reasoning engine. We present rigorous benchmarks showcasing the power and novel
capabilities of our foundation models and validate that when used together,
they provide complementary value for geospatial inference and their synergies
unlock superior predictive capabilities. To handle complex, multi-step queries,
we developed a Gemini-powered agent that jointly reasons over our multiple
foundation models along with large geospatial data sources and tools. On a new
benchmark of real-world crisis scenarios, our agent demonstrates the ability to
deliver critical and timely insights, effectively bridging the gap between raw
geospatial data and actionable understanding.

</details>


### [22] [ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.18342)
*Peng Tang,Xiaoxiao Yan,Xiaobin Hu,Yuning Cui,Donghao Luo,Jiangning Zhang,Pengcheng Xu,Jinlong Peng,Qingdong He,Feiyue Huang,Song Xue,Tobias Lasser*

Main category: cs.AI

TL;DR: 提出了ShortcutBreaker框架，通过低秩噪声瓶颈和全局扰动注意力解决多类无监督异常检测中的身份捷径问题，在多个数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 多类无监督异常检测需要统一模型处理多个类别，但现有Transformer架构存在身份捷径问题，即直接复制输入到输出，导致正常与异常样本的重建误差差异变小，难以区分。

Method: 1. 基于矩阵秩不等式设计低秩噪声瓶颈，将高维特征投影到低秩潜在空间，防止平凡身份复制；2. 利用ViT的全局建模能力，引入全局扰动注意力防止解码器中的信息捷径。

Result: 在四个基准数据集上测试：MVTec-AD(99.8%)、ViSA(98.9%)、Real-IAD(90.6%)和Universal Medical(87.8%)的图像级AUROC，均优于现有MUAD方法。

Conclusion: ShortcutBreaker通过解决身份捷径问题，在多类无监督异常检测任务中实现了显著性能提升，验证了所提方法的有效性。

Abstract: Multi-class unsupervised anomaly detection (MUAD) has garnered growing
research interest, as it seeks to develop a unified model for anomaly detection
across multiple classes, i.e., eliminating the need to train separate models
for distinct objects and thereby saving substantial computational resources.
Under the MUAD setting, while advanced Transformer-based architectures have
brought significant performance improvements, identity shortcuts persist: they
directly copy inputs to outputs, narrowing the gap in reconstruction errors
between normal and abnormal cases, and thereby making the two harder to
distinguish. Therefore, we propose ShortcutBreaker, a novel unified
feature-reconstruction framework for MUAD tasks, featuring two key innovations
to address the issue of shortcuts. First, drawing on matrix rank inequality, we
design a low-rank noisy bottleneck (LRNB) to project highdimensional features
into a low-rank latent space, and theoretically demonstrate its capacity to
prevent trivial identity reproduction. Second, leveraging ViTs global modeling
capability instead of merely focusing on local features, we incorporate a
global perturbation attention to prevent information shortcuts in the decoders.
Extensive experiments are performed on four widely used anomaly detection
benchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD)
and one medical dataset (Universal Medical). The proposed method achieves a
remarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four
datasets, respectively, consistently outperforming previous MUAD methods across
different scenarios.

</details>


### [23] [Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games](https://arxiv.org/abs/2510.18395)
*Runnan Qi,Yanan Ni,Lumin Jiang,Zongyuan Li,Kuihua Huang,Xian Guo*

Main category: cs.AI

TL;DR: 提出了Memory-Augmented State Machine Prompting (MASMP)框架，用于解决LLM智能体在实时策略游戏中的幻觉和决策碎片化问题，通过状态机提示和记忆机制实现结构化行动与长期战术一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在幻觉和碎片化决策问题，需要统一结构化行动与长期战术连贯性。

Method: 结合自然语言驱动的状态机架构和轻量级记忆模块，通过提示引导LLM模拟有限状态机和行为树，保持战略变量跨决策周期的连续性。

Result: 在StarCraft II实验中，对最强内置AI (Lv7)达到60%胜率，远超基线(0%)，有效解决了"知行差距"问题。

Conclusion: 建立了结合神经和符号AI在复杂决策中的新范式，实现了可解释性和类似FSM的可靠性。

Abstract: This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel
framework for LLM agents in real-time strategy games. Addressing key challenges
like hallucinations and fragmented decision-making in existing approaches,
MASMP integrates state machine prompting with memory mechanisms to unify
structured actions with long-term tactical coherence. The framework features:
(1) a natural language-driven state machine architecture that guides LLMs to
emulate finite state machines and behavior trees through prompts, and (2) a
lightweight memory module preserving strategic variables (e.g., tactics,
priority units) across decision cycles. Experiments in StarCraft II demonstrate
MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly
outperforming baselines (0%). Case studies reveal the method retains LLMs'
semantic comprehension while resolving the "Knowing-Doing Gap" through strict
state-action mapping, achieving both interpretability and FSM-like reliability.
This work establishes a new paradigm for combining neural and symbolic AI in
complex decision-making.

</details>


### [24] [Heterogeneous Adversarial Play in Interactive Environments](https://arxiv.org/abs/2510.18407)
*Manjie Xu,Xinyi Yang,Jiayu Zhan,Wei Liang,Chi Zhang,Yixin Zhu*

Main category: cs.AI

TL;DR: 提出Heterogeneous Adversarial Play (HAP)框架，通过对抗性自动课程学习实现教师-学生的不对称交互，解决传统自博弈在开放学习场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统自博弈方法在零和竞争场景中依赖智能体对称性，但在开放学习场景中存在固有不对称性。人类教学系统展示了不对称的教学框架，教师根据学生发展轨迹系统构建挑战。

Method: HAP将教师-学生交互形式化为极小极大优化，任务生成教师和问题解决学生通过对抗动态共同进化。建立双向反馈系统，教师根据实时学习者表现指标持续重新校准任务复杂度。

Result: 在多任务学习领域的实验验证表明，该框架达到最先进基线的性能水平，同时生成的课程提高了人工智能体和人类受试者的学习效率。

Conclusion: HAP框架成功实现了不对称、自适应教学机制在人工智能系统中的操作化，能够自主合成适当课程而无需预定义任务层次结构。

Abstract: Self-play constitutes a fundamental paradigm for autonomous skill
acquisition, whereby agents iteratively enhance their capabilities through
self-directed environmental exploration. Conventional self-play frameworks
exploit agent symmetry within zero-sum competitive settings, yet this approach
proves inadequate for open-ended learning scenarios characterized by inherent
asymmetry. Human pedagogical systems exemplify asymmetric instructional
frameworks wherein educators systematically construct challenges calibrated to
individual learners' developmental trajectories. The principal challenge
resides in operationalizing these asymmetric, adaptive pedagogical mechanisms
within artificial systems capable of autonomously synthesizing appropriate
curricula without predetermined task hierarchies. Here we present Heterogeneous
Adversarial Play (HAP), an adversarial Automatic Curriculum Learning framework
that formalizes teacher-student interactions as a minimax optimization wherein
task-generating instructor and problem-solving learner co-evolve through
adversarial dynamics. In contrast to prevailing ACL methodologies that employ
static curricula or unidirectional task selection mechanisms, HAP establishes a
bidirectional feedback system wherein instructors continuously recalibrate task
complexity in response to real-time learner performance metrics. Experimental
validation across multi-task learning domains demonstrates that our framework
achieves performance parity with SOTA baselines while generating curricula that
enhance learning efficacy in both artificial agents and human subjects.

</details>


### [25] [Deep Learning-Based Control Optimization for Glass Bottle Forming](https://arxiv.org/abs/2510.18412)
*Mattia Pujatti,Andrea Di Luca,Nicola Peghini,Federico Monegaglia,Marco Cristoforetti*

Main category: cs.AI

TL;DR: 提出了一种基于深度学习的控制算法，用于优化玻璃瓶制造中的成型过程，通过神经网络预测参数变化的影响并确定最佳机器设置。


<details>
  <summary>Details</summary>
Motivation: 在玻璃瓶制造中，成型机器的精确控制对于确保质量和减少缺陷至关重要，需要优化成型过程。

Method: 使用来自实际生产工厂的运行数据，通过神经网络预测参数变化的影响，并设计特定的反演机制来确定实现所需玻璃料特性的最佳机器设置。

Result: 在多个生产线的历史数据集上的实验结果表明，该方法产生了有希望的结果，表明具有增强过程稳定性、减少浪费和提高产品一致性的潜力。

Conclusion: 这些结果突显了深度学习在玻璃制造过程控制中的潜力。

Abstract: In glass bottle manufacturing, precise control of forming machines is
critical for ensuring quality and minimizing defects. This study presents a
deep learning-based control algorithm designed to optimize the forming process
in real production environments. Using real operational data from active
manufacturing plants, our neural network predicts the effects of parameter
changes based on the current production setup. Through a specifically designed
inversion mechanism, the algorithm identifies the optimal machine settings
required to achieve the desired glass gob characteristics. Experimental results
on historical datasets from multiple production lines show that the proposed
method yields promising outcomes, suggesting potential for enhanced process
stability, reduced waste, and improved product consistency. These results
highlight the potential of deep learning to process control in glass
manufacturing.

</details>


### [26] [Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents](https://arxiv.org/abs/2510.18424)
*Guangfu Guo,Xiaoqian Lu,Yue Feng*

Main category: cs.AI

TL;DR: 提出Med-VRAgent框架，结合视觉引导、自奖励和蒙特卡洛树搜索，提升视觉语言模型在医学推理中的表现，并通过PPO微调进一步优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在医学推理中存在的幻觉、模糊描述、逻辑不一致和定位能力差等问题。

Method: 基于视觉引导和自奖励范式，结合蒙特卡洛树搜索，使用PPO目标对VLMs进行微调。

Result: 在多个医学VQA基准测试中表现优于现有方法。

Conclusion: Med-VRAgent框架有效提升了VLMs在医学视觉推理任务中的性能。

Abstract: Visual Language Models (VLMs) achieve promising results in medical reasoning
but struggle with hallucinations, vague descriptions, inconsistent logic and
poor localization. To address this, we propose a agent framework named Medical
Visual Reasoning Agent (\textbf{Med-VRAgent}). The approach is based on Visual
Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By
combining the Visual Guidance with tree search, Med-VRAgent improves the
medical visual reasoning capabilities of VLMs. We use the trajectories
collected by Med-VRAgent as feedback to further improve the performance by
fine-tuning the VLMs with the proximal policy optimization (PPO) objective.
Experiments on multiple medical VQA benchmarks demonstrate that our method
outperforms existing approaches.

</details>


### [27] [Automated urban waterlogging assessment and early warning through a mixture of foundation models](https://arxiv.org/abs/2510.18425)
*Chenxu Zhang,Fuxiang Huang,Lei Zhang*

Main category: cs.AI

TL;DR: UWAssess是一个基于基础模型的城市内涝评估框架，能够自动识别监控图像中的积水区域并生成结构化评估报告，解决了传统人工监测方法的不足。


<details>
  <summary>Details</summary>
Motivation: 随着气候变化加剧，城市内涝对全球公共安全和基础设施构成日益严重的威胁。现有监测方法主要依赖人工报告，无法提供及时全面的评估。

Method: 采用基础模型驱动框架，设计了半监督微调策略和思维链提示策略，以解决标注数据稀缺问题，释放基础模型在数据稀缺下游任务中的潜力。

Result: 在具有挑战性的视觉基准测试中显示出感知性能的显著提升。基于GPT的评估证实了UWAssess生成可靠文本报告的能力，能够准确描述积水范围、深度、风险和影响。

Conclusion: 这种双重能力使内涝监测从感知转向生成，而多基础模型的协作框架为智能可扩展系统奠定了基础，支持城市管理、灾害响应和气候韧性。

Abstract: With climate change intensifying, urban waterlogging poses an increasingly
severe threat to global public safety and infrastructure. However, existing
monitoring approaches rely heavily on manual reporting and fail to provide
timely and comprehensive assessments. In this study, we present Urban
Waterlogging Assessment (UWAssess), a foundation model-driven framework that
automatically identifies waterlogged areas in surveillance images and generates
structured assessment reports. To address the scarcity of labeled data, we
design a semi-supervised fine-tuning strategy and a chain-of-thought (CoT)
prompting strategy to unleash the potential of the foundation model for
data-scarce downstream tasks. Evaluations on challenging visual benchmarks
demonstrate substantial improvements in perception performance. GPT-based
evaluations confirm the ability of UWAssess to generate reliable textual
reports that accurately describe waterlogging extent, depth, risk and impact.
This dual capability enables a shift of waterlogging monitoring from perception
to generation, while the collaborative framework of multiple foundation models
lays the groundwork for intelligent and scalable systems, supporting urban
management, disaster response and climate resilience.

</details>


### [28] [AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library](https://arxiv.org/abs/2510.18428)
*Minwei Kong,Ao Qu,Xiaotong Guo,Wenbin Ouyang,Chonghe Jiang,Han Zheng,Yining Ma,Dingyi Zhuang,Yuhan Tang,Junyi Li,Hai Wang,Cathy Wu,Jinhua Zhao*

Main category: cs.AI

TL;DR: AlphaOPT是一个自改进的经验库系统，使LLM能够从有限演示和求解器反馈中学习优化建模，无需标注推理轨迹或参数更新。


<details>
  <summary>Details</summary>
Motivation: 优化建模在工业决策中至关重要但难以自动化，现有LLM方法依赖脆弱提示或代价高昂的重训练且泛化能力有限。

Method: 采用持续两阶段循环：库学习阶段从失败尝试中提取求解器验证的结构化洞察；库演化阶段诊断检索偏差并精炼存储洞察的适用条件。

Result: AlphaOPT随数据增加稳步改进（从100到300训练项，性能从65%提升到72%），在仅使用答案训练时，在OOD OptiBench数据集上超越最强基线7.7%。

Conclusion: 该方法能够高效从有限演示中学习而无需精心设计的推理，通过更新库而非模型权重实现持续扩展，使知识明确且可解释。

Abstract: Optimization modeling enables critical decisions across industries but
remains difficult to automate: informal language must be mapped to precise
mathematical formulations and executable solver code. Prior LLM approaches
either rely on brittle prompting or costly retraining with limited
generalization. We present AlphaOPT, a self-improving experience library that
enables an LLM to learn from limited demonstrations (even answers alone,
without gold-standard programs) and solver feedback - without annotated
reasoning traces or parameter updates. AlphaOPT operates in a continual
two-phase cycle: (i) a Library Learning phase that reflects on failed attempts,
extracting solver-verified, structured insights as {taxonomy, condition,
explanation, example}; and (ii) a Library Evolution phase that diagnoses
retrieval misalignments and refines the applicability conditions of stored
insights, improving transfer across tasks. This design (1) learns efficiently
from limited demonstrations without curated rationales, (2) expands continually
without costly retraining by updating the library rather than model weights,
and (3) makes knowledge explicit and interpretable for human inspection and
intervention. Experiments show that AlphaOPT steadily improves with more data
(65% to 72% from 100 to 300 training items) and surpasses the strongest
baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only
on answers. Code and data are available at:
https://github.com/Minw913/AlphaOPT.

</details>


### [29] [PlanU: Large Language Model Decision Making through Planning under Uncertainty](https://arxiv.org/abs/2510.18442)
*Ziwei Deng,Mian Deng,Chenjing Liang,Zeming Gao,Chennan Ma,Chenxing Lin,Haipeng Zhang,Songzhu Mei,Cheng Wang,Siqi Shen*

Main category: cs.AI

TL;DR: 提出了PlanU方法，在MCTS中通过分位数分布建模回报分布，使用UCC评分平衡探索与利用，解决LLM决策中的不确定性挑战


<details>
  <summary>Details</summary>
Motivation: LLM在不确定性环境下的决策表现不佳，现有方法要么只关注LLM不确定性而忽略环境不确定性，要么不适合多步决策任务

Method: 基于MCTS的规划方法，将节点回报建模为分位数分布，引入UCC评分来估计节点不确定性并平衡探索与利用

Result: 实验证明PlanU在不确定性环境下的LLM决策任务中表现有效

Conclusion: PlanU通过结合分位数分布和UCC评分，成功解决了LLM决策中的不确定性挑战

Abstract: Large Language Models (LLMs) are increasingly being explored across a range
of decision-making tasks. However, LLMs sometimes struggle with decision-making
tasks under uncertainty that are relatively easy for humans, such as planning
actions in stochastic environments. The adoption of LLMs for decision-making is
impeded by uncertainty challenges, such as LLM uncertainty and environmental
uncertainty. LLM uncertainty arises from the stochastic sampling process
inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM
uncertainty through multiple reasoning chains or search trees. However, these
approaches overlook environmental uncertainty, which leads to poor performance
in environments with stochastic state transitions. Some recent LDM approaches
deal with uncertainty by forecasting the probability of unknown variables.
However, they are not designed for multi-step decision-making tasks that
require interaction with the environment. To address uncertainty in LLM
decision-making, we introduce PlanU, an LLM-based planning method that captures
uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of
each node in the MCTS as a quantile distribution, which uses a set of quantiles
to represent the return distribution. To balance exploration and exploitation
during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity
(UCC) score which estimates the uncertainty of MCTS nodes. Through extensive
experiments, we demonstrate the effectiveness of PlanU in LLM-based
decision-making tasks under uncertainty.

</details>


### [30] [CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs](https://arxiv.org/abs/2510.18470)
*Shaobo Wang,Yongliang Miao,Yuancheng Liu,and Qianli Ma,Ning Liao,Linfeng Zhang*

Main category: cs.AI

TL;DR: CircuitSeer：一种基于模型内部注意力机制的数据选择方法，通过识别核心推理电路来选择高质量训练数据，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法依赖昂贵的外部模型或不透明的启发式方法，需要转向利用模型内部机制来更有效地选择推理数据

Method: 发现复杂推理任务会激活稀疏的专用注意力头，形成核心推理电路。CircuitSeer通过测量数据对这些关键电路的影响来量化推理复杂度

Result: 在4个模型和9个数据集上的实验显示，CircuitSeer仅使用10%的数据就能在Qwen2.5-Math-7B上获得比全数据集训练高1.4个百分点的Pass@1平均提升

Conclusion: CircuitSeer通过利用模型内部推理电路实现了高效的数据选择，显著减少了训练成本同时提升了模型性能

Abstract: Large language models (LLMs) have demonstrated impressive reasoning
capabilities, but scaling their performance often relies on massive reasoning
datasets that are computationally expensive to train on. Existing data
selection methods aim to curate smaller, high-quality subsets but often rely on
costly external models or opaque heuristics. In this work, we shift the focus
from external heuristics to the model's internal mechanisms. We find that
complex reasoning tasks consistently activate a sparse, specialized subset of
attention heads, forming core reasoning circuits. Building on this insight, we
propose CircuitSeer, a novel data selection method that quantifies the
reasoning complexity of data by measuring its influence on these crucial
circuits. Extensive experiments on 4 models and 9 datasets demonstrate
CircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of
data selected by our method achieves a 1.4-point gain in average Pass@1 over
training on the full dataset, highlighting its efficiency and effectiveness.

</details>


### [31] [Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents](https://arxiv.org/abs/2510.18476)
*Feifan Xia,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 提出了一种用于多轮社交对话中LLM代理的概率意图建模框架，通过维护对伙伴潜在意图的信念分布，实现自适应对话策略。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM代理在多轮社交对话中理解伙伴意图的不确定性，提升社交智能对话能力。

Method: 基于上下文先验初始化意图信念分布，通过每轮对话后的似然估计动态更新，为策略提供额外的上下文基础。

Result: 在SOTOPIA环境中，相比Qwen2.5-7B基线，总体得分提升9.0%（SOTOPIA-All）和4.1%（SOTOPIA-Hard），甚至略微超过直接观察伙伴意图的oracle代理。

Conclusion: 概率意图建模有助于开发社交智能LLM代理，在不确定性下实现更好的对话适应性。

Abstract: We present a probabilistic intent modeling framework for large language model
(LLM) agents in multi-turn social dialogue. The framework maintains a belief
distribution over a partner's latent intentions, initialized from contextual
priors and dynamically updated through likelihood estimation after each
utterance. The evolving distribution provides additional contextual grounding
for the policy, enabling adaptive dialogue strategies under uncertainty.
Preliminary experiments in the SOTOPIA environment show consistent
improvements: the proposed framework increases the Overall score by 9.0% on
SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and
slightly surpasses an oracle agent that directly observes partner intentions.
These early results suggest that probabilistic intent modeling can contribute
to the development of socially intelligent LLM agents.

</details>


### [32] [LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources](https://arxiv.org/abs/2510.18477)
*Haichao Ji,Zibo Wang,Yifei Zhu,Meng han,Dan Wang,Zhu Han*

Main category: cs.AI

TL;DR: LAFA是首个将基于LLM代理的数据分析与联邦分析相结合的系统，通过分层多代理架构将自然语言查询转换为优化的可执行FA工作流，实现隐私保护的自然语言数据分析。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理分析框架假设集中式数据访问，缺乏隐私保护；而联邦分析虽然支持隐私保护计算，但需要结构化查询且不支持自然语言输入。

Method: 引入分层多代理架构：粗粒度规划器分解复杂查询为子查询，细粒度规划器将子查询映射为FA操作的有向无环图，优化代理重写合并多个DAG以消除冗余操作。

Result: 实验表明LAFA在基线提示策略上表现更优，实现了更高的执行计划成功率，并大幅减少了资源密集型FA操作。

Conclusion: 这项工作为支持自然语言输入的联邦分析环境中的隐私保护、LLM驱动分析建立了实用基础。

Abstract: Large Language Models (LLMs) have shown great promise in automating data
analytics tasks by interpreting natural language queries and generating
multi-operation execution plans. However, existing LLM-agent-based analytics
frameworks operate under the assumption of centralized data access, offering
little to no privacy protection. In contrast, federated analytics (FA) enables
privacy-preserving computation across distributed data sources, but lacks
support for natural language input and requires structured, machine-readable
queries. In this work, we present LAFA, the first system that integrates
LLM-agent-based data analytics with FA. LAFA introduces a hierarchical
multi-agent architecture that accepts natural language queries and transforms
them into optimized, executable FA workflows. A coarse-grained planner first
decomposes complex queries into sub-queries, while a fine-grained planner maps
each subquery into a Directed Acyclic Graph of FA operations using prior
structural knowledge. To improve execution efficiency, an optimizer agent
rewrites and merges multiple DAGs, eliminating redundant operations and
minimizing computational and communicational overhead. Our experiments
demonstrate that LAFA consistently outperforms baseline prompting strategies by
achieving higher execution plan success rates and reducing resource-intensive
FA operations by a substantial margin. This work establishes a practical
foundation for privacy-preserving, LLM-driven analytics that supports natural
language input in the FA setting.

</details>


### [33] [StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking](https://arxiv.org/abs/2510.18483)
*Haoran Zhang,Chenhao Zhu,Sicong Guo,Hanzhe Guo,Haiming Li,Donglin Yu*

Main category: cs.AI

TL;DR: StarBench是一个基于《崩坏：星穹铁道》的回合制RPG基准测试，评估视觉语言模型在从像素到动作的多模态决策和主动信息寻求方面的能力，包含直接控制和工具辅助控制两种模式。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在真实客户端中实现人类级别的游戏能力——将原始截图映射到时间一致的低级动作，并决定何时寻求指导——仍然是一个开放挑战。

Method: 设计了StarBench基准测试，包含8个战斗任务和两种控制模式：直接控制（仅接收截图并输出低级操作）和工具辅助控制（可使用检测器和OCR输出）。还包括ask-or-act诊断来测量代理何时选择寻求指导。

Result: 结果显示在直接控制模式下感知到控制的保真度存在显著差距，而明智的信息寻求与改进的成功率相关。

Conclusion: StarBench为真实客户端游戏中的主动信息寻求和多模态决策提供了可复现的衡量标准。

Abstract: Human players do more than press buttons: they ground what they see on screen
into precise keyboard-mouse actions and, when stuck, they seek information
before trying again. We ask whether current vision-language models (VLMs) can
do the same. Despite encouraging results under simplified control or tool
scaffolds, human-like play in a real client - mapping raw screenshots to
temporally coherent low-level actions while deciding when to ask for guidance -
remains an open challenge. We introduce StarBench, a turn-based RPG benchmark
derived from Honkai: Star Rail that targets these two human-like competencies:
multimodal decision-making from pixels to actions and agentic information
seeking. StarBench standardizes evaluation across eight combat tasks and two
regimes with shared tasks and metrics: (i) direct control, where agents receive
only screenshots and must emit low-level primitives (click and keypress) with
no semantic hints; and (ii) tool-assisted control, where higher-level intents
can be mapped to primitives by detectors and OCR outputs provide optional
textualized observations to ease UI grounding. To mirror human practice,
StarBench also includes an ask-or-act diagnostic that measures whether and when
agents choose to request brief guidance before proceeding, and how that choice
affects subsequent performance. We report reference baselines for contemporary
VLMs and a human reference. Results expose sizable gaps in
perception-to-control fidelity in the direct regime, while showing that
judicious information seeking correlates with improved success, establishing
StarBench as a reproducible yardstick for agentic information seeking and
multimodal decision-making in real-client play.

</details>


### [34] [AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification](https://arxiv.org/abs/2510.18488)
*Ho Fai Leung,Xiaoyan Xi,Fei Zuo*

Main category: cs.AI

TL;DR: 研究发现现有GUI代理基准测试AndroidControl存在缺陷，通过改进创建了AndroidControl-Curated基准，使SOTA模型在复杂任务上的成功率从60%提升至75%。同时开发了仅需少量训练数据的轻量级模型Magma-R1-3B，性能媲美大200倍的模型。


<details>
  <summary>Details</summary>
Motivation: 当前设备端虚拟助手依赖僵化的API，GUI代理提供替代方案但被认为性能不足。研究发现问题不仅在于模型，更在于基准测试本身的缺陷。

Method: 识别AndroidControl基准的模糊性和事实错误，通过严格净化流程改进为AndroidControl-Curated。开发轻量级模型Magma-R1-3B，仅用2.4k精选样本进行后训练。

Result: 在改进后的基准上，SOTA模型在复杂任务上的成功率从约60%提升至近75%。Magma-R1-3B模型参数仅为3B，性能却与235B的Qwen3-VL相当。

Conclusion: 设备端GUI代理的实际能力被低估，通过改进基准测试和开发高效模型，证明其更接近实际部署。发布新基准和模型以促进研究发展。

Abstract: On-device virtual assistants like Siri and Google Assistant are increasingly
pivotal, yet their capabilities are hamstrung by a reliance on rigid,
developer-dependent APIs. GUI agents offer a powerful, API-independent
alternative, but their adoption is hindered by the perception of poor
performance, as even the best models (e.g. Qwen3-VL-235B) scores are capped at
around 60% on benchmarks like AndroidControl, far from viability for real-world
use. Our research reveals that issue lies not only with the models but with the
benchmarks themselves. We identified notable shortcomings in AndroidControl,
including ambiguities and factual errors, which systematically underrates agent
capabilities. To address this critical oversight, we enhanced AndroidControl
into AndroidControl-Curated, a refined version of the benchmark improved
through a rigorous purification pipeline. On this enhanced benchmark,
state-of-the-art models achieve success rates nearing 75% on complex tasks (15%
improvement), reflecting that on-device GUI agents are actually closer to
practical deployment than previously thought. We introduce our new SOTA model,
Magma-R1- 3B, post-trained on just 2.4k curated samples using 60 hours of an
H20 GPU (approximately $60). Despite being 200 times smaller in parameters,
this model delivers performance comparable to Qwen3- VL-235B. We release both
AndroidControl-Curated benchmark and Magma-R1 model to the research community,
encouraging adoption of this enhanced benchmark to better reflect model
capabilities and accelerate the development of robust, on-device virtual
assistants.

</details>


### [35] [Crucible: Quantifying the Potential of Control Algorithms through LLM Agents](https://arxiv.org/abs/2510.18491)
*Lianchen Jia,Chaoyang Li,Qian Houde,Tianchi Huang,Jiangchuan Liu,Lifeng Sun*

Main category: cs.AI

TL;DR: 提出Crucible框架，使用LLM驱动的多级专家模拟来量化算法的调优潜力，为算法分析和设计提供新维度


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注算法在理想或默认配置下的性能，忽略了调优潜力这一关键方面

Method: 使用LLM驱动的多级专家模拟来调优算法，并定义形式化指标定量评估调优潜力

Result: 在从经典控制任务到复杂计算机系统的广泛案例研究中验证了Crucible的有效性，并在真实部署中验证了其发现

Conclusion: Crucible系统性地量化了不同算法的可调空间，为算法分析和设计提供了新维度，最终带来性能提升

Abstract: Control algorithms in production environments typically require domain
experts to tune their parameters and logic for specific scenarios. However,
existing research predominantly focuses on algorithmic performance under ideal
or default configurations, overlooking the critical aspect of Tuning Potential.
To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven,
multi-level expert simulation to turn algorithms and defines a formalized
metric to quantitatively evaluate their Tuning Potential. We demonstrate
Crucible's effectiveness across a wide spectrum of case studies, from classic
control tasks to complex computer systems, and validate its findings in a
real-world deployment. Our experimental results reveal that Crucible
systematically quantifies the tunable space across different algorithms.
Furthermore, Crucible provides a new dimension for algorithm analysis and
design, which ultimately leads to performance improvements. Our code is
available at https://github.com/thu-media/Crucible.

</details>


### [36] [Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models](https://arxiv.org/abs/2510.18526)
*Hanze Guo,Jing Yao,Xiao Zhou,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: COUPLE是一个基于反事实推理的框架，用于解决大语言模型与多元人类价值观对齐的挑战，通过结构因果模型处理价值观的复杂性和可引导性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在不同文化、社区和人口统计群体中的应用日益增多，需要超越平均原则（如HHH），将模型与多元人类价值观对齐。现有方法在处理细粒度价值目标时面临价值观复杂性和可引导性两大挑战。

Method: 提出COUPLE框架，使用结构因果模型（SCM）来表征特征间的复杂相互依赖关系和优先级，以及高层次价值维度与行为间的因果关系。应用反事实推理生成符合任意期望价值目标的输出。

Result: 在两个具有不同价值体系的数据集上评估COUPLE，结果显示COUPLE在多种类型价值目标上优于其他基线方法。

Conclusion: COUPLE通过显式因果建模不仅提升了价值观对齐性能，还提供了更好的可解释性，能够有效处理多元价值观的复杂性和可引导性问题。

Abstract: As large language models (LLMs) become increasingly integrated into
applications serving users across diverse cultures, communities and
demographics, it is critical to align LLMs with pluralistic human values beyond
average principles (e.g., HHH). In psychological and social value theories such
as Schwartz's Value Theory, pluralistic values are represented by multiple
value dimensions paired with various priorities. However, existing methods
encounter two challenges when aligning with such fine-grained value objectives:
1) they often treat multiple values as independent and equally important,
ignoring their interdependence and relative priorities (value complexity); 2)
they struggle to precisely control nuanced value priorities, especially those
underrepresented ones (value steerability). To handle these challenges, we
propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE
alignment. It introduces a structural causal model (SCM) to feature complex
interdependency and prioritization among features, as well as the causal
relationship between high-level value dimensions and behaviors. Moreover, it
applies counterfactual reasoning to generate outputs aligned with any desired
value objectives. Benefitting from explicit causal modeling, COUPLE also
provides better interpretability. We evaluate COUPLE on two datasets with
different value systems and demonstrate that COUPLE advances other baselines
across diverse types of value objectives.

</details>


### [37] [Physics-guided Emulators Reveal Resilience and Fragility under Operational Latencies and Outages](https://arxiv.org/abs/2510.18535)
*Sarth Dubey,Subimal Ghosh,Udit Bhatia*

Main category: cs.AI

TL;DR: 开发了一个可操作就绪的全球洪水预警系统模拟器，结合长短时记忆网络和松弛水量平衡约束，在数据延迟、缺失或不一致时保持稳定性。


<details>
  <summary>Details</summary>
Motivation: 可靠的水文和洪水预报需要模型在输入数据延迟、缺失或不一致时保持稳定，但大多数降雨径流预测的进展都在理想数据条件下评估，强调准确性而非操作弹性。

Method: 结合长短时记忆网络与松弛水量平衡约束，开发五种架构覆盖从完整历史预报到数据延迟和中断的不同信息可用性场景，在美国最小管理流域训练并在5000多个流域测试。

Result: 模拟器重现了GloFAS的水文核心，随着信息质量下降而平稳退化，在对比水文气候和管理制度间的迁移产生降低但物理一致的性能。

Conclusion: 该框架将操作稳健性确立为水文机器学习可测量的属性，推进了可靠实时预报系统的设计。

Abstract: Reliable hydrologic and flood forecasting requires models that remain stable
when input data are delayed, missing, or inconsistent. However, most advances
in rainfall-runoff prediction have been evaluated under ideal data conditions,
emphasizing accuracy rather than operational resilience. Here, we develop an
operationally ready emulator of the Global Flood Awareness System (GloFAS) that
couples long- and short-term memory networks with a relaxed water-balance
constraint to preserve physical coherence. Five architectures span a continuum
of information availability: from complete historical and forecast forcings to
scenarios with data latency and outages, allowing systematic evaluation of
robustness. Trained in minimally managed catchments across the United States
and tested in more than 5,000 basins, including heavily regulated rivers in
India, the emulator reproduces the hydrological core of GloFAS and degrades
smoothly as information quality declines. Transfer across contrasting
hydroclimatic and management regimes yields reduced yet physically consistent
performance, defining the limits of generalization under data scarcity and
human influence. The framework establishes operational robustness as a
measurable property of hydrological machine learning and advances the design of
reliable real-time forecasting systems.

</details>


### [38] [SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation](https://arxiv.org/abs/2510.18551)
*Yuncheng Hua,Sion Weatherhead,Mehdi Jafari,Hao Xue,Flora D. Salim*

Main category: cs.AI

TL;DR: SOCIA-Nabla是一个端到端的智能体框架，将模拟器构建视为文本计算图中的代码实例优化，通过专门的LLM驱动智能体作为图节点，执行代码合成→执行→评估→代码修复的损失驱动循环。


<details>
  <summary>Details</summary>
Motivation: 将脆弱的提示管道转换为可复现、约束感知的模拟器代码生成，统一多智能体编排与损失对齐的优化视角，实现跨领域和模拟粒度的扩展。

Method: 使用文本梯度下降(TGD)优化器，在文本计算图中嵌入专门的LLM驱动智能体作为节点，工作流管理器执行损失驱动循环：代码合成→执行→评估→代码修复，保留人在环交互用于任务规范确认。

Result: 在三个CPS任务（用户建模、口罩采用和个人移动性）中，SOCIA-Nabla实现了最先进的整体准确率。

Conclusion: SOCIA-Nabla通过将代码本身作为可训练对象，最小化专家工作量，将脆弱的提示管道转换为可复现、约束感知的模拟器代码生成，实现了跨领域和模拟粒度的扩展。

Abstract: In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that
treats simulator construction asinstance optimization over code within a
textual computation graph. Specialized LLM-driven agents are embedded as graph
nodes, and a workflow manager executes a loss-driven loop: code synthesis ->
execution -> evaluation -> code repair. The optimizer performs Textual-Gradient
Descent (TGD), while human-in-the-loop interaction is reserved for task-spec
confirmation, minimizing expert effort and keeping the code itself as the
trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption,
and Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy.
By unifying multi-agent orchestration with a loss-aligned optimization view,
SOCIA-Nabla converts brittle prompt pipelines into reproducible,
constraint-aware simulator code generation that scales across domains and
simulation granularities. This work is under review, and we will release the
code soon.

</details>


### [39] [Extracting alignment data in open models](https://arxiv.org/abs/2510.18554)
*Federico Barbero,Xiangming Gu,Christopher A. Choquette-Choo,Chawin Sitawarin,Matthew Jagielski,Itay Yona,Petar Veličković,Ilia Shumailov,Jamie Hayes*

Main category: cs.AI

TL;DR: 研究表明可以从经过对齐训练后的模型中提取大量对齐训练数据，这些数据可用于提升模型的长上下文推理、安全性、指令遵循和数学能力。使用嵌入模型比字符串匹配更适合检测语义相似性。


<details>
  <summary>Details</summary>
Motivation: 揭示对齐训练数据提取的风险，以及蒸馏实践可能带来的间接训练原始数据集的问题。

Method: 使用高质量的嵌入模型来衡量语义相似性，而不是传统的字符串匹配方法，从后训练模型（如SFT或RL）中提取训练数据。

Result: 成功从对齐训练后的模型中提取了大量训练数据，这些数据可用于训练基础模型并恢复相当程度的原始性能。字符串匹配方法会严重低估可提取的数据量（约10倍）。

Conclusion: 这项工作暴露了提取对齐数据的潜在风险，并引发了关于蒸馏实践下游影响的讨论，因为模型似乎在复现其训练集的某些方面。

Abstract: In this work, we show that it is possible to extract significant amounts of
alignment training data from a post-trained model -- useful to steer the model
to improve certain capabilities such as long-context reasoning, safety,
instruction following, and maths. While the majority of related work on
memorisation has focused on measuring success of training data extraction
through string matching, we argue that embedding models are better suited for
our specific goals. Distances measured through a high quality embedding model
can identify semantic similarities between strings that a different metric such
as edit distance will struggle to capture. In fact, in our investigation,
approximate string matching would have severely undercounted (by a conservative
estimate of $10\times$) the amount of data that can be extracted due to trivial
artifacts that deflate the metric. Interestingly, we find that models readily
regurgitate training data that was used in post-training phases such as SFT or
RL. We show that this data can be then used to train a base model, recovering a
meaningful amount of the original performance. We believe our work exposes a
possibly overlooked risk towards extracting alignment data. Finally, our work
opens up an interesting discussion on the downstream effects of distillation
practices: since models seem to be regurgitating aspects of their training set,
distillation can therefore be thought of as indirectly training on the model's
original dataset.

</details>


### [40] [QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework](https://arxiv.org/abs/2510.18569)
*Junhyeog Yun,Hyoun Jun Lee,Insu Jeon*

Main category: cs.AI

TL;DR: QuantEvolve是一个结合质量多样性优化和假设驱动策略生成的进化框架，用于自动化开发量化交易策略，能够在动态市场中保持策略多样性并适应市场变化。


<details>
  <summary>Details</summary>
Motivation: 动态市场中自动化开发量化交易策略具有挑战性，现有方法难以在探索广阔策略空间的同时保持多样性，而个性化投资解决方案的需求日益增长。

Method: 使用与投资者偏好对齐的特征图（策略类型、风险特征、换手率、收益特征等）来维持多样化有效策略，并整合假设驱动的多智能体系统通过迭代生成和评估来系统探索策略空间。

Result: 实证结果表明QuantEvolve优于传统基线方法，验证了其有效性，并发布了进化策略数据集支持未来研究。

Conclusion: QuantEvolve能够产生多样化、复杂的策略，既能适应市场机制变化，又能满足个人投资需求，为量化交易策略开发提供了有效解决方案。

Abstract: Automating quantitative trading strategy development in dynamic markets is
challenging, especially with increasing demand for personalized investment
solutions. Existing methods often fail to explore the vast strategy space while
preserving the diversity essential for robust performance across changing
market conditions. We present QuantEvolve, an evolutionary framework that
combines quality-diversity optimization with hypothesis-driven strategy
generation. QuantEvolve employs a feature map aligned with investor
preferences, such as strategy type, risk profile, turnover, and return
characteristics, to maintain a diverse set of effective strategies. It also
integrates a hypothesis-driven multi-agent system to systematically explore the
strategy space through iterative generation and evaluation. This approach
produces diverse, sophisticated strategies that adapt to both market regime
shifts and individual investment needs. Empirical results show that QuantEvolve
outperforms conventional baselines, validating its effectiveness. We release a
dataset of evolved strategies to support future research.

</details>


### [41] [VAR: Visual Attention Reasoning via Structured Search and Backtracking](https://arxiv.org/abs/2510.18619)
*Wei Cai,Jian Zhao,Yuchen Yuan,Tianle Zhang,Ming Zhu,Haichuan Tang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出VAR框架，将视觉推理重构为结构化搜索过程，通过证据追踪和搜索式思维链生成解决多模态大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在高幻觉倾向和脆弱的线性推理过程，导致复杂任务失败。

Method: 将推理过程分解为可追踪的证据基础和搜索式思维链生成，包含回溯机制进行自我修正，使用多维度奖励函数进行语义和几何自验证。

Result: 7B模型VAR-7B在幻觉和安全基准测试中创下新纪录，显著优于开源模型，与领先专有系统表现相当。

Conclusion: VAR框架通过结构化搜索和自验证机制有效解决了多模态大语言模型的幻觉问题，提升了推理可靠性。

Abstract: Multimodal Large Language Models (MLLMs), despite their advances, are
hindered by their high hallucination tendency and heavy reliance on brittle,
linear reasoning processes, leading to failures in complex tasks. To address
these limitations, we introduce Visual Attention Reasoning (VAR), a novel
framework that recasts grounded reasoning as a structured search over a
reasoning trajectory space. VAR decomposes the reasoning process into two key
stages: traceable evidence grounding and search-based chain-of-thought (CoT)
generation, which incorporates a backtracking mechanism for self-correction.
The search is guided by a multi-faceted reward function with semantic and
geometric self-verification components, which penalize outputs that are not
faithfully grounded in the visual input. We provide a theoretical analysis for
our search strategy, validating its capability to find the correct solution
with high probability. Experimental results show that our 7B model, VAR-7B,
sets a new state-of-the-art on a comprehensive suite of hallucination and
safety benchmarks, significantly outperforming existing open-source models and
demonstrating competitive performance against leading proprietary systems.

</details>


### [42] [Leveraging Association Rules for Better Predictions and Better Explanations](https://arxiv.org/abs/2510.18628)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.AI

TL;DR: 提出一种结合数据和知识的分类方法，通过数据挖掘获取关联规则来提升树模型的预测性能和解释能力。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法主要依赖数据，缺乏领域知识的融入。本文旨在结合数据挖掘得到的关联规则来增强树模型的性能。

Method: 从数据中挖掘关联规则（可能包含否定），将这些规则整合到决策树和随机森林中，用于提升分类性能和生成更泛化的解释。

Result: 实验表明该方法能提高树模型的预测性能，并生成更简洁的解释。

Conclusion: 结合数据和知识的分类方法在预测性能和解释质量方面都有显著优势。

Abstract: We present a new approach to classification that combines data and knowledge.
In this approach, data mining is used to derive association rules (possibly
with negations) from data. Those rules are leveraged to increase the predictive
performance of tree-based models (decision trees and random forests) used for a
classification task. They are also used to improve the corresponding
explanation task through the generation of abductive explanations that are more
general than those derivable without taking such rules into account.
Experiments show that for the two tree-based models under consideration,
benefits can be offered by the approach in terms of predictive performance and
in terms of explanation sizes.

</details>


### [43] [Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises](https://arxiv.org/abs/2510.18631)
*Carlo Proietti,Antonio Yuste-Ginel*

Main category: cs.AI

TL;DR: 本文研究了形式论证中定性不确定性的建模问题，比较了抽象模型和结构化模型在表达不确定性方面的表达能力，并给出了正反两方面的表达性结果。


<details>
  <summary>Details</summary>
Motivation: 在形式论证中建模定性不确定性对于实际应用和理论理解都至关重要，但现有工作大多关注抽象模型。本文旨在研究这些抽象模型的合理实例化。

Method: 通过将论证的不确定性基于其组成部分（规则和前提）来构建结构化模型，引入了一种能够处理抽象和结构化形式的表达性概念，并比较了抽象模型和结构化模型的表达性。

Result: 提出了正反两方面的表达性结果，影响了不完整抽象论证框架及其依赖扩展（抽象方面）和ASPIC+（结构化方面）。

Conclusion: 本文为形式论证中不确定性建模提供了理论分析，比较了不同方法的表达性，对相关领域的发展具有重要意义。

Abstract: Modelling qualitative uncertainty in formal argumentation is essential both
for practical applications and theoretical understanding. Yet, most of the
existing works focus on \textit{abstract} models for arguing with uncertainty.
Following a recent trend in the literature, we tackle the open question of
studying plausible instantiations of these abstract models. To do so, we ground
the uncertainty of arguments in their components, structured within rules and
premises. Our main technical contributions are: i) the introduction of a notion
of expressivity that can handle abstract and structured formalisms, and ii) the
presentation of both negative and positive expressivity results, comparing the
expressivity of abstract and structured models of argumentation with
uncertainty. These results affect incomplete abstract argumentation frameworks,
and their extension with dependencies, on the abstract side, and ASPIC+, on the
structured side.

</details>


### [44] [Query Decomposition for RAG: Balancing Exploration-Exploitation](https://arxiv.org/abs/2510.18633)
*Roxana Petcu,Kenton Murray,Daniel Khashabi,Evangelos Kanoulas,Maarten de Rijke,Dawn Lawrie,Kevin Duh*

Main category: cs.AI

TL;DR: 该论文提出了一种基于多臂老虎机框架的检索增强生成系统，通过动态选择信息量最大的子查询来平衡检索的广度和效率，显著提升了文档检索精度和长文本生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成系统中平衡检索广度与效率的关键权衡问题，既要捕获所有相关材料，又要避免过度检索带来的噪声和计算成本。

Method: 将查询分解和文档检索建模为利用-探索设置，使用多种老虎机学习方法动态选择最有信息量的子查询，并利用排名信息和人工判断来估计文档相关性。

Result: 使用排名信息和人工判断估计文档相关性，实现了文档级精度35%的提升，α-nDCG指标15%的增长，并在长文本生成任务上表现更好。

Conclusion: 基于老虎机学习的动态子查询选择方法能有效平衡检索的利用与探索，显著提升检索增强生成系统的性能。

Abstract: Retrieval-augmented generation (RAG) systems address complex user requests by
decomposing them into subqueries, retrieving potentially relevant documents for
each, and then aggregating them to generate an answer. Efficiently selecting
informative documents requires balancing a key trade-off: (i) retrieving
broadly enough to capture all the relevant material, and (ii) limiting
retrieval to avoid excessive noise and computational cost. We formulate query
decomposition and document retrieval in an exploitation-exploration setting,
where retrieving one document at a time builds a belief about the utility of a
given sub-query and informs the decision to continue exploiting or exploring an
alternative. We experiment with a variety of bandit learning methods and
demonstrate their effectiveness in dynamically selecting the most informative
sub-queries. Our main finding is that estimating document relevance using rank
information and human judgments yields a 35% gain in document-level precision,
15% increase in {\alpha}-nDCG, and better performance on the downstream task of
long-form generation.

</details>


### [45] [Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval](https://arxiv.org/abs/2510.18659)
*Dong Yun,Marco Schouten,Dim Papadopoulos*

Main category: cs.AI

TL;DR: SherlockLLM是一个基于强化学习的对话驱动检索框架，通过生成二进制问题序列来高效缩小搜索空间，解决信息检索中用户查询模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 信息检索中用户查询往往具有模糊性，现有对话式交互检索系统缺乏明确的策略来询问最有信息量的问题，导致效率低下。

Method: 使用强化学习训练代理生成二进制问题序列，无需大规模标注对话数据，通过优化提问策略来高效缩小搜索空间。

Result: 在结构化任务中性能与强基线相当，接近二分搜索的理论最优；在非结构化任务中显著优于基线，展示了学习高效信息寻求对话策略的能力。

Conclusion: SherlockLLM是一个鲁棒且高效的解决方案，能够学习高度有效的信息寻求对话策略，在模糊查询的信息检索任务中表现优异。

Abstract: User queries in information retrieval are often ambiguous, making it
challenging for systems to identify a user's target from a single query. While
recent dialogue-based interactive retrieval systems can clarify user intent,
they are inefficient as they often lack an explicit strategy to ask the most
informative questions. To address this limitation, we propose SherlockLLM, a
dialogue-driven retrieval framework that learns an optimal questioning strategy
via Reinforcement Learning (RL) and avoids the need for large-scale annotated
dialogue data. In our framework, an agent is trained to generate a sequence of
binary questions to efficiently narrow down the search space. To validate our
approach, we introduce a benchmark with both structured and unstructured tasks.
Experimental results show that SherlockLLM is a robust and efficient solution.
On the structured tasks, its performance matches strong baselines and
approaches the theoretical optimal defined by binary search. On the challenging
unstructured task, our agent significantly outperforms these baselines,
showcasing its ability to learn a highly effective information-seeking dialogue
policy.

</details>


### [46] [Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation](https://arxiv.org/abs/2510.18751)
*Patterson Hsieh,Jerry Yeh,Mao-Chi He,Wen-Han Hsieh,Elvis Hsieh*

Main category: cs.AI

TL;DR: ALGOS是一个用于有害藻华监测的分割与推理系统，结合遥感图像理解和严重程度估计，通过GeoSAM辅助人工评估生成高质量分割掩码，并基于NASA数据微调视觉语言模型进行严重程度预测。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了有害藻华的发生，特别是蓝藻，威胁水生生态系统和人类健康。传统监测方法劳动密集且时空覆盖有限，需要开发可扩展的AI驱动解决方案。

Method: 结合GeoSAM辅助人工评估进行高质量分割掩码生成，并基于NASA的CAML数据集微调视觉语言模型进行严重程度预测。

Result: ALGOS在分割和严重程度估计方面均表现出稳健性能。

Conclusion: 该系统为实现实用和自动化的蓝藻监测系统铺平了道路。

Abstract: Climate change is intensifying the occurrence of harmful algal bloom (HAB),
particularly cyanobacteria, which threaten aquatic ecosystems and human health
through oxygen depletion, toxin release, and disruption of marine biodiversity.
Traditional monitoring approaches, such as manual water sampling, remain
labor-intensive and limited in spatial and temporal coverage. Recent advances
in vision-language models (VLMs) for remote sensing have shown potential for
scalable AI-driven solutions, yet challenges remain in reasoning over imagery
and quantifying bloom severity. In this work, we introduce ALGae Observation
and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB
monitoring that combines remote sensing image understanding with severity
estimation. Our approach integrates GeoSAM-assisted human evaluation for
high-quality segmentation mask curation and fine-tunes vision language model on
severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)
from NASA. Experiments demonstrate that ALGOS achieves robust performance on
both segmentation and severity-level estimation, paving the way toward
practical and automated cyanobacterial monitoring systems.

</details>


### [47] [Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location](https://arxiv.org/abs/2510.18803)
*Shirin Tavakoli Kafiabad,Andrea Schiffauerova,Ashkan Ebadi*

Main category: cs.AI

TL;DR: 该研究比较了LDA、STM和BERTopic三种主题建模方法在分析加拿大NSERC研究提案中的应用，并开发了COFFEE算法来增强BERTopic的协变量分析能力，揭示了省级研究专业化和性别主题模式。


<details>
  <summary>Details</summary>
Motivation: 优化国家科学投资需要了解研究趋势演变及人口地理因素影响，特别是在关注公平、多样性和包容性的背景下。

Method: 分析了18年NSERC资助的研究提案，比较LDA、STM和BERTopic三种主题建模方法，并开发COFFEE算法来增强BERTopic的协变量分析。

Result: BERTopic在识别更细粒度、连贯和新兴主题方面表现最佳，如人工智能的快速扩张；协变量分析揭示了省级研究专业化和跨学科一致的性别主题模式。

Conclusion: 这些发现为资助机构制定更公平和有效的资助策略提供了实证基础，有助于提升科学生态系统的效率。

Abstract: Optimizing national scientific investment requires a clear understanding of
evolving research trends and the demographic and geographical forces shaping
them, particularly in light of commitments to equity, diversity, and inclusion.
This study addresses this need by analyzing 18 years (2005-2022) of research
proposals funded by the Natural Sciences and Engineering Research Council of
Canada (NSERC). We conducted a comprehensive comparative evaluation of three
topic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic
Modelling (STM), and BERTopic. We also introduced a novel algorithm, named
COFFEE, designed to enable robust covariate effect estimation for BERTopic.
This advancement addresses a significant gap, as BERTopic lacks a native
function for covariate analysis, unlike the probabilistic STM. Our findings
highlight that while all models effectively delineate core scientific domains,
BERTopic outperformed by consistently identifying more granular, coherent, and
emergent themes, such as the rapid expansion of artificial intelligence.
Additionally, the covariate analysis, powered by COFFEE, confirmed distinct
provincial research specializations and revealed consistent gender-based
thematic patterns across various scientific disciplines. These insights offer a
robust empirical foundation for funding organizations to formulate more
equitable and impactful funding strategies, thereby enhancing the effectiveness
of the scientific ecosystem.

</details>
