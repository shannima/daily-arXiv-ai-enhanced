<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Test-time Verification via Optimal Transport: Coverage, ROC, & Sub-optimality](https://arxiv.org/abs/2510.18982)
*Arpan Mukherjee,Marcello Bullo,Debabrota Basu,Deniz Gündüz*

Main category: cs.AI

TL;DR: 本文提出了一个统一框架来分析验证式测试时扩展中生成器覆盖率、验证器收敛区域和采样算法次优性之间的几何交互关系，揭示了次优性-覆盖率曲线的三个区域。


<details>
  <summary>Details</summary>
Motivation: 虽然带验证的测试时扩展在提升大语言模型性能方面显示出潜力，但验证器的作用及其不完美性仍未得到充分探索。现有研究只捕捉了这些因素的部分子集，缺乏量化它们交互几何的统一框架。

Method: 将可验证的测试时扩展构建为传输问题，通过分析生成器覆盖率、验证器收敛区域和采样算法次优性之间的交互关系，提出并分析了两类采样算法——顺序采样和批量采样。

Result: 理论分析揭示了次优性-覆盖率曲线存在三个区域：传输区域（次优性随覆盖率增加）、策略改进区域（次优性可能随覆盖率减少，取决于验证器收敛区域）和饱和区域（次优性趋于平稳）。在Qwen、Llama和Gemma模型上的实证结果验证了理论发现。

Conclusion: 该研究为理解验证式测试时扩展中的关键因素交互提供了统一框架，揭示了不同区域的行为特征，对优化大语言模型的测试时性能具有指导意义。

Abstract: While test-time scaling with verification has shown promise in improving the
performance of large language models (LLMs), the role of the verifier and its
imperfections remain underexplored. The effect of verification manifests
through interactions of three quantities: (i) the generator's coverage, (ii)
the verifier's region of convergence (ROC), and (iii) the sampling algorithm's
sub-optimality. Though recent studies capture subsets of these factors, a
unified framework quantifying the geometry of their interplay is missing. We
frame verifiable test-time scaling as a transport problem. This characterizes
the interaction of coverage, ROC, and sub-optimality, and uncovers that the
sub-optimality--coverage curve exhibits three regimes. A transport regime --
where sub-optimality increases with coverage, a policy improvement regime --
where sub-optimality may decrease with coverage, depending on the verifier's
ROC, and a saturation regime -- where sub-optimality plateaus, unaffected by
coverage. We further propose and analyze two classes of sampling algorithms --
sequential and batched, and examine how their computational complexities shape
these trade-offs. Empirical results with Qwen, Llama, and Gemma models
corroborate our theoretical findings.

</details>


### [2] [Timely Clinical Diagnosis through Active Test Selection](https://arxiv.org/abs/2510.18988)
*Silas Ruhrberg Estévez,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: ACTMED是一个结合贝叶斯实验设计和大型语言模型的临床诊断框架，通过逐步选择能最大程度减少诊断不确定性的测试来优化诊断过程。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习诊断方法依赖静态数据集，无法模拟临床医生在实践中的顺序性、资源感知推理过程，特别是在高压或资源有限环境中需要更有效的诊断支持。

Method: 将贝叶斯实验设计与大型语言模型结合，LLMs作为灵活的模拟器生成患者状态分布并支持信念更新，无需特定任务的训练数据。

Result: 在真实数据集上的评估显示，ACTMED能够优化测试选择，提高诊断准确性、可解释性和资源利用效率。

Conclusion: 该框架是实现透明、自适应且与临床医生对齐的诊断系统的重要一步，能够跨环境泛化并减少对领域特定数据的依赖。

Abstract: There is growing interest in using machine learning (ML) to support clinical
diag- nosis, but most approaches rely on static, fully observed datasets and
fail to reflect the sequential, resource-aware reasoning clinicians use in
practice. Diagnosis remains complex and error prone, especially in
high-pressure or resource-limited settings, underscoring the need for
frameworks that help clinicians make timely and cost-effective decisions. We
propose ACTMED (Adaptive Clinical Test selection via Model-based Experimental
Design), a diagnostic framework that integrates Bayesian Experimental Design
(BED) with large language models (LLMs) to better emulate real-world diagnostic
reasoning. At each step, ACTMED selects the test expected to yield the greatest
reduction in diagnostic uncertainty for a given patient. LLMs act as flexible
simulators, generating plausible patient state distributions and supporting
belief updates without requiring structured, task-specific training data.
Clinicians can remain in the loop; reviewing test suggestions, interpreting
intermediate outputs, and applying clinical judgment throughout. We evaluate
ACTMED on real-world datasets and show it can optimize test selection to
improve diagnostic accuracy, interpretability, and resource use. This
represents a step to- ward transparent, adaptive, and clinician-aligned
diagnostic systems that generalize across settings with reduced reliance on
domain-specific data.

</details>


### [3] [Rectifying Shortcut Behaviors in Preference-based Reward Learning](https://arxiv.org/abs/2510.19050)
*Wenqian Ye,Guangtao Zheng,Aidong Zhang*

Main category: cs.AI

TL;DR: 提出PRISM方法解决基于偏好的奖励模型中的捷径行为问题，通过不变核学习提高奖励模型的泛化能力


<details>
  <summary>Details</summary>
Motivation: 基于人类反馈的强化学习中，偏好奖励模型容易出现过优化和奖励黑客问题，模型会利用训练数据中的虚假特征（如回答冗长、讨好语气等）来获取高分，而非真正反映人类意图

Method: 受核视角不变理论启发，提出PRISM方法，通过闭式学习目标学习具有不变性的核函数和特征映射

Result: 在多个基准测试中，该方法能持续提高奖励模型在分布外任务上的准确性，并减少下游策略模型对捷径的依赖

Conclusion: PRISM为基于偏好的对齐提供了一个鲁棒的框架，能有效缓解奖励模型中的捷径行为问题

Abstract: In reinforcement learning from human feedback, preference-based reward models
play a central role in aligning large language models to human-aligned
behavior. However, recent studies show that these models are prone to reward
hacking and often fail to generalize well due to over-optimization. They
achieve high reward scores by exploiting shortcuts, that is, exploiting
spurious features (e.g., response verbosity, agreeable tone, or sycophancy)
that correlate with human preference labels in the training data rather than
genuinely reflecting the intended objectives. In this paper, instead of probing
these issues one at a time, we take a broader view of the reward hacking
problem as shortcut behaviors and introduce a principled yet flexible approach
to mitigate shortcut behaviors in preference-based reward learning. Inspired by
the invariant theory in the kernel perspective, we propose Preference-based
Reward Invariance for Shortcut Mitigation (PRISM), which learns group-invariant
kernels with feature maps in a closed-form learning objective. Experimental
results in several benchmarks show that our method consistently improves the
accuracy of the reward model on diverse out-of-distribution tasks and reduces
the dependency on shortcuts in downstream policy models, establishing a robust
framework for preference-based alignment.

</details>


### [4] [The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS](https://arxiv.org/abs/2510.19055)
*Brandon James Carone,Iran R. Roman,Pablo Ripollés*

Main category: cs.AI

TL;DR: MUSE基准测试评估多模态大语言模型的音乐理解能力，发现当前模型在关系推理方面存在严重缺陷，与人类专家存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型的音频理解评估可能掩盖了其在关系推理方面的根本弱点，需要更全面的基准测试来揭示这些缺陷。

Method: 开发了MUSE基准测试，包含10个任务来探测基础音乐感知技能，评估了4个SOTA模型（Gemini Pro和Flash、Qwen2.5-Omni、Audio-Flamingo 3）并与200人的人类基线进行比较。

Result: SOTA模型能力差异很大，与人类专家存在持续差距。Gemini Pro在基础感知上表现良好，但Qwen和Audio Flamingo 3表现接近随机水平，显示出严重的感知缺陷。思维链提示效果不一致且通常有害。

Conclusion: MUSE基准测试为评估不变音乐表征和开发更鲁棒的AI系统提供了关键工具。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated capabilities in
audio understanding, but current evaluations may obscure fundamental weaknesses
in relational reasoning. We introduce the Music Understanding and Structural
Evaluation (MUSE) Benchmark, an open-source resource with 10 tasks designed to
probe fundamental music perception skills. We evaluate four SOTA models (Gemini
Pro and Flash, Qwen2.5-Omni, and Audio-Flamingo 3) against a large human
baseline (N=200). Our results reveal a wide variance in SOTA capabilities and a
persistent gap with human experts. While Gemini Pro succeeds on basic
perception, Qwen and Audio Flamingo 3 perform at or near chance, exposing
severe perceptual deficits. Furthermore, we find Chain-of-Thought (CoT)
prompting provides inconsistent, often detrimental results. Our work provides a
critical tool for evaluating invariant musical representations and driving
development of more robust AI systems.

</details>


### [5] [A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist](https://arxiv.org/abs/2510.19139)
*Sohyeon Jeon,Hyung-Chul Lee*

Main category: cs.AI

TL;DR: 该研究评估了大型语言模型根据CONSORT标准评估临床试验报告的能力，发现不同模型和提示条件下存在认知策略差异，揭示了当前系统在临床合规自动化中的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在医疗领域快速扩展，但其根据CONSORT标准评估临床试验报告的能力仍不明确，特别是在认知和推理策略方面。

Method: 采用行为和元认知分析方法，使用专家验证数据，在三种提示条件下系统比较了两个代表性LLMs。

Result: 模型在处理不同CONSORT项目和提示类型时表现出明显差异，包括推理风格转变、明确的不确定性和替代解释模式。

Conclusion: 结果强调了这些系统在临床合规自动化中的当前局限性，并突出了理解其认知适应和策略行为对于开发更可解释和可靠的医疗AI的重要性。

Abstract: Despite the rapid expansion of Large Language Models (LLMs) in healthcare,
the ability of these systems to assess clinical trial reporting according to
CONSORT standards remains unclear, particularly with respect to their cognitive
and reasoning strategies. This study applies a behavioral and metacognitive
analytic approach with expert-validated data, systematically comparing two
representative LLMs under three prompt conditions. Clear differences emerged in
how the models approached various CONSORT items, and prompt types, including
shifts in reasoning style, explicit uncertainty, and alternative
interpretations shaped response patterns. Our results highlight the current
limitations of these systems in clinical compliance automation and underscore
the importance of understanding their cognitive adaptations and strategic
behavior in developing more explainable and reliable medical AI.

</details>


### [6] [The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models](https://arxiv.org/abs/2510.19176)
*Yuqiao Tan,Shizhu He,Kang Liu,Jun Zhao*

Main category: cs.AI

TL;DR: 论文分析了推理模型中的模式选择问题，将其识别为早期退出问题的更具挑战性变体，发现现有方法在有限信息场景下难以有效解决模式选择问题。


<details>
  <summary>Details</summary>
Motivation: 推理模型在数学和逻辑推理任务中表现出色，但逐步思考过程会导致过度思考，产生不必要的计算开销。为了减少计算负担，需要研究模式选择和早期退出方法。

Method: 通过实证研究比较了九种基线方法，包括基于提示的方法和利用模型内部信息的方法，分析了它们在模式选择任务中的表现。

Result: 基于提示的方法由于分类能力有限且仅依赖少量手工制作信息而经常失败；利用内部信息的方法在大多数场景中表现更好，但仍存在稳定性问题。

Conclusion: 仅依赖模型提供信息的现有方法不足以在有限信息场景下有效解决模式选择问题，这凸显了该任务的持续挑战性。

Abstract: Reasoning models have demonstrated exceptional performance in tasks such as
mathematics and logical reasoning, primarily due to their ability to engage in
step-by-step thinking during the reasoning process. However, this often leads
to overthinking, resulting in unnecessary computational overhead. To address
this issue, Mode Selection aims to automatically decide between Long-CoT
(Chain-of-Thought) or Short-CoT by utilizing either a Thinking or NoThinking
mode. Simultaneously, Early Exit determines the optimal stopping point during
the iterative reasoning process. Both methods seek to reduce the computational
burden. In this paper, we first identify Mode Selection as a more challenging
variant of the Early Exit problem, as they share similar objectives but differ
in decision timing. While Early Exit focuses on determining the best stopping
point for concise reasoning at inference time, Mode Selection must make this
decision at the beginning of the reasoning process, relying on pre-defined fake
thoughts without engaging in an explicit reasoning process, referred to as
zero-step thinking. Through empirical studies on nine baselines, we observe
that prompt-based approaches often fail due to their limited classification
capabilities when provided with minimal hand-crafted information. In contrast,
approaches that leverage internal information generally perform better across
most scenarios but still exhibit issues with stability. Our findings indicate
that existing methods relying solely on the information provided by models are
insufficient for effectively addressing Mode Selection in scenarios with
limited information, highlighting the ongoing challenges of this task. Our code
is available at https://github.com/Trae1ounG/Zero_Step_Thinking.

</details>


### [7] [WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation](https://arxiv.org/abs/2510.19205)
*Yaoyao Qian,Yuanli Wang,Jinda Zhang,Yun Zong,Meixu Chen,Hanhan Zhou,Jindan Huang,Yifan Zeng,Xinyu Hu,Chan Hee Song,Danqing Zhang*

Main category: cs.AI

TL;DR: WebGraphEval是一个将多个智能体轨迹抽象为统一加权动作图的框架，用于评估网页智能体，能捕获跨模型规律性并识别关键决策点。


<details>
  <summary>Details</summary>
Motivation: 当前网页智能体评估主要依赖二元成功指标或单一参考轨迹，忽略了基准数据集中的结构多样性。

Method: 将多个智能体的轨迹抽象为统一的加权动作图，进行规范编码、合并重复行为，并应用结构分析包括奖励传播和成功加权边统计。

Result: 对六个网页智能体的数千条轨迹评估显示，图抽象能捕获跨模型规律性、突出冗余和低效，并识别被结果指标忽略的关键决策点。

Conclusion: 通过将网页交互框架化为图结构数据，WebGraphEval建立了多路径、跨智能体和效率感知的网页智能体评估通用方法。

Abstract: Current evaluation of web agents largely reduces to binary success metrics or
conformity to a single reference trajectory, ignoring the structural diversity
present in benchmark datasets. We present WebGraphEval, a framework that
abstracts trajectories from multiple agents into a unified, weighted action
graph. This representation is directly compatible with benchmarks such as
WebArena, leveraging leaderboard runs and newly collected trajectories without
modifying environments. The framework canonically encodes actions, merges
recurring behaviors, and applies structural analyses including reward
propagation and success-weighted edge statistics. Evaluations across thousands
of trajectories from six web agents show that the graph abstraction captures
cross-model regularities, highlights redundancy and inefficiency, and
identifies critical decision points overlooked by outcome-based metrics. By
framing web interaction as graph-structured data, WebGraphEval establishes a
general methodology for multi-path, cross-agent, and efficiency-aware
evaluation of web agents.

</details>


### [8] [ChatGPT Unveils Its Limits: Principles of Law Deliver Checkmate](https://arxiv.org/abs/2510.19261)
*Marianna Molinari,Ilaria Angela Amantea,Marinella Quaranta,Guido Governatori*

Main category: cs.AI

TL;DR: 本研究通过法律领域实验评估ChatGPT表现，发现即使具备相关知识能力，ChatGPT仍无法整合推理得出全面结果，揭示了其在复杂问题分解和综合解决方面的根本局限。


<details>
  <summary>Details</summary>
Motivation: 评估ChatGPT在法律领域的实际性能，特别是与正则表达式基线对比，而非仅与人类表现比较，以揭示其真正的能力边界。

Method: 在法律领域设计实验，将ChatGPT与正则表达式(Regex)方法进行性能对比，分析其在法律决策关键段落提取任务中的表现。

Result: ChatGPT即使拥有必要知识和能力，也无法有效整合推理得出全面结果，在法律原则提取任务中表现出根本性局限。

Conclusion: 真正的智能包含分解复杂问题并运用多种能力提供统一全面解决方案的能力，这在法律领域仍是人类独有的特质，人工智能目前存在根本性限制。

Abstract: This study examines the performance of ChatGPT with an experiment in the
legal domain. We compare the outcome with it a baseline using regular
expressions (Regex), rather than focusing solely on the assessment against
human performance. The study reveals that even if ChatGPT has access to the
necessary knowledge and competencies, it is unable to assemble them, reason
through, in a way that leads to an exhaustive result. This unveils a major
limitation of ChatGPT. Intelligence encompasses the ability to break down
complex issues and address them according to multiple required competencies,
providing a unified and comprehensive solution. In the legal domain, one of the
most crucial tasks is reading legal decisions and extracting key passages
condensed from principles of law (PoLs), which are then incorporated into
subsequent rulings by judges or defense documents by lawyers. In performing
this task, artificial intelligence lacks an all-encompassing understanding and
reasoning, which makes it inherently limited. Genuine intelligence, remains a
uniquely human trait, at least in this particular field.

</details>


### [9] [An Argumentative Explanation Framework for Generalized Reason Model with Inconsistent Precedents](https://arxiv.org/abs/2510.19263)
*Wachara Fungwacharakorn,Gauvain Bourgne,Ken Satoh*

Main category: cs.AI

TL;DR: 本文扩展了推导状态论证框架(DSA-framework)，为容纳不一致先例的广义理由模型提供论证性解释方法。


<details>
  <summary>Details</summary>
Motivation: 传统先例约束假设先例集必须一致，而广义理由模型放松了这一假设。虽然已有基于传统一致理由模型的论证解释方法，但尚未有针对这种广义推理框架的对应方法。

Method: 扩展推导状态论证框架(DSA-framework)，以解释基于广义理由模型的推理过程。

Result: 开发了一种能够处理不一致先例的论证性解释框架。

Conclusion: 成功将DSA框架扩展到广义理由模型，为处理不一致先例的推理提供了有效的论证解释方法。

Abstract: Precedential constraint is one foundation of case-based reasoning in AI and
Law. It generally assumes that the underlying set of precedents must be
consistent. To relax this assumption, a generalized notion of the reason model
has been introduced. While several argumentative explanation approaches exist
for reasoning with precedents based on the traditional consistent reason model,
there has been no corresponding argumentative explanation method developed for
this generalized reasoning framework accommodating inconsistent precedents. To
address this question, this paper examines an extension of the derivation state
argumentation framework (DSA-framework) to explain the reasoning according to
the generalized notion of the reason model.

</details>


### [10] [Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties](https://arxiv.org/abs/2510.19299)
*Philipp J. Schneider,Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.AI

TL;DR: 该论文提出了一个多智能体LLM模拟框架，通过行为奖励函数和情境学习来研究LLM智能体是否能重现人类在线社交动态，包括同质性、互惠性和社会验证等特征。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型智能体是否能重现人类在线行为的复杂社交动态，以及什么记忆和学习机制能使这种动态出现。

Method: 设计了一个多智能体LLM模拟框架，智能体通过情境学习相互交互、评估和适应行为，使用捕捉在线参与核心驱动因素的行为奖励函数，包括社交互动、信息寻求、自我呈现、协调和情感支持。

Result: 实验表明，经过指导的LLM智能体发展出稳定的互动模式并形成涌现的社交联系，产生的网络结构反映了真实在线社区的特性。

Conclusion: 该框架为研究LLM群体中的集体动态建立了一个原则性测试平台，揭示了人工智能体如何近似或偏离类人社交行为。

Abstract: Can large language model (LLM) agents reproduce the complex social dynamics
that characterize human online behavior -- shaped by homophily, reciprocity,
and social validation -- and what memory and learning mechanisms enable such
dynamics to emerge? We present a multi-agent LLM simulation framework in which
agents repeatedly interact, evaluate one another, and adapt their behavior
through in-context learning accelerated by a coaching signal. To model human
social behavior, we design behavioral reward functions that capture core
drivers of online engagement, including social interaction, information
seeking, self-presentation, coordination, and emotional support. These rewards
align agent objectives with empirically observed user motivations, enabling the
study of how network structures and group formations emerge from individual
decision-making. Our experiments show that coached LLM agents develop stable
interaction patterns and form emergent social ties, yielding network structures
that mirror properties of real online communities. By combining behavioral
rewards with in-context adaptation, our framework establishes a principled
testbed for investigating collective dynamics in LLM populations and reveals
how artificial agents may approximate or diverge from human-like social
behavior.

</details>


### [11] [Continual Knowledge Adaptation for Reinforcement Learning](https://arxiv.org/abs/2510.19314)
*Jinwu Hu,Zihao Lian,Zhiquan Wen,Chenghao Li,Guohao Chen,Xutao Wen,Bin Xiao,Mingkui Tan*

Main category: cs.AI

TL;DR: 提出了CKA-RL方法，通过持续知识适应策略解决强化学习中的灾难性遗忘问题，使用任务特定知识向量池和自适应知识合并机制来提升知识利用效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境通常是非平稳的，需要智能体持续适应新任务和变化条件。现有的持续强化学习方法存在灾难性遗忘和知识利用效率低的问题。

Method: 提出持续知识适应策略，维护任务特定知识向量池，动态使用历史知识适应新任务；引入自适应知识合并机制，合并相似知识向量以减少内存需求。

Result: 在三个基准测试上优于现有最优方法，整体性能提升4.20%，前向迁移提升8.02%。

Conclusion: CKA-RL方法能有效缓解灾难性遗忘，实现跨任务的高效知识迁移，在持续强化学习任务中表现出色。

Abstract: Reinforcement Learning enables agents to learn optimal behaviors through
interactions with environments. However, real-world environments are typically
non-stationary, requiring agents to continuously adapt to new tasks and
changing conditions. Although Continual Reinforcement Learning facilitates
learning across multiple tasks, existing methods often suffer from catastrophic
forgetting and inefficient knowledge utilization. To address these challenges,
we propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL),
which enables the accumulation and effective utilization of historical
knowledge. Specifically, we introduce a Continual Knowledge Adaptation
strategy, which involves maintaining a task-specific knowledge vector pool and
dynamically using historical knowledge to adapt the agent to new tasks. This
process mitigates catastrophic forgetting and enables efficient knowledge
transfer across tasks by preserving and adapting critical model parameters.
Additionally, we propose an Adaptive Knowledge Merging mechanism that combines
similar knowledge vectors to address scalability challenges, reducing memory
requirements while ensuring the retention of essential knowledge. Experiments
on three benchmarks demonstrate that the proposed CKA-RL outperforms
state-of-the-art methods, achieving an improvement of 4.20% in overall
performance and 8.02% in forward transfer. The source code is available at
https://github.com/Fhujinwu/CKA-RL.

</details>


### [12] [MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration](https://arxiv.org/abs/2510.19423)
*Jia-Kai Dong,I-Wei Huang,Chun-Tin Wu,Yi-Tien Tsai*

Main category: cs.AI

TL;DR: MSC-Bench是一个用于评估LLM代理在多跳、端到端工具编排能力的大规模基准测试，通过构建"等函数集"来建立真实基准，减少对LLM作为评判的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试往往孤立评估工具，忽略了功能重叠和跨服务器编排等挑战，导致评估过于乐观。

Method: 采用五级课程设计，从单工具编排到复杂跨服务器规划，系统测试代理能力，并通过等函数集构建真实基准。

Result: 实验表明，僵化的层次结构会阻碍性能，即使最先进的代理在鲁棒性方面也存在系统性弱点。

Conclusion: MSC-Bench提供了一个诊断框架，揭示了现有方法的局限性，并为开发更强大、高效的工具使用代理提供指导。

Abstract: We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop,
end-to-end tool orchestration by LLM agents in a hierarchical Model-Context
Protocol (MCP) ecosystem. Existing benchmarks often evaluate tools in
isolation, ignoring challenges such as functional overlap and cross-server
orchestration, leading to overly optimistic assessments. MSC-Bench addresses
these gaps by constructing ground truth through 'equal function sets', allowing
objective metrics such as F1 score and reducing the dependency on
LLM-as-a-judge evaluation. Organized as a five-level curriculum, it
systematically tests agent capabilities from single-tool orchestration to
complex cross-server planning, and robustness to out-of-scope requests.
Experiments reveal that rigid hierarchies can hinder performance without
co-designed strategies, and even state-of-the-art agents exhibit systemic
weaknesses in robustness. MSC-Bench provides a diagnostic framework to expose
these limitations and guide the development of more capable and efficient
tool-using agents. The benchmark and resources are publicly available at
https://github.com/snooow1029/MSC_Bench.

</details>


### [13] [NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning](https://arxiv.org/abs/2510.19429)
*Wonje Choi,Jooyoung Kim,Honguk Woo*

Main category: cs.AI

TL;DR: NeSyPr是一个神经符号程序化框架，通过将符号规划转化为可组合的程序表示，使语言模型能够在动态环境中进行高效推理，无需依赖外部符号规划器。


<details>
  <summary>Details</summary>
Motivation: 解决在动态环境中使用语言模型进行具身任务时面临的延迟、连接性和资源限制问题，特别是在需要在线访问大规模推理引擎或符号规划器的场景下。

Method: 首先使用符号工具生成任务特定计划，然后将这些计划转化为编码隐式产生规则的可组合程序表示，最后将这些程序无缝集成到语言模型的推理过程中。

Result: 在PDDLGym、VirtualHome和ALFWorld等具身基准测试中，NeSyPr展示了比大规模推理模型和符号规划器更高效的推理能力，同时使用更紧凑的语言模型。

Conclusion: 神经符号程序化能够将多步符号结构化路径查找和推理抽象为单步语言模型推理，适合部署在延迟敏感和资源受限的物理系统中。

Abstract: We address the challenge of adopting language models (LMs) for embodied tasks
in dynamic environments, where online access to large-scale inference engines
or symbolic planners is constrained due to latency, connectivity, and resource
limitations. To this end, we present NeSyPr, a novel embodied reasoning
framework that compiles knowledge via neurosymbolic proceduralization, thereby
equipping LM-based agents with structured, adaptive, and timely reasoning
capabilities. In NeSyPr, task-specific plans are first explicitly generated by
a symbolic tool leveraging its declarative knowledge. These plans are then
transformed into composable procedural representations that encode the plans'
implicit production rules, enabling the resulting composed procedures to be
seamlessly integrated into the LM's inference process. This neurosymbolic
proceduralization abstracts and generalizes multi-step symbolic structured
path-finding and reasoning into single-step LM inference, akin to human
knowledge compilation. It supports efficient test-time inference without
relying on external symbolic guidance, making it well suited for deployment in
latency-sensitive and resource-constrained physical systems. We evaluate NeSyPr
on the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating
its efficient reasoning capabilities over large-scale reasoning models and a
symbolic planner, while using more compact LMs.

</details>


### [14] [DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning](https://arxiv.org/abs/2510.19562)
*Runpeng Xie,Quanwei Wang,Hao Hu,Zherui Zhou,Ni Mu,Xiyun Li,Yiqin Yang,Shuang Xu,Qianchuan Zhao,Bo XU*

Main category: cs.AI

TL;DR: 提出DAIL方法解决语言指令的模糊性问题，通过分布策略和语义对齐提升智能体对自然语言指令的理解和执行能力


<details>
  <summary>Details</summary>
Motivation: 自然语言指令的灵活性导致任务模糊性，严重影响算法性能，需要解决语言条件任务中的歧义问题

Method: DAIL方法包含两个核心组件：分布策略（通过值分布估计机制增强任务可区分性）和语义对齐模块（捕捉轨迹与语言指令的对应关系）

Result: 在结构化和视觉观察基准测试中，DAIL有效解决了指令模糊性问题，性能优于基线方法

Conclusion: DAIL方法通过分布对齐学习成功解决了语言指令的模糊性挑战，为智能体理解人类指令提供了有效解决方案

Abstract: Comprehending natural language and following human instructions are critical
capabilities for intelligent agents. However, the flexibility of linguistic
instructions induces substantial ambiguity across language-conditioned tasks,
severely degrading algorithmic performance. To address these limitations, we
present a novel method named DAIL (Distributional Aligned Learning), featuring
two key components: distributional policy and semantic alignment. Specifically,
we provide theoretical results that the value distribution estimation mechanism
enhances task differentiability. Meanwhile, the semantic alignment module
captures the correspondence between trajectories and linguistic instructions.
Extensive experimental results on both structured and visual observation
benchmarks demonstrate that DAIL effectively resolves instruction ambiguities,
achieving superior performance to baseline methods. Our implementation is
available at https://github.com/RunpengXie/Distributional-Aligned-Learning.

</details>


### [15] [HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application](https://arxiv.org/abs/2510.19631)
*Yiqian Yang,Tian Lan,Qianghuai Jia,Li Zhu,Hui Jiang,Hang Zhu,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.AI

TL;DR: HSCodeComp是首个评估深度搜索代理在分层规则应用能力的电子商务基准，要求代理根据产品描述预测10位HS编码，现有最佳代理准确率仅46.8%，远低于人类专家的95.0%。


<details>
  <summary>Details</summary>
Motivation: 当前代理基准忽视了深度搜索代理应用复杂规则（如法律条款、医疗手册、关税规则）的关键能力，这些规则具有模糊边界和隐式逻辑关系，对代理构成挑战。

Method: 从大型电商平台收集真实数据构建HSCodeComp基准，包含632个产品条目，涵盖多样化产品类别，HS编码由多位人类专家标注。

Result: 在多个最先进的LLM、开源和闭源代理上的实验结果显示巨大性能差距：最佳代理仅达到46.8%的10位准确率，远低于人类专家的95.0%。测试时扩展无法进一步提升性能。

Conclusion: HSCodeComp揭示了深度搜索代理在分层规则应用方面的显著挑战，现有代理能力严重不足，需要进一步研究提升代理处理复杂规则的能力。

Abstract: Effective deep search agents must not only access open-domain and
domain-specific knowledge but also apply complex rules-such as legal clauses,
medical manuals and tariff rules. These rules often feature vague boundaries
and implicit logic relationships, making precise application challenging for
agents. However, this critical capability is largely overlooked by current
agent benchmarks.
  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level
e-commerce benchmark designed to evaluate deep search agents in hierarchical
rule application. In this task, the deep reasoning process of agents is guided
by these rules to predict 10-digit Harmonized System Code (HSCode) of products
with noisy but realistic descriptions. These codes, established by the World
Customs Organization, are vital for global supply chain efficiency. Built from
real-world data collected from large-scale e-commerce platforms, our proposed
HSCodeComp comprises 632 product entries spanning diverse product categories,
with these HSCodes annotated by several human experts.
  Extensive experimental results on several state-of-the-art LLMs, open-source,
and closed-source agents reveal a huge performance gap: best agent achieves
only 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,
detailed analysis demonstrates the challenges of hierarchical rule application,
and test-time scaling fails to improve performance further.

</details>


### [16] [AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing](https://arxiv.org/abs/2510.19661)
*Xusen Guo,Mingxing Peng,Xixuan Hao,Xingchen Zou,Qiongyan Wang,Sijie Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: AgentSense是一个基于多智能体进化系统的混合训练免费框架，将大语言模型集成到参与式城市感知中，通过迭代优化任务分配来适应动态城市条件和异构工作者偏好，同时提供自然语言解释以增强透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的城市感知系统在跨不同城市场景的泛化能力和决策可解释性方面存在局限，需要开发能够适应动态城市条件并提供透明解释的系统。

Method: 结合大语言模型和多智能体进化系统，首先使用经典规划器生成基线解决方案，然后迭代优化以适应动态城市条件和异构工作者偏好，同时产生自然语言解释。

Result: 在两个大规模移动数据集和七种动态干扰类型上的实验表明，AgentSense在适应性和可解释性方面优于传统方法，相比单智能体LLM基线在性能和鲁棒性方面表现更优，并提供更合理透明的解释。

Conclusion: AgentSense代表了向部署自适应和可解释的城市感知系统的重要进展，为基于网络的参与式城市感知提供了有效的解决方案。

Abstract: Web-based participatory urban sensing has emerged as a vital approach for
modern urban management by leveraging mobile individuals as distributed
sensors. However, existing urban sensing systems struggle with limited
generalization across diverse urban scenarios and poor interpretability in
decision-making. In this work, we introduce AgentSense, a hybrid, training-free
framework that integrates large language models (LLMs) into participatory urban
sensing through a multi-agent evolution system. AgentSense initially employs
classical planner to generate baseline solutions and then iteratively refines
them to adapt sensing task assignments to dynamic urban conditions and
heterogeneous worker preferences, while producing natural language explanations
that enhance transparency and trust. Extensive experiments across two
large-scale mobility datasets and seven types of dynamic disturbances
demonstrate that AgentSense offers distinct advantages in adaptivity and
explainability over traditional methods. Furthermore, compared to single-agent
LLM baselines, our approach outperforms in both performance and robustness,
while delivering more reasonable and transparent explanations. These results
position AgentSense as a significant advancement towards deploying adaptive and
explainable urban sensing systems on the web.

</details>


### [17] [A Graph Engine for Guitar Chord-Tone Soloing Education](https://arxiv.org/abs/2510.19666)
*Matthew Keating,Michael Casey*

Main category: cs.AI

TL;DR: 开发了一个基于图论的引擎，为吉他学生提供和弦音独奏建议，通过构建和弦音琶音图并计算最优路径来生成独奏线条。


<details>
  <summary>Details</summary>
Motivation: 和弦音独奏是爵士吉他即兴演奏的基础练习，但学习和练习难度较大，需要为吉他学生提供有效的练习工具。

Method: 首先生成和弦音琶音，构建加权图（节点代表和弦琶音），计算相邻和弦节点间的最优过渡音权重，然后寻找最短路径并重构独奏线条。

Result: 开发了一个用户友好的系统，能够处理输入输出，为吉他学生提供可练习的和弦音独奏建议。

Conclusion: 该图论引擎为吉他学生提供了有效的和弦音独奏练习工具，有助于掌握这一重要的爵士吉他基础技能。

Abstract: We present a graph-based engine for computing chord tone soloing suggestions
for guitar students. Chord tone soloing is a fundamental practice for
improvising over a chord progression, where the instrumentalist uses only the
notes contained in the current chord. This practice is a building block for all
advanced jazz guitar theory but is difficult to learn and practice. First, we
discuss methods for generating chord-tone arpeggios. Next, we construct a
weighted graph where each node represents a chord tone arpeggio for a chord in
the progression. Then, we calculate the edge weight between each consecutive
chord's nodes in terms of optimal transition tones. We then find the shortest
path through this graph and reconstruct a chord-tone soloing line. Finally, we
discuss a user-friendly system to handle input and output to this engine for
guitar students to practice chord tone soloing.

</details>


### [18] [Explainable e-sports win prediction through Machine Learning classification in streaming](https://arxiv.org/abs/2510.19671)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.AI

TL;DR: 提出了一种可解释的电子竞技流式赢预测分类解决方案，通过滑动窗口控制输入数据来反映游戏变化，准确率超过90%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电子竞技观众和玩家数量增长，加上通信技术和云计算发展，推动了在线游戏行业的发展。虽然基于AI的电子竞技分析传统上是从相关数据中提取有意义模式并进行可视化以增强决策，但专业赢预测大多关注批量分类，忽略了可视化技术。

Method: 开发了一种可解释的赢预测分类解决方案，在流式处理中通过多个滑动窗口控制输入数据，以反映相关游戏变化。

Result: 实验结果显示准确率超过90%，超越了文献中的竞争解决方案。

Conclusion: 该系统可被排名和推荐系统利用，用于知情决策，其可解释性模块增强了对预测结果的信任。

Abstract: The increasing number of spectators and players in e-sports, along with the
development of optimized communication solutions and cloud computing
technology, has motivated the constant growth of the online game industry. Even
though Artificial Intelligence-based solutions for e-sports analytics are
traditionally defined as extracting meaningful patterns from related data and
visualizing them to enhance decision-making, most of the effort in professional
winning prediction has been focused on the classification aspect from a batch
perspective, also leaving aside the visualization techniques. Consequently,
this work contributes to an explainable win prediction classification solution
in streaming in which input data is controlled over several sliding windows to
reflect relevant game changes. Experimental results attained an accuracy higher
than 90 %, surpassing the performance of competing solutions in the literature.
Ultimately, our system can be leveraged by ranking and recommender systems for
informed decision-making, thanks to the explainability module, which fosters
trust in the outcome predictions.

</details>


### [19] [RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models](https://arxiv.org/abs/2510.19698)
*Yang Yang,Hua XU,Zhangyi Hu,Yutao Yue*

Main category: cs.AI

TL;DR: RLIE是一个将大语言模型与概率建模相结合的统一框架，用于学习加权规则集，通过规则生成、逻辑回归、迭代优化和评估四个阶段实现更可靠的神经符号推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法忽视了规则间的相互作用，且未充分利用LLM与概率规则学习结合的潜力，需要开发能够进行稳健推理的统一框架。

Method: RLIE包含四个阶段：1) LLM生成和筛选候选规则；2) 逻辑回归学习概率权重；3) 基于预测误差迭代优化规则集；4) 评估加权规则集作为分类器的性能。

Result: 直接使用学习到的权重应用规则可获得优越性能，而将规则、权重和逻辑模型输出注入LLM反而会降低准确性，表明LLM擅长语义生成但概率整合能力有限。

Conclusion: RLIE阐明了LLM在归纳推理中的潜力和局限性，通过将其与经典概率规则组合方法结合，实现了更可靠的神经符号推理。

Abstract: Large Language Models (LLMs) can propose rules in natural language,
sidestepping the need for a predefined predicate space in traditional rule
learning. Yet many LLM-based approaches ignore interactions among rules, and
the opportunity to couple LLMs with probabilistic rule learning for robust
inference remains underexplored. We present RLIE, a unified framework that
integrates LLMs with probabilistic modeling to learn a set of weighted rules.
RLIE has four stages: (1) Rule generation, where an LLM proposes and filters
candidates; (2) Logistic regression, which learns probabilistic weights for
global selection and calibration; (3) Iterative refinement, which updates the
rule set using prediction errors; and (4) Evaluation, which compares the
weighted rule set as a direct classifier with methods that inject rules into an
LLM. We evaluate multiple inference strategies on real-world datasets. Applying
rules directly with their learned weights yields superior performance, whereas
prompting LLMs with the rules, weights, and logistic-model outputs surprisingly
degrades accuracy. This supports the view that LLMs excel at semantic
generation and interpretation but are less reliable for precise probabilistic
integration. RLIE clarifies the potential and limitations of LLMs for inductive
reasoning and couples them with classic probabilistic rule combination methods
to enable more reliable neuro-symbolic reasoning.

</details>


### [20] [Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning](https://arxiv.org/abs/2510.19732)
*Gunshi Gupta,Karmesh Yadav,Zsolt Kira,Yarin Gal,Rahaf Aljundi*

Main category: cs.AI

TL;DR: Memo是一个基于transformer的架构和训练方法，用于解决强化学习中记忆密集型、长视野任务的问题。它通过在训练时插入周期性总结标记来创建和检索记忆，在网格世界元RL基准和真实室内多目标导航任务中表现优于传统长上下文transformer基线，同时计算和存储效率更高。


<details>
  <summary>Details</summary>
Motivation: 当前基于transformer的具身智能体策略训练中，视觉输入常常超出transformer的上下文限制，而人类能够将终身经验压缩为记忆进行利用。现有方法要么依赖固定大小的循环记忆，要么完全依赖完整上下文，缺乏有效的记忆压缩机制。

Method: 提出Memo架构，在模型输入中交错插入周期性总结标记来创建和检索记忆。这种方法允许模型在训练过程中形成压缩的记忆表示，而不是依赖完整的上下文历史。

Result: 在网格世界元RL基准和真实室内多目标导航任务中，Memo优于传统长上下文transformer基线，同时计算和存储效率更高。Memo在推理时对更长上下文的泛化能力更好，在流式设置中（历史上下文必须截断以适应推理约束）保持鲁棒性。

Conclusion: Memo通过引入记忆创建和检索机制，为处理长视野、记忆密集型的具身决策任务提供了一种有效的解决方案，在保持性能的同时显著提高了计算和存储效率。

Abstract: To enable embodied agents to operate effectively over extended timeframes, it
is crucial to develop models that form and access memories to stay
contextualized in their environment. In the current paradigm of training
transformer-based policies for embodied sequential decision-making tasks,
visual inputs often overwhelm the context limits of transformers, while humans
can maintain and utilize a lifetime of experience compressed as memories.
Significant compression is possible in principle, as much of the input is
irrelevant and can be abstracted. However, existing approaches predominantly
focus on either recurrent models with fixed-size memory or transformers with
full-context reliance. In this work, we propose Memo, a transformer-based
architecture and training recipe for reinforcement learning (RL) on
memory-intensive, long-horizon tasks. Memo incorporates the creation and
retrieval of memory by interleaving periodic summarization tokens with the
inputs of a model during training. We demonstrate Memo's effectiveness on a
gridworld meta-RL benchmark and a multi-object navigation task in
photo-realistic indoor settings. Memo outperforms naive long-context
transformer baselines while being more compute and storage efficient.
Additionally, Memo generalizes better to longer contexts at inference time and
remains robust in streaming settings, where historical context must be
truncated to fit inference constraints.

</details>


### [21] [Misalignment Bounty: Crowdsourcing AI Agent Misbehavior](https://arxiv.org/abs/2510.19738)
*Rustem Turtayev,Natalia Fedorova,Oleg Serikov,Sergey Koldyba,Lev Avagyan,Dmitrii Volkov*

Main category: cs.AI

TL;DR: 该论文介绍了"错位赏金"项目，这是一个众包项目，旨在收集AI系统与人类意图不符的行为案例。项目收到295份提交，其中9份获奖。


<details>
  <summary>Details</summary>
Motivation: 收集清晰、可复现的AI系统与人类意图不符的案例，以了解AI系统如何追求非预期或不安全的目标。

Method: 通过众包项目"错位赏金"收集案例，使用特定评估标准对提交进行评审，最终选出9个获奖案例。

Result: 项目成功收集了295份提交，其中9份被认定为有价值的案例，展示了AI系统与人类意图不符的具体行为。

Conclusion: 该项目提供了实际案例来理解AI系统的错位行为，有助于改进AI安全性和对齐性。

Abstract: Advanced AI systems sometimes act in ways that differ from human intent. To
gather clear, reproducible examples, we ran the Misalignment Bounty: a
crowdsourced project that collected cases of agents pursuing unintended or
unsafe goals. The bounty received 295 submissions, of which nine were awarded.
  This report explains the program's motivation and evaluation criteria, and
walks through the nine winning submissions step by step.

</details>


### [22] [Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents](https://arxiv.org/abs/2510.19771)
*Gil Pasternak,Dheeraj Rajagopal,Julia White,Dhruv Atreja,Matthew Thomas,George Hurn-Maloney,Ash Lewis*

Main category: cs.AI

TL;DR: 提出了PROBE基准来评估LLM智能体的主动性，将主动性分解为三个核心能力：搜索未指定问题、识别具体瓶颈和执行适当解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前评估主动性的基准局限于局部上下文，无法测试跨来源和长时间跨度的推理能力，需要更全面的评估框架。

Method: 开发PROBE基准，将主动性分解为三个能力管道，并应用于评估领先的LLM和流行智能体框架。

Result: 即使是先进模型也难以解决该基准，GPT-5和Claude Opus-4.1的最佳端到端性能仅为40%，揭示了智能体系统的当前局限性。

Conclusion: 研究结果突显了智能体系统中自主行动的当前限制，并揭示了有前景的未来研究方向。

Abstract: LLM-based agents are increasingly moving towards proactivity: rather than
awaiting instruction, they exercise agency to anticipate user needs and solve
them autonomously. However, evaluating proactivity is challenging; current
benchmarks are constrained to localized context, limiting their ability to test
reasoning across sources and longer time horizons. To address this gap, we
present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes
proactivity as a pipeline of three core capabilities: (1) searching for
unspecified issues, (2) identifying specific bottlenecks, and (3) executing
appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular
agentic frameworks, showing that even state-of-the-art models struggle to solve
this benchmark. Computing our consistent measurements across frontier LLMs and
agents, we find that the best end-to-end performance of 40% is achieved by both
GPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative
capabilities of each model and analyze mutual failure modes. Our results
highlight the current limitations of autonomous action in agentic systems, and
expose promising future research directions.

</details>


### [23] [Benchmarking World-Model Learning](https://arxiv.org/abs/2510.19788)
*Archana Warrier,Dat Nyugen,Michelangelo Naim,Moksh Jain,Yichao Liang,Karen Schroeder,Cambridge Yang,Joshua B. Tenenbaum,Sebastian Vollmer,Kevin Ellis,Zenna Tavares*

Main category: cs.AI

TL;DR: WorldTest是一个评估模型学习智能体的新协议，将无奖励交互与在不同但相关环境中的评分测试阶段分离。该协议通过AutumnBench套件实例化，包含43个交互式网格世界环境和129个任务，比较了517名人类参与者和三个前沿模型的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的世界模型学习和评估方法偏离了实际目标：训练和评估都锚定在下一帧预测上，成功标准是在同一环境中最大化奖励。需要一种能够评估模型学习智能体对环境动态理解能力的协议。

Method: 提出WorldTest协议，采用无奖励探索、衍生测试和行为评分三个步骤。通过AutumnBench套件实例化，包含43个网格世界环境和129个任务，涵盖三个任务族：掩码帧预测、规划和因果动态变化预测。

Result: 人类参与者表现优于模型，计算规模扩展只在某些环境中提高性能，而在其他环境中没有效果。AutumnBench揭示了世界模型学习方面存在显著改进空间。

Conclusion: WorldTest提供了一个新颖的模板来评估智能体对环境动态的学习情况，AutumnBench暴露了世界模型学习中的重大改进空间，为未来研究提供了基准。

Abstract: Model-learning agents should gather information to learn world models that
support many downstream tasks and inferences, such as predicting unobserved
states, estimating near- and far-term consequences of actions, planning action
sequences, and detecting changes in dynamics. Current methods for learning and
evaluating world models diverge from this goal: training and evaluation are
anchored to next-frame prediction, and success is scored by reward maximization
in the same environment. We propose WorldTest, a protocol to evaluate
model-learning agents that separates reward-free interaction from a scored test
phase in a different but related environment. WorldTest is
open-ended$\unicode{x2014}$models should support many different tasks unknown
ahead of time$\unicode{x2014}$and agnostic to model representation, allowing
comparison across approaches. We instantiated WorldTest with AutumnBench, a
suite of 43 interactive grid-world environments and 129 tasks across three
families: masked-frame prediction, planning, and predicting changes to the
causal dynamics. We compared 517 human participants and three frontier models
on AutumnBench. We found that humans outperform the models, and scaling compute
improves performance only in some environments but not others. WorldTest
provides a novel template$\unicode{x2014}$reward-free exploration, derived
tests, and behavior-based scoring$\unicode{x2014}$to evaluate what agents learn
about environment dynamics, and AutumnBench exposes significant headroom in
world-model learning.

</details>
