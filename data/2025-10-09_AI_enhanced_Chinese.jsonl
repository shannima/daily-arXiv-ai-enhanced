{"id": "2510.06261", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06261", "abs": "https://arxiv.org/abs/2510.06261", "authors": ["Zhanke Zhou", "Chentao Cao", "Xiao Feng", "Xuan Li", "Zongze Li", "Xiangyu Lu", "Jiangchao Yao", "Weikai Huang", "Linrui Xu", "Tian Cheng", "Guanyu Jiang", "Yiming Zheng", "Brando Miranda", "Tongliang Liu", "Sanmi Koyejo", "Masashi Sugiyama", "Bo Han"], "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning", "comment": "Ongoing project", "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to\naddress two bottlenecks in foundation model (FM) reasoning-limited\nmodel-intrinsic capacity and unreliable test-time iteration. AlphaApollo\norchestrates multiple models with professional tools to enable deliberate,\nverifiable reasoning. It couples (i) a computation tool (Python with numerical\nand symbolic libraries) and (ii) a retrieval tool (task-relevant external\ninformation) to execute exact calculations and ground decisions. The system\nfurther supports multi-round, multi-model solution evolution via a shared state\nmap that records candidates, executable checks, and feedback for iterative\nrefinement. In evaluations on AIME 2024/2025 across multiple models,\nAlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32\nfor Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for\nLlama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool\ncalls are successfully executed, with consistent outperformance of non-tool\nbaselines, thereby lifting the capability ceiling of FMs. More empirical\nresults and implementation details will be updated at\nhttps://github.com/tmlr-group/AlphaApollo.", "AI": {"tldr": "AlphaApollo\u662f\u4e00\u4e2a\u81ea\u6f14\u5316\u7684\u667a\u80fd\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u4e2a\u6a21\u578b\u548c\u4e13\u4e1a\u5de5\u5177\u6765\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u63a8\u7406\u80fd\u529b\u6709\u9650\u548c\u6d4b\u8bd5\u65f6\u8fed\u4ee3\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u5728AIME 2024/2025\u8bc4\u4f30\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u63a8\u7406\u80fd\u529b\u6709\u9650\u548c\u6d4b\u8bd5\u65f6\u8fed\u4ee3\u4e0d\u53ef\u9760\u4e24\u4e2a\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u534f\u8c03\u591a\u4e2a\u6a21\u578b\u4e0e\u4e13\u4e1a\u5de5\u5177\uff08\u8ba1\u7b97\u5de5\u5177\u548c\u68c0\u7d22\u5de5\u5177\uff09\u8fdb\u884c\u53ef\u9a8c\u8bc1\u63a8\u7406\uff0c\u4f7f\u7528\u5171\u4eab\u72b6\u6001\u5730\u56fe\u652f\u6301\u591a\u8f6e\u591a\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u6f14\u5316\uff0c\u5305\u62ec\u5019\u9009\u65b9\u6848\u3001\u53ef\u6267\u884c\u68c0\u67e5\u548c\u8fed\u4ee3\u53cd\u9988\u3002", "result": "\u5728AIME 2024/2025\u8bc4\u4f30\u4e2d\uff0cQwen2.5-14B-Instruct\u6a21\u578b\u5e73\u5747\u63d0\u53475.15%\uff0c\u901a\u8fc7\u7387\u63d0\u534723.34%\uff1bLlama-3.3-70B-Instruct\u6a21\u578b\u5e73\u5747\u63d0\u53478.91%\uff0c\u901a\u8fc7\u7387\u63d0\u534726.67%\u3002\u8d85\u8fc780%\u7684\u5de5\u5177\u8c03\u7528\u6210\u529f\u6267\u884c\u3002", "conclusion": "AlphaApollo\u7cfb\u7edf\u901a\u8fc7\u5de5\u5177\u4f7f\u7528\u548c\u534f\u4f5c\u63a8\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u4e0a\u9650\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.06274", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06274", "abs": "https://arxiv.org/abs/2510.06274", "authors": ["Mohammad Mahdi Samiei Paqaleh", "Arash Marioriyad", "Arman Tahmasebi-Zadeh", "Mohamadreza Fereydooni", "Mahdi Ghaznavai", "Mahdieh Soleymani Baghshah"], "title": "Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization", "comment": null, "summary": "Recent progress has pushed AI frontiers from pattern recognition tasks toward\nproblems that require step by step, System2 style reasoning, especially with\nlarge language models. Yet, unlike learning, where generalization and out of\ndistribution (OoD) evaluation concepts are well formalized, there is no clear,\nconsistent definition or metric for reasoning ability. We propose Complexity\nOut of Distribution (Complexity OoD) generalization as a framework and problem\nsetting to define and measure reasoning. A model exhibits Complexity OoD\ngeneralization when it maintains performance on test instances whose minimal\nrequired solution complexity, either representational (richer solution\nstructure) or computational (more reasoning steps/program length), exceeds that\nof all training examples. We formalize complexity via solution description\nKolmogorov complexity and operational proxies (e.g., object/relation counts;\nreasoning step counts), clarifying how Complexity OoD differs from length and\ncompositional OoD. This lens unifies learning and reasoning: many cases\nsolvable with System1 like processing at low complexity become System2 like\nunder complexity pressure, while System2 can be viewed as generalization over\nsolution structures. We translate this perspective into practice with\nrecommendations for operationalizing Complexity OoD across the stack:\nincorporating complexity into benchmark and evaluation metric design,\nrethinking supervision to target solution traces, seeking and designing\ninductive biases for Complexity OoD generalization, addressing learning to\nreason spillovers such as spurious shortcuts, semantic robustness, catastrophic\nforgetting, and step wise calibration. Because Complexity OoD cannot be solved\nby scaling data alone, progress toward robust reasoning will require\narchitectures and training regimes that explicitly model and allocate\ncomputation with respect to complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u590d\u6742\u6027\u5206\u5e03\u5916\u6cdb\u5316\u6846\u67b6\u6765\u5b9a\u4e49\u548c\u8861\u91cf\u63a8\u7406\u80fd\u529b\uff0c\u5f3a\u8c03\u5f53\u6d4b\u8bd5\u5b9e\u4f8b\u6240\u9700\u7684\u6700\u5c0f\u89e3\u51b3\u65b9\u6848\u590d\u6742\u6027\u8d85\u8fc7\u8bad\u7ec3\u793a\u4f8b\u65f6\uff0c\u6a21\u578b\u4ecd\u80fd\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5f53\u524dAI\u5728\u6a21\u5f0f\u8bc6\u522b\u4efb\u52a1\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u63a8\u7406\u80fd\u529b\u7684\u660e\u786e\u5b9a\u4e49\u548c\u5ea6\u91cf\u6807\u51c6\uff0c\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u6027\u6846\u67b6\u6765\u8bc4\u4f30\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u89e3\u51b3\u65b9\u6848\u63cf\u8ff0Kolmogorov\u590d\u6742\u6027\u548c\u64cd\u4f5c\u4ee3\u7406\u6765\u5f62\u5f0f\u5316\u590d\u6742\u6027\uff0c\u533a\u5206\u590d\u6742\u6027OoD\u4e0e\u957f\u5ea6\u548c\u7ec4\u5408OoD\uff0c\u5e76\u5c06\u8be5\u89c6\u89d2\u8f6c\u5316\u4e3a\u5b9e\u8df5\u5efa\u8bae\u3002", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u5b66\u4e60\u548c\u63a8\u7406\u7684\u6846\u67b6\uff0c\u8868\u660e\u8bb8\u591a\u5728\u4f4e\u590d\u6742\u6027\u4e0b\u53ef\u7528System1\u5904\u7406\u7684\u60c5\u51b5\u5728\u590d\u6742\u6027\u538b\u529b\u4e0b\u9700\u8981System2\uff0c\u800cSystem2\u53ef\u89c6\u4e3a\u5bf9\u89e3\u51b3\u65b9\u6848\u7ed3\u6784\u7684\u6cdb\u5316\u3002", "conclusion": "\u590d\u6742\u6027OoD\u4e0d\u80fd\u4ec5\u901a\u8fc7\u6269\u5c55\u6570\u636e\u89e3\u51b3\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u6027\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u673a\u5236\u6765\u63a8\u8fdb\u7a33\u5065\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.06288", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06288", "abs": "https://arxiv.org/abs/2510.06288", "authors": ["Raj Ghugare", "Catherine Ji", "Kathryn Wantlin", "Jin Schofield", "Benjamin Eysenbach"], "title": "BuilderBench -- A benchmark for generalist agents", "comment": "Project page: https://rajghugare19.github.io/builderbench and Code:\n  https://github.com/rajghugare19/builderbench", "summary": "Today's AI models learn primarily through mimicry and sharpening, so it is\nnot surprising that they struggle to solve problems beyond the limits set by\nexisting data. To solve novel problems, agents should acquire skills for\nexploring and learning through experience. Finding a scalable learning\nmechanism for developing agents that learn through interaction remains a major\nopen problem. In this work, we introduce BuilderBench, a benchmark to\naccelerate research into agent pre-training that centers open-ended\nexploration. BuilderBench requires agents to learn how to build any structure\nusing blocks. BuilderBench is equipped with $(1)$ a hardware accelerated\nsimulator of a robotic agent interacting with various physical blocks, and\n$(2)$ a task-suite with over 42 diverse target structures that are carefully\ncurated to test an understanding of physics, mathematics, and long-horizon\nplanning. During training, agents have to explore and learn general principles\nabout the environment without any external supervision. During evaluation,\nagents have to build the unseen target structures from the task suite. Solving\nthese tasks requires a sort of \\emph{embodied reasoning} that is not reflected\nin words but rather in actions, experimenting with different strategies and\npiecing them together. Our experiments show that many of these tasks challenge\nthe current iteration of algorithms. Hence, we also provide a ``training\nwheels'' protocol, in which agents are trained and evaluated to build a single\ntarget structure from the task suite. Finally, we provide single-file\nimplementations of six different algorithms as a reference point for\nresearchers.", "AI": {"tldr": "\u63d0\u51fa\u4e86BuilderBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u901a\u8fc7\u5f00\u653e\u5f0f\u63a2\u7d22\u5b66\u4e60\u6784\u5efa\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u5305\u542b42\u4e2a\u591a\u6837\u5316\u76ee\u6807\u7ed3\u6784\uff0c\u6d4b\u8bd5\u7269\u7406\u7406\u89e3\u3001\u6570\u5b66\u548c\u957f\u65f6\u7a0b\u89c4\u5212\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u4e3b\u8981\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\uff0c\u96be\u4ee5\u89e3\u51b3\u8d85\u51fa\u6570\u636e\u8303\u56f4\u7684\u65b0\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u901a\u8fc7\u4ea4\u4e92\u7ecf\u9a8c\u5b66\u4e60\u7684\u667a\u80fd\u4f53\uff0c\u4f46\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u673a\u5236\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "BuilderBench\u5305\u542b\u786c\u4ef6\u52a0\u901f\u7684\u673a\u5668\u4eba\u6a21\u62df\u5668\u548c42\u4e2a\u76ee\u6807\u7ed3\u6784\u4efb\u52a1\u3002\u667a\u80fd\u4f53\u5728\u8bad\u7ec3\u65f6\u65e0\u76d1\u7763\u63a2\u7d22\u73af\u5883\uff0c\u5728\u8bc4\u4f30\u65f6\u6784\u5efa\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u7b97\u6cd5\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u63d0\u4f9b\u4e86\"\u8bad\u7ec3\u8f6e\"\u534f\u8bae\u548c6\u79cd\u7b97\u6cd5\u7684\u5355\u6587\u4ef6\u5b9e\u73b0\u4f5c\u4e3a\u53c2\u8003\u3002", "conclusion": "BuilderBench\u65e8\u5728\u52a0\u901f\u57fa\u4e8e\u5f00\u653e\u5f0f\u63a2\u7d22\u7684\u667a\u80fd\u4f53\u9884\u8bad\u7ec3\u7814\u7a76\uff0c\u9700\u8981\u4e00\u79cd\u4f53\u73b0\u5728\u884c\u52a8\u4e2d\u7684\u5177\u8eab\u63a8\u7406\u80fd\u529b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8bed\u8a00\u8868\u8fbe\u3002"}}
{"id": "2510.06302", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06302", "abs": "https://arxiv.org/abs/2510.06302", "authors": ["Ksenija Lace", "Marite Kirikova"], "title": "Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration", "comment": null, "summary": "Post-merger integration states unique challenges for professionals\nresponsible for information system integration aimed on alignment and\ncombination diverse system architectures of merging organizations. Although the\ntheoretical and practical guidance exists for post-merger integration on the\nbusiness level, there is a significant gap in training for information system\nintegration in this context. In prior research specific methods AMILI (Support\nmethod for informed decision identification) and AMILP (Support method for\ninformed decision-making) were introduced for the support of information system\nintegration decisions in the post-merger integration. But during the practical\napplication was reported high learning curve and low learner motivation. This\npaper explores how game-based learning design can address these limitations by\ntransforming static method training into engaging learning experience. The\nstudy analyzes foundational learning theories, cognitive load and motivation\nmodels, and serious game design frameworks to identify the essential\nrequirements for a game-based learning design framework tailored to information\nsystem integration in post-merger integration. Requirements are structured in\ntwo components: the transformation process and resulting learning experience.\nThe paper concludes with a plan for developing and evaluating the proposed\nframework through iterative design and real-world validation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u6e38\u620f\u5316\u5b66\u4e60\u8bbe\u8ba1\u6539\u8fdb\u5e76\u8d2d\u540e\u4fe1\u606f\u7cfb\u7edf\u96c6\u6210\u7684\u65b9\u6cd5\u57f9\u8bad\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5b66\u4e60\u66f2\u7ebf\u9ad8\u548c\u52a8\u673a\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u5e76\u8d2d\u540e\u4fe1\u606f\u7cfb\u7edf\u96c6\u6210\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u73b0\u6709AMILI\u548cAMILP\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5b66\u4e60\u66f2\u7ebf\u9ad8\u548c\u5b66\u4e60\u8005\u52a8\u673a\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u57f9\u8bad\u65b9\u5f0f\u3002", "method": "\u5206\u6790\u57fa\u7840\u5b66\u4e60\u7406\u8bba\u3001\u8ba4\u77e5\u8d1f\u8377\u4e0e\u52a8\u673a\u6a21\u578b\u3001\u4e25\u8083\u6e38\u620f\u8bbe\u8ba1\u6846\u67b6\uff0c\u8bc6\u522b\u6e38\u620f\u5316\u5b66\u4e60\u8bbe\u8ba1\u6846\u67b6\u7684\u5173\u952e\u8981\u6c42\uff0c\u5305\u62ec\u8f6c\u6362\u8fc7\u7a0b\u548c\u6700\u7ec8\u5b66\u4e60\u4f53\u9a8c\u4e24\u4e2a\u7ec4\u6210\u90e8\u5206\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u5e76\u8d2d\u540e\u4fe1\u606f\u7cfb\u7edf\u96c6\u6210\u7684\u6e38\u620f\u5316\u5b66\u4e60\u8bbe\u8ba1\u6846\u67b6\uff0c\u5e76\u5236\u5b9a\u4e86\u901a\u8fc7\u8fed\u4ee3\u8bbe\u8ba1\u548c\u5b9e\u9645\u9a8c\u8bc1\u6765\u5f00\u53d1\u548c\u8bc4\u4f30\u8be5\u6846\u67b6\u7684\u8ba1\u5212\u3002", "conclusion": "\u6e38\u620f\u5316\u5b66\u4e60\u8bbe\u8ba1\u80fd\u591f\u5c06\u9759\u6001\u65b9\u6cd5\u57f9\u8bad\u8f6c\u53d8\u4e3a\u5f15\u4eba\u5165\u80dc\u7684\u5b66\u4e60\u4f53\u9a8c\uff0c\u6709\u671b\u89e3\u51b3\u73b0\u6709\u4fe1\u606f\u7cfb\u7edf\u96c6\u6210\u57f9\u8bad\u4e2d\u7684\u52a8\u673a\u548c\u5b66\u4e60\u66f2\u7ebf\u95ee\u9898\u3002"}}
{"id": "2510.06307", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06307", "abs": "https://arxiv.org/abs/2510.06307", "authors": ["Wentao Deng", "Jiahuan Pei", "Zhiwei Xu", "Zhaochun Ren", "Zhumin Chen", "Pengjie Ren"], "title": "Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks", "comment": "This paper has been accepted by NeurIPS 2025", "summary": "A multi-agent system (MAS) enhances its capacity to solve complex natural\nlanguage processing (NLP) tasks through collaboration among multiple agents,\nwhere consensus-seeking serves as a fundamental mechanism. However, existing\nconsensus-seeking approaches typically rely on voting mechanisms to judge\nconsensus, overlooking contradictions in system-internal beliefs that\ndestabilize the consensus. Moreover, these methods often involve agents\nupdating their results through indiscriminate collaboration with every other\nagent. Such uniform interaction fails to identify the optimal collaborators for\neach agent, hindering the emergence of a stable consensus. To address these\nchallenges, we provide a theoretical framework for selecting optimal\ncollaborators that maximize consensus stability. Based on the theorems, we\npropose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate\nstable consensus via selecting optimal collaborators and calibrating the\nconsensus judgment by system-internal beliefs. Experimental results on the MATH\nand MMLU benchmark datasets demonstrate that the proposed BCCS framework\noutperforms the best existing results by 2.23% and 3.95% of accuracy on\nchallenging tasks, respectively. Our code and data are available at\nhttps://github.com/dengwentao99/BCCS.", "AI": {"tldr": "\u63d0\u51fa\u4e86BCCS\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u4f18\u5408\u4f5c\u8005\u548c\u6821\u51c6\u5171\u8bc6\u5224\u65ad\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5171\u8bc6\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728MATH\u548cMMLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5171\u8bc6\u5bfb\u6c42\u65b9\u6cd5\u4f9d\u8d56\u6295\u7968\u673a\u5236\uff0c\u5ffd\u89c6\u4e86\u7cfb\u7edf\u5185\u90e8\u4fe1\u5ff5\u7684\u77db\u76fe\uff0c\u4e14\u667a\u80fd\u4f53\u4e0e\u6240\u6709\u5176\u4ed6\u667a\u80fd\u4f53\u65e0\u5dee\u522b\u534f\u4f5c\uff0c\u65e0\u6cd5\u627e\u5230\u6700\u4f18\u5408\u4f5c\u8005\uff0c\u963b\u788d\u7a33\u5b9a\u5171\u8bc6\u7684\u5f62\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u7406\u8bba\u6846\u67b6\u6765\u9009\u62e9\u6700\u5927\u5316\u5171\u8bc6\u7a33\u5b9a\u6027\u7684\u6700\u4f18\u5408\u4f5c\u8005\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86BCCS\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u4f18\u5408\u4f5c\u8005\u548c\u6821\u51c6\u7cfb\u7edf\u5185\u90e8\u4fe1\u5ff5\u7684\u5171\u8bc6\u5224\u65ad\u6765\u4fc3\u8fdb\u7a33\u5b9a\u5171\u8bc6\u3002", "result": "\u5728MATH\u548cMMLU\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cBCCS\u6846\u67b6\u5728\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u5206\u522b\u6bd4\u73b0\u6709\u6700\u4f73\u7ed3\u679c\u63d0\u9ad8\u4e862.23%\u548c3.95%\u3002", "conclusion": "BCCS\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u5408\u4f5c\u8005\u9009\u62e9\u548c\u5171\u8bc6\u5224\u65ad\u6821\u51c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5171\u8bc6\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742NLP\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2510.06410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06410", "abs": "https://arxiv.org/abs/2510.06410", "authors": ["Aochong Oliver Li", "Tanya Goyal"], "title": "Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?", "comment": null, "summary": "Reasoning LLMs are trained to verbalize their reasoning process, yielding\nstrong gains on complex tasks. This transparency also opens a promising\ndirection: multiple reasoners can directly collaborate on each other's thinking\nwithin a shared trajectory, yielding better inference efficiency and\nexploration. A key prerequisite, however, is the ability to assess the\nusefulness and build on another model's partial thinking -- we call this\noff-trajectory reasoning. Our paper investigates a critical question: can\nstandard solo-reasoning training pipelines deliver desired off-trajectory\nbehaviors? We propose twin tests that capture the two extremes of the\noff-trajectory spectrum, namely Recoverability, which tests whether LLMs can\nbacktrack from \"distractions\" induced by misleading reasoning traces, and\nGuidability, which tests their ability to build upon correct reasoning from\nstronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and\nreveals a counterintuitive finding -- \"stronger\" LLMs on benchmarks are often\nmore fragile under distraction. Moreover, all models tested fail to effectively\nleverage guiding steps from collaborators on problems beyond their inherent\ncapabilities with solve rates remaining under 9.2%. Finally, we conduct control\nstudies to isolate the effects of three factors in post-training on these\nbehaviors: the choice of distillation teacher, the use of RL, and data\nselection strategy. Our results provide actionable insights for training\nnatively strong reasoning collaborators; e.g., we find that suboptimal\nrecoverability behaviors of teacher models are transferred to distilled\nstudents even if the distillation trajectories are correct. Taken together,\nthis work lays the groundwork for evaluating multi-model collaborations in\nshared reasoning trajectories and highlights the limitations of off-the-shelf\nreasoning LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\u5728\u534f\u4f5c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff1a\u66f4\u5f3a\u7684\u6a21\u578b\u53cd\u800c\u66f4\u5bb9\u6613\u88ab\u8bef\u5bfc\u63a8\u7406\u8f68\u8ff9\u5e72\u6270\uff0c\u4e14\u6240\u6709\u6a21\u578b\u90fd\u65e0\u6cd5\u6709\u6548\u5229\u7528\u534f\u4f5c\u8005\u7684\u6b63\u786e\u63a8\u7406\u6b65\u9aa4\u6765\u89e3\u51b3\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u6807\u51c6\u5355\u6a21\u578b\u63a8\u7406\u8bad\u7ec3\u6d41\u7a0b\u662f\u5426\u80fd\u4ea7\u751f\u6709\u6548\u7684\u534f\u4f5c\u63a8\u7406\u80fd\u529b\uff0c\u5373\u6a21\u578b\u80fd\u5426\u8bc4\u4f30\u548c\u5229\u7528\u5176\u4ed6\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u53cc\u6d4b\u8bd5\u6846\u67b6\uff1a\u53ef\u6062\u590d\u6027\u6d4b\u8bd5\uff08\u4ece\u8bef\u5bfc\u63a8\u7406\u4e2d\u6062\u590d\uff09\u548c\u53ef\u5f15\u5bfc\u6027\u6d4b\u8bd5\uff08\u5229\u7528\u66f4\u5f3a\u534f\u4f5c\u8005\u7684\u6b63\u786e\u63a8\u7406\uff09\uff0c\u8bc4\u4f3015\u4e2a\u5f00\u6e90LLM\uff081.5B-32B\uff09\uff0c\u5e76\u5206\u6790\u84b8\u998f\u6559\u5e08\u9009\u62e9\u3001RL\u4f7f\u7528\u548c\u6570\u636e\u9009\u62e9\u7b56\u7565\u7684\u5f71\u54cd\u3002", "result": "\u53cd\u76f4\u89c9\u53d1\u73b0\uff1a\u57fa\u51c6\u6d4b\u8bd5\u66f4\u5f3a\u7684LLM\u5728\u5e72\u6270\u4e0b\u66f4\u8106\u5f31\uff1b\u6240\u6709\u6a21\u578b\u5728\u8d85\u51fa\u81ea\u8eab\u80fd\u529b\u7684\u95ee\u9898\u4e0a\u5229\u7528\u5f15\u5bfc\u6b65\u9aa4\u7684\u89e3\u51b3\u7387\u4f4e\u4e8e9.2%\uff1b\u6559\u5e08\u6a21\u578b\u7684\u4e0d\u4f73\u53ef\u6062\u590d\u6027\u884c\u4e3a\u4f1a\u4f20\u9012\u7ed9\u84b8\u998f\u5b66\u751f\u3002", "conclusion": "\u5f53\u524d\u73b0\u6210\u7684\u63a8\u7406LLM\u5728\u534f\u4f5c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u9700\u8981\u4e13\u95e8\u8bad\u7ec3\u6765\u57f9\u517b\u539f\u751f\u5f3a\u534f\u4f5c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.06433", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06433", "abs": "https://arxiv.org/abs/2510.06433", "authors": ["Aryan Singh Dalal", "Yinglun Zhang", "Duru Do\u011fan", "Atalay Mert \u0130leri", "Hande K\u00fc\u00e7\u00fck McGinty"], "title": "Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health", "comment": null, "summary": "The focus on \"food as medicine\" is gaining traction in the field of health\nand several studies conducted in the past few years discussed this aspect of\nfood in the literature. However, very little research has been done on\nrepresenting the relationship between food and health in a standardized,\nmachine-readable format using a semantic web that can help us leverage this\nknowledge effectively. To address this gap, this study aims to create a\nknowledge graph to link food and health through the knowledge graph's ability\nto combine information from various platforms focusing on flavonoid contents of\nfood found in the USDA databases and cancer connections found in the\nliterature. We looked closely at these relationships using KNARM methodology\nand represented them in machine-operable format. The proposed knowledge graph\nserves as an example for researchers, enabling them to explore the complex\ninterplay between dietary choices and disease management. Future work for this\nstudy involves expanding the scope of the knowledge graph by capturing nuances,\nadding more related data, and performing inferences on the acquired knowledge\nto uncover hidden relationships.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u8bed\u4e49\u7f51\u7edc\u6807\u51c6\u5316\u8868\u793a\u98df\u7269\u4e0e\u5065\u5eb7\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u91cd\u70b9\u5173\u6ce8\u98df\u7269\u4e2d\u7684\u9ec4\u916e\u7c7b\u5316\u5408\u7269\u542b\u91cf\u4e0e\u764c\u75c7\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "motivation": "\u76ee\u524d\u5f88\u5c11\u6709\u7814\u7a76\u4f7f\u7528\u6807\u51c6\u5316\u7684\u673a\u5668\u53ef\u8bfb\u683c\u5f0f\u6765\u8868\u793a\u98df\u7269\u4e0e\u5065\u5eb7\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u9650\u5236\u4e86\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528KNARM\u65b9\u6cd5\uff0c\u7ed3\u5408USDA\u6570\u636e\u5e93\u4e2d\u7684\u98df\u7269\u9ec4\u916e\u7c7b\u5316\u5408\u7269\u542b\u91cf\u6570\u636e\u548c\u6587\u732e\u4e2d\u7684\u764c\u75c7\u5173\u8054\u4fe1\u606f\uff0c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u673a\u5668\u53ef\u64cd\u4f5c\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u80fd\u591f\u63a2\u7d22\u996e\u98df\u9009\u62e9\u4e0e\u75be\u75c5\u7ba1\u7406\u4e4b\u95f4\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u8be5\u77e5\u8bc6\u56fe\u8c31\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8303\u4f8b\uff0c\u672a\u6765\u5de5\u4f5c\u5c06\u6269\u5c55\u56fe\u8c31\u8303\u56f4\uff0c\u6dfb\u52a0\u66f4\u591a\u76f8\u5173\u6570\u636e\u5e76\u8fdb\u884c\u63a8\u7406\u4ee5\u53d1\u73b0\u9690\u85cf\u5173\u7cfb\u3002"}}
{"id": "2510.06475", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06475", "abs": "https://arxiv.org/abs/2510.06475", "authors": ["Yitao Long", "Yuru Jiang", "Hongjun Liu", "Yilun Zhao", "Jingchen Sun", "Yiqiu Shen", "Chen Zhao", "Arman Cohan", "Dennis Shasha"], "title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles", "comment": null, "summary": "This work investigates the reasoning and planning capabilities of foundation\nmodels and their scalability in complex, dynamic environments. We introduce\nPuzzlePlex, a benchmark designed to assess these capabilities through a diverse\nset of puzzles. PuzzlePlex consists of 15 types of puzzles, including\ndeterministic and stochastic games of varying difficulty, as well as\nsingle-player and two-player scenarios. The PuzzlePlex framework provides a\ncomprehensive environment for each game, and supports extensibility to generate\nmore challenging instances as foundation models evolve. Additionally, we\nimplement customized game-playing strategies for comparison. Building on this\nbenchmark, we develop fine-grained metrics to measure performance and conduct\nan in-depth analysis of frontier foundation models across two settings:\ninstruction-based and code-based. Furthermore, we systematically investigate\ntheir scaling limits. Our findings show that reasoning models outperform others\nin instruction-based settings, while code-based execution presents greater\nchallenges but offers a scalable and efficient alternative. PuzzlePlex enables\ntargeted evaluation and guides future improvements in reasoning, planning, and\ngeneralization for foundation models.", "AI": {"tldr": "PuzzlePlex\u662f\u4e00\u4e2a\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b15\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u8c1c\u9898\u6e38\u620f\uff0c\u652f\u6301\u6307\u4ee4\u5f0f\u548c\u4ee3\u7801\u5f0f\u4e24\u79cd\u6267\u884c\u65b9\u5f0f\uff0c\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u5728\u6307\u4ee4\u5f0f\u8bbe\u7f6e\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u7814\u7a76\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u53ca\u5176\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u5f00\u53d1PuzzlePlex\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b15\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u8c1c\u9898\u6e38\u620f\uff08\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\u3001\u5355\u4eba\u548c\u53cc\u4eba\u573a\u666f\uff09\uff0c\u5b9e\u73b0\u5b9a\u5236\u5316\u6e38\u620f\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u5f00\u53d1\u7ec6\u7c92\u5ea6\u6027\u80fd\u6307\u6807\u3002", "result": "\u63a8\u7406\u6a21\u578b\u5728\u6307\u4ee4\u5f0f\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u800c\u4ee3\u7801\u5f0f\u6267\u884c\u867d\u7136\u66f4\u5177\u6311\u6218\u6027\u4f46\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "PuzzlePlex\u80fd\u591f\u8fdb\u884c\u9488\u5bf9\u6027\u8bc4\u4f30\uff0c\u6307\u5bfc\u57fa\u7840\u6a21\u578b\u5728\u63a8\u7406\u3001\u89c4\u5212\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u672a\u6765\u6539\u8fdb\u3002"}}
{"id": "2510.06534", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06534", "abs": "https://arxiv.org/abs/2510.06534", "authors": ["Jiahe Jin", "Abhijay Paladugu", "Chenyan Xiong"], "title": "Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them", "comment": null, "summary": "Agentic search leverages large language models (LLMs) to interpret complex\nuser information needs and execute a multi-step process of planning, searching,\nand synthesizing information to provide answers. This paradigm introduces\nunique challenges for LLMs' reasoning and agentic capabilities when interacting\nwith retrieval systems and the broader web. In this paper, we propose a\nreasoning-driven LLM-based pipeline to study effective reasoning behavior\npatterns in agentic search. Using this pipeline, we analyze successful agentic\nsearch trajectories and identify four beneficial reasoning behaviors:\nInformation Verification, Authority Evaluation, Adaptive Search, and Error\nRecovery. Based on these findings, we propose a technique called Behavior\nPriming to train more effective agentic search models. It synthesizes agentic\nsearch trajectories that exhibit these four behaviors and integrates them into\nthe agentic search model through supervised fine-tuning (SFT), followed by\nstandard reinforcement learning (RL). Experiments on three benchmarks (GAIA,\nWebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in\nLlama3.2-3B and Qwen3-1.7B compared to directly training agentic search models\nwith RL. Crucially, we demonstrate that the desired reasoning behaviors in the\nSFT data, rather than the correctness of the final answer, is the critical\nfactor for achieving strong final performance after RL: fine-tuning on\ntrajectories with desirable reasoning behaviors but incorrect answers leads to\nbetter performance than fine-tuning on trajectories with correct answers. Our\nanalysis further reveals the underlying mechanism: the introduced reasoning\nbehaviors endow models with more effective exploration (higher pass@k and\nentropy) and test-time scaling (longer trajectories) capabilities, providing a\nstrong foundation for RL. Our code will be released as open source.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u884c\u4e3a\u5f15\u5bfc\u6280\u672f\uff0c\u901a\u8fc7\u8bc6\u522b\u56db\u79cd\u6709\u76ca\u63a8\u7406\u884c\u4e3a\uff08\u4fe1\u606f\u9a8c\u8bc1\u3001\u6743\u5a01\u8bc4\u4f30\u3001\u81ea\u9002\u5e94\u641c\u7d22\u3001\u9519\u8bef\u6062\u590d\uff09\u6765\u8bad\u7ec3\u66f4\u6709\u6548\u7684\u667a\u80fd\u641c\u7d22\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u8d85\u8fc735%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u667a\u80fd\u641c\u7d22\u5229\u7528LLMs\u89e3\u91ca\u590d\u6742\u7528\u6237\u4fe1\u606f\u9700\u6c42\u5e76\u6267\u884c\u591a\u6b65\u9aa4\u641c\u7d22\u8fc7\u7a0b\uff0c\u8fd9\u5bf9LLMs\u7684\u63a8\u7406\u548c\u667a\u80fd\u80fd\u529b\u63d0\u51fa\u4e86\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u7814\u7a76\u6709\u6548\u7684\u63a8\u7406\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u9a71\u52a8\u7684LLM\u7ba1\u9053\u5206\u6790\u6210\u529f\u641c\u7d22\u8f68\u8ff9\uff0c\u8bc6\u522b\u56db\u79cd\u6709\u76ca\u63a8\u7406\u884c\u4e3a\uff0c\u7136\u540e\u901a\u8fc7\u884c\u4e3a\u5f15\u5bfc\u6280\u672f\u5408\u6210\u5c55\u73b0\u8fd9\u4e9b\u884c\u4e3a\u7684\u8f68\u8ff9\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08GAIA\u3001WebWalker\u3001HLE\uff09\u4e0a\uff0c\u884c\u4e3a\u5f15\u5bfc\u4f7fLlama3.2-3B\u548cQwen3-1.7B\u76f8\u6bd4\u76f4\u63a5RL\u8bad\u7ec3\u83b7\u5f97\u8d85\u8fc735%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14SFT\u6570\u636e\u4e2d\u671f\u671b\u7684\u63a8\u7406\u884c\u4e3a\u6bd4\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\u66f4\u91cd\u8981\u3002", "conclusion": "\u5f15\u5165\u7684\u63a8\u7406\u884c\u4e3a\u8d4b\u4e88\u6a21\u578b\u66f4\u6709\u6548\u7684\u63a2\u7d22\u80fd\u529b\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u80fd\u529b\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u63a8\u7406\u884c\u4e3a\u8d28\u91cf\u5bf9\u6700\u7ec8\u6027\u80fd\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2510.06538", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06538", "abs": "https://arxiv.org/abs/2510.06538", "authors": ["Jiajie Li", "Huayi Zhang", "Peng Lin", "Jinjun Xiong", "Wei Xu"], "title": "Auto-Prompt Ensemble for LLM Judge", "comment": null, "summary": "We present a novel framework that improves the reliability of LLM judges by\nselectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM\njudges often miss crucial evaluation dimensions because they fail to recognize\nthe implicit standards underlying human assessments. To address this challenge,\nwe propose the Auto-Prompt Ensemble (APE), an adaptive framework that\nautomatically learns evaluation dimensions from its failure cases. APE\nincorporates a confidence-based ensemble mechanism to decide when to adopt the\njudgments from additional evaluation dimensions through a novel confidence\nestimation approach called Collective Confidence. Extensive experiments\ndemonstrate that APE improves the reliability of LLM Judge across diverse\nstandard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward\nBench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a\nprincipled approach for LLM Judge to leverage test-time computation, and bridge\nthe evaluation gap between human and LLM judges.", "AI": {"tldr": "\u63d0\u51faAuto-Prompt Ensemble (APE)\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5b66\u4e60\u8bc4\u4f30\u7ef4\u5ea6\u6765\u63d0\u5347LLM\u8bc4\u5224\u8005\u7684\u53ef\u9760\u6027", "motivation": "\u73b0\u6709LLM\u8bc4\u5224\u8005\u7ecf\u5e38\u5ffd\u7565\u5173\u952e\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u8bc6\u522b\u4eba\u7c7b\u8bc4\u4f30\u80cc\u540e\u7684\u9690\u542b\u6807\u51c6", "method": "APE\u6846\u67b6\u5305\u542b\u81ea\u9002\u5e94\u5b66\u4e60\u8bc4\u4f30\u7ef4\u5ea6\u7684\u673a\u5236\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u91c7\u7528Collective Confidence\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5", "result": "APE\u5728Reward Bench\u4e0a\u5c06GPT-4o\u7684\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u4e00\u81f4\u6027\u7387\u4ece87.2%\u63d0\u5347\u523090.5%", "conclusion": "APE\u4e3aLLM\u8bc4\u5224\u8005\u63d0\u4f9b\u4e86\u5229\u7528\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7684\u539f\u7406\u6027\u65b9\u6cd5\uff0c\u5f25\u5408\u4e86\u4eba\u7c7b\u4e0eLLM\u8bc4\u5224\u8005\u4e4b\u95f4\u7684\u8bc4\u4f30\u5dee\u8ddd"}}
{"id": "2510.06587", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06587", "abs": "https://arxiv.org/abs/2510.06587", "authors": ["Jingbo Yang", "Bairu Hou", "Wei Wei", "Shiyu Chang", "Yujia Bao"], "title": "WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks", "comment": null, "summary": "Large language model (LLM) agents are becoming competent at straightforward\nweb tasks, such as opening an item page or submitting a form, but still\nstruggle with objectives that require long horizon navigation, large scale\ninformation extraction, and reasoning under constraints. We present WebDART, a\ngeneral framework that enables a single LLM to handle such complex chores.\nWebDART (i) dynamically decomposes each objective into three focused subtasks:\nnavigation, information extraction, and execution, so the model concentrates on\none skill at a time, and (ii) continuously replans the decomposition as new\nwebpages are revealed, taking advantage of newly discovered filters or\nshortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,\nWebDART lifts success rates by up to 13.7 percentage points over previous SOTA\nagents, while matching their performance on the easier WebArena suite and\ncompleting tasks with up to 14.7 fewer navigation steps.", "AI": {"tldr": "WebDART\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u5206\u89e3\u548c\u6301\u7eed\u91cd\u89c4\u5212\uff0c\u4f7f\u5355\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u7f51\u9875\u4efb\u52a1\uff0c\u5728WebChoreArena\u4e0a\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u4ee3\u7406\u63d0\u5347\u4e8613.7\u4e2a\u767e\u5206\u70b9\u7684\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u7b80\u5355\u7f51\u9875\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u957f\u89c6\u91ce\u5bfc\u822a\u3001\u5927\u89c4\u6a21\u4fe1\u606f\u63d0\u53d6\u548c\u7ea6\u675f\u63a8\u7406\u7684\u590d\u6742\u76ee\u6807\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002", "method": "WebDART\u6846\u67b6\uff08i\uff09\u5c06\u6bcf\u4e2a\u76ee\u6807\u52a8\u6001\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u6ce8\u5b50\u4efb\u52a1\uff1a\u5bfc\u822a\u3001\u4fe1\u606f\u63d0\u53d6\u548c\u6267\u884c\uff1b\uff08ii\uff09\u968f\u7740\u65b0\u7f51\u9875\u7684\u63ed\u793a\u6301\u7eed\u91cd\u89c4\u5212\u5206\u89e3\uff0c\u5229\u7528\u65b0\u53d1\u73b0\u7684\u8fc7\u6ee4\u5668\u6216\u6377\u5f84\uff0c\u907f\u514d\u5197\u4f59\u63a2\u7d22\u3002", "result": "\u5728WebChoreArena\u4e0a\uff0cWebDART\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u4ee3\u7406\u63d0\u5347\u4e8613.7\u4e2a\u767e\u5206\u70b9\u7684\u6210\u529f\u7387\uff0c\u5728\u66f4\u7b80\u5355\u7684WebArena\u5957\u4ef6\u4e0a\u8868\u73b0\u76f8\u5f53\uff0c\u5b8c\u6210\u4efb\u52a1\u65f6\u5bfc\u822a\u6b65\u9aa4\u51cf\u5c11\u4e8614.7\u6b65\u3002", "conclusion": "WebDART\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u5206\u89e3\u548c\u6301\u7eed\u91cd\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u7f51\u9875\u4efb\u52a1\u7684\u80fd\u529b\u3002"}}
{"id": "2510.06600", "categories": ["cs.AI", "H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.06600", "abs": "https://arxiv.org/abs/2510.06600", "authors": ["Zhaochun Ren", "Zhou Yang", "Chenglong Ye", "Haizhou Sun", "Chao Chen", "Xiaofei Zhu", "Xiangwen Liao"], "title": "Fine-Grained Emotion Recognition via In-Context Learning", "comment": "9 pages, 10 figures, 4 tables", "summary": "Fine-grained emotion recognition aims to identify the emotional type in\nqueries through reasoning and decision-making processes, playing a crucial role\nin various systems. Recent methods use In-Context Learning (ICL), enhancing the\nrepresentation of queries in the reasoning process through semantically similar\nexamples, while further improving emotion recognition by explaining the\nreasoning mechanisms. However, these methods enhance the reasoning process but\noverlook the decision-making process. This paper investigates decision-making\nin fine-grained emotion recognition through prototype theory. We show that ICL\nrelies on similarity matching between query representations and emotional\nprototypes within the model, where emotion-accurate representations are\ncritical. However, semantically similar examples often introduce emotional\ndiscrepancies, hindering accurate representations and causing errors. To\naddress this, we propose Emotion In-Context Learning (EICL), which introduces\nemotionally similar examples and uses a dynamic soft-label strategy to improve\nquery representations in the emotion reasoning process. A two-stage exclusion\nstrategy is then employed to assess similarity from multiple angles, further\noptimizing the decision-making process. Extensive experiments show that EICL\nsignificantly outperforms ICL on multiple datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEICL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u60c5\u611f\u76f8\u4f3c\u793a\u4f8b\u548c\u52a8\u6001\u8f6f\u6807\u7b7e\u7b56\u7565\u6765\u6539\u8fdb\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u63a8\u7406\u548c\u51b3\u7b56\u8fc7\u7a0b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfICL\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709ICL\u65b9\u6cd5\u867d\u7136\u589e\u5f3a\u4e86\u60c5\u611f\u8bc6\u522b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f46\u5ffd\u7565\u4e86\u51b3\u7b56\u8fc7\u7a0b\u3002\u8bed\u4e49\u76f8\u4f3c\u793a\u4f8b\u5e38\u5f15\u5165\u60c5\u611f\u5dee\u5f02\uff0c\u963b\u788d\u51c6\u786e\u7684\u60c5\u611f\u8868\u793a\u3002", "method": "\u63d0\u51faEICL\u65b9\u6cd5\uff1a\u5f15\u5165\u60c5\u611f\u76f8\u4f3c\u793a\u4f8b\uff0c\u4f7f\u7528\u52a8\u6001\u8f6f\u6807\u7b7e\u7b56\u7565\u6539\u8fdb\u67e5\u8be2\u8868\u793a\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u6392\u9664\u7b56\u7565\u4ece\u591a\u89d2\u5ea6\u8bc4\u4f30\u76f8\u4f3c\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEICL\u663e\u8457\u4f18\u4e8eICL\u65b9\u6cd5\u3002", "conclusion": "EICL\u901a\u8fc7\u6539\u8fdb\u60c5\u611f\u8868\u793a\u548c\u51b3\u7b56\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86ICL\u5728\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.06674", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06674", "abs": "https://arxiv.org/abs/2510.06674", "authors": ["Cen", "Zhao", "Tiantian Zhang", "Hanchen Su", "Yufeng", "Zhang", "Shaowei Su", "Mingzhi Xu", "Yu", "Liu", "Wei Han", "Jeremy Werner", "Claire Na Cheng", "Yashar Mehdad"], "title": "Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support", "comment": "EMNLP 2025 Industry Track submission (Paper #305). Preprint. Main\n  text within the 7-page industry limit (references/appendices excluded).\n  Contains multiple figures and tables", "summary": "We introduce an Agent-in-the-Loop (AITL) framework that implements a\ncontinuous data flywheel for iteratively improving an LLM-based customer\nsupport system. Unlike standard offline approaches that rely on batch\nannotations, AITL integrates four key types of annotations directly into live\ncustomer operations: (1) pairwise response preferences, (2) agent adoption and\nrationales, (3) knowledge relevance checks, and (4) identification of missing\nknowledge. These feedback signals seamlessly feed back into models' updates,\nreducing retraining cycles from months to weeks. Our production pilot involving\nUS-based customer support agents demonstrated significant improvements in\nretrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality\n(+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore\nthe effectiveness of embedding human feedback loops directly into operational\nworkflows to continuously refine LLM-based customer support system.", "AI": {"tldr": "\u63d0\u51fa\u4e86Agent-in-the-Loop\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u53cd\u9988\u5faa\u73af\u6301\u7eed\u6539\u8fdb\u57fa\u4e8eLLM\u7684\u5ba2\u670d\u7cfb\u7edf\uff0c\u5c06\u91cd\u8bad\u7ec3\u5468\u671f\u4ece\u6570\u6708\u7f29\u77ed\u81f3\u6570\u5468\u3002", "motivation": "\u4f20\u7edf\u7684\u79bb\u7ebf\u6279\u91cf\u6807\u6ce8\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5b9e\u65f6\u53cd\u9988\u673a\u5236\u6765\u6301\u7eed\u6539\u8fdb\u5ba2\u670d\u7cfb\u7edf\u3002", "method": "AITL\u6846\u67b6\u6574\u5408\u56db\u79cd\u5173\u952e\u6807\u6ce8\u7c7b\u578b\u5230\u5b9e\u65f6\u5ba2\u670d\u8fd0\u8425\u4e2d\uff1a\u54cd\u5e94\u504f\u597d\u5bf9\u6bd4\u3001\u5ba2\u670d\u91c7\u7eb3\u4e0e\u7406\u7531\u3001\u77e5\u8bc6\u76f8\u5173\u6027\u68c0\u67e5\u3001\u7f3a\u5931\u77e5\u8bc6\u8bc6\u522b\u3002", "result": "\u751f\u4ea7\u8bd5\u70b9\u663e\u793a\u68c0\u7d22\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff08\u53ec\u56de\u7387+11.7%\uff0c\u7cbe\u786e\u7387+14.8%\uff09\uff0c\u751f\u6210\u8d28\u91cf\u63d0\u5347\uff08\u5e2e\u52a9\u6027+8.4%\uff09\uff0c\u5ba2\u670d\u91c7\u7eb3\u7387\u63d0\u5347\uff08+4.5%\uff09\u3002", "conclusion": "\u5c06\u4eba\u5de5\u53cd\u9988\u5faa\u73af\u76f4\u63a5\u5d4c\u5165\u8fd0\u8425\u5de5\u4f5c\u6d41\u7a0b\u80fd\u6709\u6548\u6301\u7eed\u6539\u8fdb\u57fa\u4e8eLLM\u7684\u5ba2\u670d\u7cfb\u7edf\u3002"}}
{"id": "2510.06711", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06711", "abs": "https://arxiv.org/abs/2510.06711", "authors": ["Batu El", "Mert Yuksekgonul", "James Zou"], "title": "Inefficiencies of Meta Agents for Agent Design", "comment": null, "summary": "Recent works began to automate the design of agentic systems using\nmeta-agents that propose and iteratively refine new agent architectures. In\nthis paper, we examine three key challenges in a common class of meta-agents.\nFirst, we investigate how a meta-agent learns across iterations and find that\nsimply expanding the context with all previous agents, as proposed by previous\nworks, performs worse than ignoring prior designs entirely. We show that the\nperformance improves with an evolutionary approach. Second, although the\nmeta-agent designs multiple agents during training, it typically commits to a\nsingle agent at test time. We find that the designed agents have low behavioral\ndiversity, limiting the potential for their complementary use. Third, we assess\nwhen automated design is economically viable. We find that only in a few\ncases--specifically, two datasets--the overall cost of designing and deploying\nthe agents is lower than that of human-designed agents when deployed on over\n15,000 examples. In contrast, the performance gains for other datasets do not\njustify the design cost, regardless of scale.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5143\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u8bbe\u8ba1\u4ee3\u7406\u7cfb\u7edf\u65f6\u9762\u4e34\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u8de8\u8fed\u4ee3\u5b66\u4e60\u6548\u7387\u4f4e\u3001\u884c\u4e3a\u591a\u6837\u6027\u4e0d\u8db3\u4ee5\u53ca\u7ecf\u6d4e\u53ef\u884c\u6027\u6709\u9650\u3002", "motivation": "\u7814\u7a76\u5143\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u8bbe\u8ba1\u4ee3\u7406\u7cfb\u7edf\u65f6\u5b58\u5728\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u5305\u62ec\u5b66\u4e60\u6548\u7387\u3001\u884c\u4e3a\u591a\u6837\u6027\u548c\u7ecf\u6d4e\u6210\u672c\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u5143\u4ee3\u7406\u7684\u8de8\u8fed\u4ee3\u5b66\u4e60\u7b56\u7565\u3001\u8bc4\u4f30\u8bbe\u8ba1\u4ee3\u7406\u7684\u884c\u4e3a\u591a\u6837\u6027\uff0c\u4ee5\u53ca\u8ba1\u7b97\u81ea\u52a8\u5316\u8bbe\u8ba1\u7684\u7ecf\u6d4e\u6210\u672c\u6548\u76ca\u3002", "result": "\u53d1\u73b0\u7b80\u5355\u6269\u5c55\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u6548\u679c\u4e0d\u5982\u5ffd\u7565\u5148\u524d\u8bbe\u8ba1\uff1b\u8bbe\u8ba1\u4ee3\u7406\u884c\u4e3a\u591a\u6837\u6027\u4f4e\uff1b\u4ec5\u5728\u5c11\u6570\u60c5\u51b5\u4e0b\u81ea\u52a8\u5316\u8bbe\u8ba1\u6bd4\u4eba\u5de5\u8bbe\u8ba1\u66f4\u7ecf\u6d4e\u3002", "conclusion": "\u5f53\u524d\u5143\u4ee3\u7406\u65b9\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u5e76\u4e0d\u7ecf\u6d4e\u53ef\u884c\uff0c\u9700\u8981\u6539\u8fdb\u5b66\u4e60\u7b56\u7565\u548c\u589e\u52a0\u884c\u4e3a\u591a\u6837\u6027\u624d\u80fd\u5b9e\u73b0\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.06742", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06742", "abs": "https://arxiv.org/abs/2510.06742", "authors": ["Ali Sarabadani", "Kheirolah Rahsepar Fard"], "title": "MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models", "comment": null, "summary": "The advent of large language models (LLMs) has revolutionized the integration\nof knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming\nlimitations in traditional machine learning methods for capturing intricate\nsemantic links among genes, diseases, and cognitive processes. We introduce\nMultiCNKG, an innovative framework that merges three key knowledge sources: the\nCognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges\nacross 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes\nand 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)\ncomprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.\nLeveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity\ncomputation, and graph augmentation to create a cohesive KG that interconnects\ngenetic mechanisms, neurological disorders, and cognitive functions. The\nresulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,\nDiseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,\nAssociated with, Regulates), facilitating a multi-layered view from molecular\nto behavioral domains. Assessments using metrics such as precision (85.20%),\nrecall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty\ndetection (40.28%), and expert validation (89.50%) affirm its robustness and\ncoherence. Link prediction evaluations with models like TransE (MR: 391, MRR:\n0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against\nbenchmarks like FB15k-237 and WN18RR. This KG advances applications in\npersonalized medicine, cognitive disorder diagnostics, and hypothesis\nformulation in cognitive neuroscience.", "AI": {"tldr": "MultiCNKG\u662f\u4e00\u4e2a\u878d\u5408\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u77e5\u8bc6\u56fe\u8c31\u3001\u57fa\u56e0\u672c\u4f53\u548c\u75be\u75c5\u672c\u4f53\u7684\u521b\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u4f53\u5bf9\u9f50\u548c\u56fe\u589e\u5f3a\uff0c\u6784\u5efa\u8fde\u63a5\u57fa\u56e0\u673a\u5236\u3001\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u548c\u8ba4\u77e5\u529f\u80fd\u7684\u7edf\u4e00\u77e5\u8bc6\u56fe\u8c31\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u6355\u6349\u57fa\u56e0\u3001\u75be\u75c5\u548c\u8ba4\u77e5\u8fc7\u7a0b\u4e4b\u95f4\u590d\u6742\u8bed\u4e49\u8054\u7cfb\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6765\u6574\u5408\u591a\u4e2a\u77e5\u8bc6\u6e90\uff0c\u6784\u5efa\u66f4\u5168\u9762\u7684\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u3002", "method": "\u6574\u5408\u4e09\u4e2a\u5173\u952e\u77e5\u8bc6\u6e90\uff1a\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u77e5\u8bc6\u56fe\u8c31\uff08CNKG\uff09\u3001\u57fa\u56e0\u672c\u4f53\uff08GO\uff09\u548c\u75be\u75c5\u672c\u4f53\uff08DO\uff09\uff0c\u5229\u7528GPT-4\u7b49\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u4f53\u5bf9\u9f50\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u548c\u56fe\u589e\u5f3a\u3002", "result": "\u6784\u5efa\u7684MultiCNKG\u5305\u542b6.9K\u4e2a\u8282\u70b9\u548c11.3K\u6761\u8fb9\uff0c\u5728\u7cbe\u5ea6\uff0885.20%\uff09\u3001\u53ec\u56de\u7387\uff0887.30%\uff09\u3001\u8986\u76d6\u7387\uff0892.18%\uff09\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u94fe\u63a5\u9884\u6d4b\u8bc4\u4f30\u663e\u793a\u4e0e\u57fa\u51c6\u6570\u636e\u96c6\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u77e5\u8bc6\u56fe\u8c31\u63a8\u52a8\u4e86\u4e2a\u6027\u5316\u533b\u7597\u3001\u8ba4\u77e5\u969c\u788d\u8bca\u65ad\u548c\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u5047\u8bbe\u5236\u5b9a\u7b49\u5e94\u7528\uff0c\u4e3a\u4ece\u5206\u5b50\u5230\u884c\u4e3a\u9886\u57df\u7684\u591a\u5c42\u7ea7\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2510.06756", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06756", "abs": "https://arxiv.org/abs/2510.06756", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb"], "title": "Verifying Memoryless Sequential Decision-making of Large Language Models", "comment": null, "summary": "We introduce a tool for rigorous and automated verification of large language\nmodel (LLM)- based policies in memoryless sequential decision-making tasks.\nGiven a Markov decision process (MDP) representing the sequential\ndecision-making task, an LLM policy, and a safety requirement expressed as a\nPCTL formula, our approach incrementally constructs only the reachable portion\nof the MDP guided by the LLM's chosen actions. Each state is encoded as a\nnatural language prompt, the LLM's response is parsed into an action, and\nreachable successor states by the policy are expanded. The resulting formal\nmodel is checked with Storm to determine whether the policy satisfies the\nspecified safety property. In experiments on standard grid world benchmarks, we\nshow that open source LLMs accessed via Ollama can be verified when\ndeterministically seeded, but generally underperform deep reinforcement\nlearning baselines. Our tool natively integrates with Ollama and supports\nPRISM-specified tasks, enabling continuous benchmarking in user-specified\nsequential decision-making tasks and laying a practical foundation for formally\nverifying increasingly capable LLMs.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a8c\u8bc1\u57fa\u4e8eLLM\u7b56\u7565\u7684\u5de5\u5177\uff0c\u5728\u65e0\u8bb0\u5fc6\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u4e2d\u68c0\u67e5\u5b89\u5168\u5c5e\u6027\u3002", "motivation": "\u968f\u7740LLM\u5728\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u589e\u591a\uff0c\u9700\u8981\u4e25\u683c\u9a8c\u8bc1\u5176\u7b56\u7565\u662f\u5426\u6ee1\u8db3\u5b89\u5168\u8981\u6c42\uff0c\u786e\u4fdd\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "method": "\u7ed9\u5b9aMDP\u3001LLM\u7b56\u7565\u548cPCTL\u5b89\u5168\u8981\u6c42\uff0c\u589e\u91cf\u6784\u5efa\u53ef\u8fbe\u72b6\u6001\u7a7a\u95f4\uff0c\u5c06\u72b6\u6001\u7f16\u7801\u4e3a\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u89e3\u6790LLM\u54cd\u5e94\u4e3a\u52a8\u4f5c\uff0c\u7528Storm\u6a21\u578b\u68c0\u67e5\u5668\u9a8c\u8bc1\u5b89\u5168\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f00\u6e90LLM\u5728\u786e\u5b9a\u6027\u79cd\u5b50\u4e0b\u53ef\u9a8c\u8bc1\uff0c\u4f46\u6027\u80fd\u901a\u5e38\u4e0d\u5982\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u5de5\u5177\u4e3a\u9a8c\u8bc1\u65e5\u76ca\u5f3a\u5927\u7684LLM\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u652f\u6301\u4e0eOllama\u96c6\u6210\u548cPRISM\u6307\u5b9a\u4efb\u52a1\u3002"}}
{"id": "2510.06761", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06761", "abs": "https://arxiv.org/abs/2510.06761", "authors": ["Zhi Zhang", "Yan Liu", "Zhejing Hu", "Gong Chen", "Sheng-hua Zhong", "Jiannong Cao"], "title": "Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration", "comment": null, "summary": "Automating the end-to-end scientific research process poses a fundamental\nchallenge: it requires both evolving high-level plans that are novel and sound,\nand executing these plans correctly amidst dynamic and uncertain conditions. To\naddress this bilevel challenge, we propose a novel Double-Loop Multi-Agent\n(DLMA) framework to solve the given research problem automatically. The leader\nloop, composed of professor agents, is responsible for evolving research plans.\nIt employs an evolutionary algorithm through involvement, improvement, and\nintegration meetings to iteratively generate and refine a pool of research\nproposals, exploring the solution space effectively. The follower loop,\ncomposed of doctoral student agents, is responsible for executing the\nbest-evolved plan. It dynamically adjusts the plan during implementation via\npre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is\nwell-supported by contextual and external observations. Extensive experiments\non benchmarks like ACLAward and Laboratory show that DLMA generates research\npapers that achieve state-of-the-art scores in automated evaluation,\nsignificantly outperforming strong baselines. Ablation studies confirm the\ncritical roles of both loops, with evolution driving novelty and execution\nensuring soundness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5faa\u73af\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff08DLMA\uff09\uff0c\u901a\u8fc7\u9886\u5bfc\u5faa\u73af\uff08\u6559\u6388\u667a\u80fd\u4f53\uff09\u8fdb\u5316\u7814\u7a76\u8ba1\u5212\uff0c\u8ddf\u968f\u5faa\u73af\uff08\u535a\u58eb\u5b66\u751f\u667a\u80fd\u4f53\uff09\u6267\u884c\u6700\u4f73\u8ba1\u5212\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u79d1\u5b66\u7814\u7a76\u3002", "motivation": "\u81ea\u52a8\u5316\u7aef\u5230\u7aef\u79d1\u5b66\u7814\u7a76\u9762\u4e34\u53cc\u91cd\u6311\u6218\uff1a\u9700\u8981\u751f\u6210\u65b0\u9896\u4e14\u5408\u7406\u7684\u9ad8\u5c42\u8ba1\u5212\uff0c\u5e76\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u6b63\u786e\u6267\u884c\u8fd9\u4e9b\u8ba1\u5212\u3002", "method": "DLMA\u6846\u67b6\u5305\u542b\u9886\u5bfc\u5faa\u73af\u548c\u8ddf\u968f\u5faa\u73af\u3002\u9886\u5bfc\u5faa\u73af\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\u901a\u8fc7\u53c2\u4e0e\u3001\u6539\u8fdb\u548c\u6574\u5408\u4f1a\u8bae\u8fed\u4ee3\u751f\u6210\u548c\u4f18\u5316\u7814\u7a76\u63d0\u6848\uff1b\u8ddf\u968f\u5faa\u73af\u901a\u8fc7\u4e8b\u524d\u548c\u4e8b\u540e\u4f1a\u8bae\u52a8\u6001\u8c03\u6574\u8ba1\u5212\u6267\u884c\u3002", "result": "\u5728ACLAward\u548cLaboratory\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDLMA\u751f\u6210\u7684\u7814\u7a76\u8bba\u6587\u5728\u81ea\u52a8\u5316\u8bc4\u4f30\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e24\u4e2a\u5faa\u73af\u90fd\u81f3\u5173\u91cd\u8981\uff0c\u8fdb\u5316\u9a71\u52a8\u65b0\u9896\u6027\uff0c\u6267\u884c\u786e\u4fdd\u5408\u7406\u6027\uff0cDLMA\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u52a8\u5316\u79d1\u5b66\u7814\u7a76\u7684\u53cc\u5c42\u6311\u6218\u3002"}}
{"id": "2510.06857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06857", "abs": "https://arxiv.org/abs/2510.06857", "authors": ["Qi Guo", "Jianing Wang", "Jianfei Zhang", "Deyang Kong", "Xiangzhou Huang", "Xiangyu Xi", "Wei Wang", "Jingang Wang", "Xunliang Cai", "Shikun Zhang", "Wei Ye"], "title": "Autoformalizer with Tool Feedback", "comment": null, "summary": "Autoformalization addresses the scarcity of data for Automated Theorem\nProving (ATP) by translating mathematical problems from natural language into\nformal statements. Efforts in recent work shift from directly prompting large\nlanguage models to training an end-to-end formalizer model from scratch,\nachieving remarkable advancements. However, existing formalizer still struggles\nto consistently generate valid statements that meet syntactic validity and\nsemantic consistency. To address this issue, we propose the Autoformalizer with\nTool Feedback (ATF), a novel approach that incorporates syntactic and\nconsistency information as tools into the formalization process. By integrating\nLean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge\napproach for consistency validation, the model is able to adaptively refine\ngenerated statements according to the tool feedback, enhancing both syntactic\nvalidity and semantic consistency. The training of ATF involves a cold-start\nphase on synthetic tool-calling data, an expert iteration phase to improve\nformalization capabilities, and Direct Preference Optimization to alleviate\nineffective revisions. Experimental results show that ATF markedly outperforms\na range of baseline formalizer models, with its superior performance further\nvalidated by human evaluations. Subsequent analysis reveals that ATF\ndemonstrates excellent inference scaling properties. Moreover, we open-source\nNumina-ATF, a dataset containing 750K synthetic formal statements to facilitate\nadvancements in autoformalization and ATP research.", "AI": {"tldr": "\u63d0\u51faATF\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u8bed\u6cd5\u4fee\u6b63\u548c\u4e00\u81f4\u6027\u9a8c\u8bc1\u5de5\u5177\u53cd\u9988\u6765\u6539\u8fdb\u81ea\u52a8\u5f62\u5f0f\u5316\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f62\u5f0f\u5316\u9648\u8ff0\u7684\u8bed\u6cd5\u6709\u6548\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u5f62\u5f0f\u5316\u5668\u5728\u751f\u6210\u7b26\u5408\u8bed\u6cd5\u6709\u6548\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u6709\u6548\u9648\u8ff0\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u6539\u8fdb\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u8d28\u91cf\u3002", "method": "\u7ed3\u5408Lean 4\u7f16\u8bd1\u5668\u8fdb\u884c\u8bed\u6cd5\u4fee\u6b63\uff0c\u91c7\u7528\u591aLLMs\u4f5c\u4e3a\u8bc4\u5224\u8005\u8fdb\u884c\u4e00\u81f4\u6027\u9a8c\u8bc1\uff0c\u901a\u8fc7\u5de5\u5177\u53cd\u9988\u81ea\u9002\u5e94\u4f18\u5316\u751f\u6210\u9648\u8ff0\u3002\u8bad\u7ec3\u5305\u62ec\u51b7\u542f\u52a8\u9636\u6bb5\u3001\u4e13\u5bb6\u8fed\u4ee3\u9636\u6bb5\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u3002", "result": "ATF\u663e\u8457\u4f18\u4e8e\u4e00\u7cfb\u5217\u57fa\u7ebf\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u79c0\u7684\u63a8\u7406\u6269\u5c55\u7279\u6027\u3002", "conclusion": "ATF\u65b9\u6cd5\u901a\u8fc7\u5de5\u5177\u53cd\u9988\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u8d28\u91cf\uff0c\u5f00\u6e90Numina-ATF\u6570\u636e\u96c6\u5c06\u4fc3\u8fdb\u81ea\u52a8\u5f62\u5f0f\u5316\u548c\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.06878", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06878", "abs": "https://arxiv.org/abs/2510.06878", "authors": ["Daria Ozerova", "Ekaterina Trofimova"], "title": "TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs", "comment": null, "summary": "Iterative refinement has been a promising paradigm to enable large language\nmodels (LLMs) to resolve difficult reasoning and problem-solving tasks. One of\nthe key challenges, however, is how to effectively search through the enormous\nsearch space of possible refinements. Existing methods typically fall back on\npredefined heuristics, which are troubled by the exploration-exploitation\ndilemma and cannot adapt based on past refinement outcomes. We introduce\nTree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with\na Thompson-Sampling-based tree search. TGPR explores both failed and successful\nrefinement paths actively, with denser training trajectories and more adaptive\npolicies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to\n+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to\n+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to\na competitive GRPO baseline. Apart from debugging code, TGPR focuses on a\nprincipled approach to combining learned policies with structured search\nmethods, offering a general framework for enhancing iterative refinement and\nstateful reasoning in LLMs.", "AI": {"tldr": "TGPR\u662f\u4e00\u4e2a\u7ed3\u5408GRPO\u548cThompson\u91c7\u6837\u7684\u6811\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u5931\u8d25\u548c\u6210\u529f\u7684\u7cbe\u5316\u8def\u5f84\u6765\u6539\u8fdbLLM\u7684\u8fed\u4ee3\u7cbe\u5316\u80fd\u529b\uff0c\u5728\u4ee3\u7801\u8c03\u8bd5\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8fed\u4ee3\u7cbe\u5316\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u542f\u53d1\u5f0f\uff0c\u9762\u4e34\u63a2\u7d22-\u5229\u7528\u56f0\u5883\u4e14\u65e0\u6cd5\u6839\u636e\u5386\u53f2\u7cbe\u5316\u7ed3\u679c\u81ea\u9002\u5e94\u8c03\u6574\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u641c\u7d22\u7b56\u7565\u6765\u5904\u7406\u5e9e\u5927\u7684\u7cbe\u5316\u7a7a\u95f4\u3002", "method": "\u63d0\u51faTGPR\u6846\u67b6\uff0c\u5c06GRPO\u4e0e\u57fa\u4e8eThompson\u91c7\u6837\u7684\u6811\u641c\u7d22\u76f8\u7ed3\u5408\uff0c\u4e3b\u52a8\u63a2\u7d22\u5931\u8d25\u548c\u6210\u529f\u7684\u7cbe\u5316\u8def\u5f84\uff0c\u751f\u6210\u66f4\u5bc6\u96c6\u7684\u8bad\u7ec3\u8f68\u8ff9\u548c\u81ea\u9002\u5e94\u7b56\u7565\u3002", "result": "\u5728HumanEval\u3001MBPP\u548cAPPS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4GRPO\u57fa\u7ebf\uff0cpass@1\u5728MBPP\u4e0a\u63d0\u53474.2\u4e2a\u767e\u5206\u70b9\uff0cpass@10\u5728APPS\u4e0a\u63d0\u534712.51\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "TGPR\u4e3a\u7ed3\u5408\u5b66\u4e60\u7b56\u7565\u4e0e\u7ed3\u6784\u5316\u641c\u7d22\u65b9\u6cd5\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u662f\u589e\u5f3aLLM\u8fed\u4ee3\u7cbe\u5316\u548c\u72b6\u6001\u63a8\u7406\u80fd\u529b\u7684\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2510.06911", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06911", "abs": "https://arxiv.org/abs/2510.06911", "authors": ["Hacane Hechehouche", "Andre Antakli", "Matthias Klusch"], "title": "LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN", "comment": null, "summary": "There are many established semantic Web standards for implementing\nmulti-agent driven applications. The AJAN framework allows to engineer\nmulti-agent systems based on these standards. In particular, agent knowledge is\nrepresented in RDF/RDFS and OWL, while agent behavior models are defined with\nBehavior Trees and SPARQL to access and manipulate this knowledge. However, the\nappropriate definition of RDF/RDFS and SPARQL-based agent behaviors still\nremains a major hurdle not only for agent modelers in practice. For example,\ndealing with URIs is very error-prone regarding typos and dealing with complex\nSPARQL queries in large-scale environments requires a high learning curve. In\nthis paper, we present an integrated development environment to overcome such\nhurdles of modeling AJAN agents and at the same time to extend the user\ncommunity for AJAN by the possibility to leverage Large Language Models for\nagent engineering.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u5f00\u53d1\u73af\u5883\u6765\u89e3\u51b3AJAN\u6846\u67b6\u4e2dRDF/RDFS\u548cSPARQL\u5efa\u6a21\u7684\u56f0\u96be\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u7528\u6237\u7fa4\u4f53", "motivation": "AJAN\u6846\u67b6\u867d\u7136\u57fa\u4e8e\u8bed\u4e49Web\u6807\u51c6\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4f46RDF/RDFS\u548cSPARQL\u7684\u884c\u4e3a\u5b9a\u4e49\u5728\u5b9e\u8df5\u4e2d\u5b58\u5728\u5f88\u5927\u969c\u788d\uff0c\u5982URI\u5904\u7406\u5bb9\u6613\u51fa\u9519\u3001\u590d\u6742SPARQL\u67e5\u8be2\u5b66\u4e60\u66f2\u7ebf\u9ad8\u7b49\u95ee\u9898", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u5f00\u53d1\u73af\u5883\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u8f85\u52a9\u667a\u80fd\u4f53\u5de5\u7a0b\uff0c\u964d\u4f4e\u5efa\u6a21\u96be\u5ea6", "result": "\u8be5\u73af\u5883\u80fd\u591f\u514b\u670dAJAN\u667a\u80fd\u4f53\u5efa\u6a21\u7684\u969c\u788d\uff0c\u540c\u65f6\u6269\u5c55\u4e86AJAN\u7684\u7528\u6237\u7fa4\u4f53", "conclusion": "\u901a\u8fc7\u96c6\u6210\u5f00\u53d1\u73af\u5883\u548cLLM\u7684\u7ed3\u5408\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u8bed\u4e49Web\u6807\u51c6\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u5b9e\u8df5\u96be\u9898"}}
{"id": "2510.06953", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06953", "abs": "https://arxiv.org/abs/2510.06953", "authors": ["Minju Gwak", "Guijin Son", "Jaehyung Kim"], "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces", "comment": null, "summary": "The Uniform Information Density (UID) hypothesis suggests that effective\ncommunication maintains a stable flow of information. In this work, we revisit\nthis principle in the context of large language model (LLM) reasoning traces,\nasking whether step-level uniformity reflects reasoning quality. To this end,\nwe propose an entropy-based stepwise information density metric and introduce\ntwo complementary measures of uniformity, local and global uniformity scores.\nAcross the experiments on six different reasoning benchmarks, we find that\nstep-level uniformity not only provides a strong theoretical lens but also\nyields practical performance benefits; for example, selecting reasoning traces\nwith more uniform information density at the step-level improves accuracy by\n10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals\nthat correct reasoning traces tend to avoid sharp information density spikes,\nwhile incorrect traces exhibit irregular information bursts. These results\ndemonstrate that UID-inspired information density measures outperform\nalternative internal signals as predictors of reasoning quality. Results\nhighlight the uniformity of the information density as a robust diagnostic and\nselection criterion for building more reliable and accurate reasoning systems.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u7edf\u4e00\u4fe1\u606f\u5bc6\u5ea6\u5047\u8bf4\u5728LLM\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u6b65\u9aa4\u7ea7\u4fe1\u606f\u5bc6\u5ea6\u5747\u5300\u6027\u53ef\u9884\u6d4b\u63a8\u7406\u8d28\u91cf\uff0c\u9009\u62e9\u66f4\u5747\u5300\u7684\u63a8\u7406\u8f68\u8ff9\u80fd\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u3002", "motivation": "\u63a2\u7d22\u7edf\u4e00\u4fe1\u606f\u5bc6\u5ea6\u5047\u8bf4\u662f\u5426\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\uff0c\u9a8c\u8bc1\u6b65\u9aa4\u7ea7\u4fe1\u606f\u5bc6\u5ea6\u5747\u5300\u6027\u662f\u5426\u80fd\u53cd\u6620\u63a8\u7406\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u71b5\u7684\u9010\u6b65\u4fe1\u606f\u5bc6\u5ea6\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5f15\u5165\u5c40\u90e8\u548c\u5168\u5c40\u5747\u5300\u6027\u8bc4\u5206\u4e24\u79cd\u4e92\u8865\u7684\u5747\u5300\u6027\u8861\u91cf\u6807\u51c6\u3002", "result": "\u5728\u516d\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6b65\u9aa4\u7ea7\u5747\u5300\u6027\u4e0d\u4ec5\u63d0\u4f9b\u7406\u8bba\u89c6\u89d2\uff0c\u8fd8\u5e26\u6765\u5b9e\u9645\u6027\u80fd\u63d0\u5347\uff1a\u9009\u62e9\u4fe1\u606f\u5bc6\u5ea6\u66f4\u5747\u5300\u7684\u63a8\u7406\u8f68\u8ff9\u5728AIME2025\u4e0a\u76f8\u5bf9\u57fa\u7ebf\u63d0\u5347\u51c6\u786e\u738710-32%\u3002\u6b63\u786e\u63a8\u7406\u8f68\u8ff9\u907f\u514d\u4fe1\u606f\u5bc6\u5ea6\u5c16\u5cf0\uff0c\u9519\u8bef\u8f68\u8ff9\u5219\u663e\u793a\u4e0d\u89c4\u5219\u4fe1\u606f\u7206\u53d1\u3002", "conclusion": "UID\u542f\u53d1\u7684\u4fe1\u606f\u5bc6\u5ea6\u5ea6\u91cf\u4f18\u4e8e\u5176\u4ed6\u5185\u90e8\u4fe1\u53f7\u4f5c\u4e3a\u63a8\u7406\u8d28\u91cf\u9884\u6d4b\u6307\u6807\uff0c\u4fe1\u606f\u5bc6\u5ea6\u5747\u5300\u6027\u53ef\u4f5c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u51c6\u786e\u63a8\u7406\u7cfb\u7edf\u7684\u7a33\u5065\u8bca\u65ad\u548c\u9009\u62e9\u6807\u51c6\u3002"}}
{"id": "2510.07038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07038", "abs": "https://arxiv.org/abs/2510.07038", "authors": ["Wenxun Wu", "Yuanyang Li", "Guhan Chen", "Linyue Wang", "Hongyang Chen"], "title": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning", "comment": null, "summary": "Recent advances in large language models (LLMs) have popularized test-time\nscaling, where models generate additional reasoning tokens before producing\nfinal answers. These approaches have demonstrated significant performance\nimprovements on benchmarks involving mathematical reasoning. However, language\nmodels relying solely on direct inference still struggle with tasks demanding\nup-to-date knowledge or computational tools such as calculators and code\ninterpreters for complex arithmetic operations. To overcome these limitations,\nwe propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement\nlearning framework that systematically integrates multi-hop reasoning with\nadaptive tool-calling capabilities. Our approach employs a modified version of\nDynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,\nwhich we adapt specifically for tool invocation scenarios, enabling models to\ndynamically interleave complex reasoning with on-demand tool usage (including\nsearch APIs and Python interpreters).\n  To support this research, we introduce two new datasets: TAPO-easy-60K and\nTAPO-hard-18K, specifically designed to train and evaluate both fact-based\nreasoning and mathematical calculation capabilities. Our experiments on\nQwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,\nwith both models achieving state-of-the-art performance on tasks requiring\nexternal knowledge and mathematical computation among methods with comparable\nparameters. Notably, TAPO achieves more efficient tool utilization than\nbaseline methods while preventing excessive calls caused by reward hacking.\nThese results highlight the significant potential of combining advanced\nreasoning with tool usage to enhance model performance in knowledge-intensive\nand computationally demanding tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86TAPO\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06\u591a\u8df3\u63a8\u7406\u4e0e\u81ea\u9002\u5e94\u5de5\u5177\u8c03\u7528\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u5728\u9700\u8981\u5916\u90e8\u77e5\u8bc6\u548c\u6570\u5b66\u8ba1\u7b97\u7684\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u6700\u65b0\u77e5\u8bc6\u6216\u8ba1\u7b97\u5de5\u5177\uff08\u5982\u8ba1\u7b97\u5668\u548c\u4ee3\u7801\u89e3\u91ca\u5668\uff09\u7684\u590d\u6742\u7b97\u672f\u8fd0\u7b97\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684\u52a8\u6001\u91c7\u6837\u7b56\u7565\u4f18\u5316\uff08DAPO\uff09\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u5de5\u5177\u8c03\u7528\u573a\u666f\u8fdb\u884c\u9002\u914d\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u52a8\u6001\u4ea4\u9519\u590d\u6742\u63a8\u7406\u4e0e\u6309\u9700\u5de5\u5177\u4f7f\u7528\u3002", "result": "\u5728Qwen2.5-3B\u548cQwen2.5-7B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9700\u8981\u5916\u90e8\u77e5\u8bc6\u548c\u6570\u5b66\u8ba1\u7b97\u7684\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u5229\u7528\u5de5\u5177\uff0c\u540c\u65f6\u9632\u6b62\u56e0\u5956\u52b1\u653b\u51fb\u5bfc\u81f4\u7684\u8fc7\u5ea6\u8c03\u7528\u3002", "conclusion": "\u7ed3\u5408\u9ad8\u7ea7\u63a8\u7406\u4e0e\u5de5\u5177\u4f7f\u7528\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.07064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07064", "abs": "https://arxiv.org/abs/2510.07064", "authors": ["Manh Hung Nguyen", "Sebastian Tschiatschek", "Adish Singla"], "title": "Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations", "comment": null, "summary": "The difficulty and expense of obtaining large-scale human responses make\nLarge Language Models (LLMs) an attractive alternative and a promising proxy\nfor human behavior. However, prior work shows that LLMs often produce\nhomogeneous outputs that fail to capture the rich diversity of human\nperspectives and behaviors. Thus, rather than trying to capture this diversity\nwith a single LLM agent, we propose a novel framework to construct a set of\nagents that collectively capture the diversity of a given human population.\nEach agent is an LLM whose behavior is steered by conditioning on a small set\nof human demonstrations (task-response pairs) through in-context learning. The\ncentral challenge is therefore to select a representative set of LLM agents\nfrom the exponentially large space of possible agents. We tackle this selection\nproblem from the lens of submodular optimization. In particular, we develop\nmethods that offer different trade-offs regarding time complexity and\nperformance guarantees. Extensive experiments in crowdsourcing and educational\ndomains demonstrate that our approach constructs agents that more effectively\nrepresent human populations compared to baselines. Moreover, behavioral\nanalyses on new tasks show that these agents reproduce the behavior patterns\nand perspectives of the students and annotators they are designed to represent.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u4e00\u7ec4LLM\u4ee3\u7406\u6765\u96c6\u4f53\u6355\u6349\u4eba\u7c7b\u7fa4\u4f53\u7684\u591a\u6837\u6027\uff0c\u6bcf\u4e2a\u4ee3\u7406\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u5c11\u91cf\u4eba\u7c7b\u6f14\u793a\u4e0a\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u4f7f\u7528\u5b50\u6a21\u4f18\u5316\u65b9\u6cd5\u9009\u62e9\u4ee3\u8868\u6027\u4ee3\u7406\u96c6\u3002", "motivation": "\u7531\u4e8e\u83b7\u53d6\u5927\u89c4\u6a21\u4eba\u7c7b\u54cd\u5e94\u7684\u56f0\u96be\u548c\u6210\u672c\uff0cLLMs\u6210\u4e3a\u4eba\u7c7b\u884c\u4e3a\u7684\u66ff\u4ee3\u54c1\uff0c\u4f46\u73b0\u6709LLMs\u8f93\u51fa\u540c\u8d28\u5316\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u89c2\u70b9\u548c\u884c\u4e3a\u7684\u4e30\u5bcc\u591a\u6837\u6027\u3002", "method": "\u6784\u5efa\u591a\u4e2aLLM\u4ee3\u7406\uff0c\u6bcf\u4e2a\u4ee3\u7406\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u5c11\u91cf\u4eba\u7c7b\u6f14\u793a\uff08\u4efb\u52a1-\u54cd\u5e94\u5bf9\uff09\u4e0a\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u4f7f\u7528\u5b50\u6a21\u4f18\u5316\u65b9\u6cd5\u4ece\u6307\u6570\u7ea7\u5927\u7684\u53ef\u80fd\u4ee3\u7406\u7a7a\u95f4\u4e2d\u9009\u62e9\u4ee3\u8868\u6027\u4ee3\u7406\u96c6\u3002", "result": "\u5728\u4f17\u5305\u548c\u6559\u80b2\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6784\u5efa\u7684\u4ee3\u7406\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u4ee3\u8868\u4eba\u7c7b\u7fa4\u4f53\uff0c\u884c\u4e3a\u5206\u6790\u663e\u793a\u8fd9\u4e9b\u4ee3\u7406\u80fd\u591f\u518d\u73b0\u76ee\u6807\u5b66\u751f\u548c\u6807\u6ce8\u8005\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u89c2\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u6784\u5efa\u4e86\u80fd\u591f\u6709\u6548\u4ee3\u8868\u4eba\u7c7b\u7fa4\u4f53\u591a\u6837\u6027\u7684LLM\u4ee3\u7406\u96c6\u5408\uff0c\u5728\u4fdd\u6301\u4eba\u7c7b\u884c\u4e3a\u6a21\u5f0f\u7684\u540c\u65f6\u89e3\u51b3\u4e86LLMs\u8f93\u51fa\u540c\u8d28\u5316\u7684\u95ee\u9898\u3002"}}
{"id": "2510.07069", "categories": ["cs.AI", "I.2.4"], "pdf": "https://arxiv.org/pdf/2510.07069", "abs": "https://arxiv.org/abs/2510.07069", "authors": ["Hongbo Hu", "Yisong Wang", "Yi Huang", "Kewen Wang"], "title": "Inductive Learning for Possibilistic Logic Programs Under Stable Models", "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Possibilistic logic programs (poss-programs) under stable models are a major\nvariant of answer set programming (ASP). While its semantics (possibilistic\nstable models) and properties have been well investigated, the problem of\ninductive reasoning has not been investigated yet. This paper presents an\napproach to extracting poss-programs from a background program and examples\n(parts of intended possibilistic stable models). To this end, the notion of\ninduction tasks is first formally defined, its properties are investigated and\ntwo algorithms ilpsm and ilpsmmin for computing induction solutions are\npresented. An implementation of ilpsmmin is also provided and experimental\nresults show that when inputs are ordinary logic programs, the prototype\noutperforms a major inductive learning system for normal logic programs from\nstable models on the datasets that are randomly generated.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4ece\u80cc\u666f\u7a0b\u5e8f\u548c\u793a\u4f8b\u4e2d\u63d0\u53d6\u53ef\u80fd\u6027\u903b\u8f91\u7a0b\u5e8f\u7684\u65b9\u6cd5\uff0c\u5b9a\u4e49\u4e86\u5f52\u7eb3\u4efb\u52a1\u6982\u5ff5\uff0c\u5f00\u53d1\u4e86ilpsm\u548cilpsmmin\u7b97\u6cd5\uff0c\u5e76\u5728\u666e\u901a\u903b\u8f91\u7a0b\u5e8f\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u53ef\u80fd\u6027\u903b\u8f91\u7a0b\u5e8f\u5728\u7a33\u5b9a\u6a21\u578b\u4e0b\u7684\u5f52\u7eb3\u63a8\u7406\u95ee\u9898\u5c1a\u672a\u88ab\u7814\u7a76\uff0c\u9700\u8981\u5f00\u53d1\u4ece\u80cc\u666f\u77e5\u8bc6\u548c\u793a\u4f8b\u4e2d\u5b66\u4e60\u53ef\u80fd\u6027\u903b\u8f91\u7a0b\u5e8f\u7684\u65b9\u6cd5\u3002", "method": "\u5b9a\u4e49\u4e86\u5f52\u7eb3\u4efb\u52a1\u7684\u5f62\u5f0f\u5316\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86ilpsm\u548cilpsmmin\u4e24\u79cd\u8ba1\u7b97\u5f52\u7eb3\u89e3\u7684\u7b97\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86ilpsmmin\u7684\u539f\u578b\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u8f93\u5165\u4e3a\u666e\u901a\u903b\u8f91\u7a0b\u5e8f\u65f6\uff0c\u8be5\u539f\u578b\u5728\u968f\u673a\u751f\u6210\u7684\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4e3b\u8981\u7684\u57fa\u4e8e\u7a33\u5b9a\u6a21\u578b\u7684\u6b63\u5e38\u903b\u8f91\u7a0b\u5e8f\u5f52\u7eb3\u5b66\u4e60\u7cfb\u7edf\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u53ef\u80fd\u6027\u903b\u8f91\u7a0b\u5e8f\u7684\u5f52\u7eb3\u5b66\u4e60\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b97\u6cd5\u5728\u666e\u901a\u903b\u8f91\u7a0b\u5e8f\u60c5\u51b5\u4e0b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.07073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07073", "abs": "https://arxiv.org/abs/2510.07073", "authors": ["Andr\u00e9 Hottung", "Federico Berto", "Chuanbo Hua", "Nayeli Gast Zepeda", "Daniel Wetzel", "Michael R\u00f6mer", "Haoran Ye", "Davide Zago", "Michael Poli", "Stefano Massaroli", "Jinkyoo Park", "Kevin Tierney"], "title": "VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems", "comment": null, "summary": "Designing high-performing heuristics for vehicle routing problems (VRPs) is a\ncomplex task that requires both intuition and deep domain knowledge. Large\nlanguage model (LLM)-based code generation has recently shown promise across\nmany domains, but it still falls short of producing heuristics that rival those\ncrafted by human experts. In this paper, we propose VRPAgent, a framework that\nintegrates LLM-generated components into a metaheuristic and refines them\nthrough a novel genetic search. By using the LLM to generate problem-specific\noperators, embedded within a generic metaheuristic framework, VRPAgent keeps\ntasks manageable, guarantees correctness, and still enables the discovery of\nnovel and powerful strategies. Across multiple problems, including the\ncapacitated VRP, the VRP with time windows, and the prize-collecting VRP, our\nmethod discovers heuristic operators that outperform handcrafted methods and\nrecent learning-based approaches while requiring only a single CPU core. To our\nknowledge, \\VRPAgent is the first LLM-based paradigm to advance the\nstate-of-the-art in VRPs, highlighting a promising future for automated\nheuristics discovery.", "AI": {"tldr": "VRPAgent\u662f\u4e00\u4e2a\u5c06LLM\u751f\u6210\u7684\u7ec4\u4ef6\u96c6\u6210\u5230\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u4e2d\uff0c\u5e76\u901a\u8fc7\u9057\u4f20\u641c\u7d22\u8fdb\u884c\u4f18\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u53d1\u73b0\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u7684\u9ad8\u6027\u80fd\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "motivation": "\u8bbe\u8ba1\u9ad8\u6027\u80fd\u7684\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u542f\u53d1\u5f0f\u7b97\u6cd5\u9700\u8981\u9886\u57df\u77e5\u8bc6\u548c\u76f4\u89c9\uff0c\u800c\u73b0\u6709\u7684LLM\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u8fd8\u65e0\u6cd5\u4ea7\u751f\u80fd\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5ab2\u7f8e\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528LLM\u751f\u6210\u95ee\u9898\u7279\u5b9a\u7684\u64cd\u4f5c\u7b26\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u5230\u901a\u7528\u5143\u542f\u53d1\u5f0f\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u9057\u4f20\u641c\u7d22\u8fdb\u884c\u4f18\u5316\uff0c\u786e\u4fdd\u4efb\u52a1\u53ef\u63a7\u6027\u548c\u6b63\u786e\u6027\u3002", "result": "\u5728\u591a\u4e2a\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u53d8\u4f53\u4e0a\uff0cVRPAgent\u53d1\u73b0\u7684\u542f\u53d1\u5f0f\u64cd\u4f5c\u7b26\u4f18\u4e8e\u624b\u5de5\u65b9\u6cd5\u548c\u8fd1\u671f\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4e14\u4ec5\u9700\u5355\u4e2aCPU\u6838\u5fc3\u3002", "conclusion": "VRPAgent\u662f\u9996\u4e2a\u5728\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u4e0a\u63a8\u8fdb\u6700\u5148\u8fdb\u6280\u672f\u7684LLM\u8303\u5f0f\uff0c\u4e3a\u81ea\u52a8\u542f\u53d1\u5f0f\u53d1\u73b0\u5c55\u793a\u4e86\u6709\u524d\u666f\u7684\u672a\u6765\u3002"}}
{"id": "2510.07091", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07091", "abs": "https://arxiv.org/abs/2510.07091", "authors": ["Baixuan Xu", "Tianshi Zheng", "Zhaowei Wang", "Hong Ting Tsang", "Weiqi Wang", "Tianqing Fang", "Yangqiu Song"], "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas", "comment": "22 pages", "summary": "Enabling LLMs to effectively operate long-horizon task which requires\nlong-term planning and multiple interactions is essential for open-world\nautonomy. Conventional methods adopt planning with actions where a executable\naction list would be provided as reference. However, this action representation\nchoice would be impractical when the environment action space is combinatorial\nexploded (e.g., open-ended real world). This naturally leads to a question: As\nenvironmental action space scales, what is the optimal action representation\nfor long-horizon agents? In this paper, we systematically study the\neffectiveness of two different action representations. The first one is\nconventional planning with actions (PwA) which is predominantly adopted for its\neffectiveness on existing benchmarks. The other one is planning with schemas\n(PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ]\nto [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable\nscalability. This alternative is motivated by its alignment with human\ncognition and its compliance with environment-imposed action format\nrestriction. We propose cognitive bandwidth perspective as a conceptual\nframework to qualitatively understand the differences between these two action\nrepresentations and empirically observe a representation-choice inflection\npoint between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve\nas evidence of the need for scalable representations. We further conduct\ncontrolled experiments to study how the location of this inflection point\ninteracts with different model capacities: stronger planning proficiency shifts\nthe inflection rightward, whereas better schema instantiation shifts it\nleftward. Finally, noting the suboptimal performance of PwS agents, we provide\nan actionable guide for building more capable PwS agents for better scalable\nautonomy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u4e24\u79cd\u52a8\u4f5c\u8868\u793a\u65b9\u6cd5\uff1a\u57fa\u4e8e\u52a8\u4f5c\u7684\u89c4\u5212\uff08PwA\uff09\u548c\u57fa\u4e8e\u6a21\u5f0f\u7684\u89c4\u5212\uff08PwS\uff09\uff0c\u53d1\u73b0\u5728\u52a8\u4f5c\u7a7a\u95f4\u7ec4\u5408\u7206\u70b8\u65f6PwS\u66f4\u5177\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u8ba4\u77e5\u5e26\u5bbd\u6846\u67b6\u6765\u7406\u89e3\u8fd9\u4e24\u79cd\u8868\u793a\u7684\u533a\u522b\u3002", "motivation": "\u5f53\u73af\u5883\u52a8\u4f5c\u7a7a\u95f4\u7ec4\u5408\u7206\u70b8\u65f6\uff08\u5982\u5f00\u653e\u4e16\u754c\uff09\uff0c\u4f20\u7edf\u57fa\u4e8e\u52a8\u4f5c\u5217\u8868\u7684\u89c4\u5212\u65b9\u6cd5\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u5bfb\u627e\u6700\u4f18\u7684\u52a8\u4f5c\u8868\u793a\u65b9\u6cd5\u6765\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u957f\u89c6\u91ce\u81ea\u4e3b\u6027\u3002", "method": "\u7cfb\u7edf\u6bd4\u8f83PwA\uff08\u63d0\u4f9b\u53ef\u6267\u884c\u52a8\u4f5c\u5217\u8868\uff09\u548cPwS\uff08\u5c06\u52a8\u4f5c\u6a21\u5f0f\u5b9e\u4f8b\u5316\u4e3a\u52a8\u4f5c\u5217\u8868\uff09\u4e24\u79cd\u8868\u793a\u65b9\u6cd5\uff0c\u63d0\u51fa\u8ba4\u77e5\u5e26\u5bbd\u6846\u67b6\uff0c\u5728ALFWorld\uff08~35\u52a8\u4f5c\uff09\u548cSciWorld\uff08~500\u52a8\u4f5c\uff09\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u89c2\u5bdf\u5230\u5728ALFWorld\u548cSciWorld\u4e4b\u95f4\u5b58\u5728\u8868\u793a\u9009\u62e9\u7684\u62d0\u70b9\uff0c\u8bc1\u660e\u4e86\u53ef\u6269\u5c55\u8868\u793a\u7684\u5fc5\u8981\u6027\u3002\u66f4\u5f3a\u7684\u89c4\u5212\u80fd\u529b\u4f7f\u62d0\u70b9\u53f3\u79fb\uff0c\u66f4\u597d\u7684\u6a21\u5f0f\u5b9e\u4f8b\u5316\u4f7f\u62d0\u70b9\u5de6\u79fb\u3002", "conclusion": "PwS\u5728\u52a8\u4f5c\u7a7a\u95f4\u6269\u5c55\u65f6\u66f4\u5177\u4f18\u52bf\uff0c\u4f46\u5f53\u524dPwS\u4ee3\u7406\u6027\u80fd\u6b20\u4f73\uff0c\u9700\u8981\u6784\u5efa\u66f4\u5f3a\u5927\u7684PwS\u4ee3\u7406\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u53ef\u6269\u5c55\u81ea\u4e3b\u6027\u3002"}}
{"id": "2510.07117", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07117", "abs": "https://arxiv.org/abs/2510.07117", "authors": ["Leonardo Christov-Moore", "Arthur Juliani", "Alex Kiefer", "Nicco Reggente", "B. Scott Rousse", "Adam Safron", "Nicol'as Hinrichs", "Daniel Polani", "Antonio Damasio"], "title": "The Contingencies of Physical Embodiment Allow for Open-Endedness and Care", "comment": "15 pages, 1 figure", "summary": "Physical vulnerability and mortality are often seen as obstacles to be\navoided in the development of artificial agents, which struggle to adapt to\nopen-ended environments and provide aligned care. Meanwhile, biological\norganisms survive, thrive, and care for each other in an open-ended physical\nworld with relative ease and efficiency. Understanding the role of the\nconditions of life in this disparity can aid in developing more robust,\nadaptive, and caring artificial agents. Here we define two minimal conditions\nfor physical embodiment inspired by the existentialist phenomenology of Martin\nHeidegger: being-in-the-world (the agent is a part of the environment) and\nbeing-towards-death (unless counteracted, the agent drifts toward terminal\nstates due to the second law of thermodynamics). We propose that from these\nconditions we can obtain both a homeostatic drive - aimed at maintaining\nintegrity and avoiding death by expending energy to learn and act - and an\nintrinsic drive to continue to do so in as many ways as possible. Drawing\ninspiration from Friedrich Nietzsche's existentialist concept of will-to-power,\nwe examine how intrinsic drives to maximize control over future states, e.g.,\nempowerment, allow agents to increase the probability that they will be able to\nmeet their future homeostatic needs, thereby enhancing their capacity to\nmaintain physical integrity. We formalize these concepts within a reinforcement\nlearning framework, which enables us to examine how intrinsically driven\nembodied agents learning in open-ended multi-agent environments may cultivate\nthe capacities for open-endedness and care.ov", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u6d77\u5fb7\u683c\u5c14\u548c\u5c3c\u91c7\u7684\u5b58\u5728\u4e3b\u4e49\u54f2\u5b66\u51fa\u53d1\uff0c\u63d0\u51fa\u4e86\u7269\u7406\u5177\u8eab\u667a\u80fd\u4f53\u7684\u4e24\u4e2a\u57fa\u672c\u6761\u4ef6\uff1a\u5728\u4e16\u5b58\u5728\u548c\u5411\u6b7b\u800c\u751f\uff0c\u5e76\u63a2\u8ba8\u5982\u4f55\u4ece\u8fd9\u4e9b\u6761\u4ef6\u4e2d\u884d\u751f\u51fa\u7ef4\u6301\u751f\u547d\u7684\u5185\u5728\u9a71\u52a8\u529b\u3002", "motivation": "\u7406\u89e3\u751f\u7269\u4f53\u5728\u5f00\u653e\u73af\u5883\u4e2d\u751f\u5b58\u548c\u76f8\u4e92\u5173\u7231\u7684\u80fd\u529b\uff0c\u4ee5\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u81ea\u9002\u5e94\u548c\u5173\u7231\u7684AI\u667a\u80fd\u4f53\u3002\u7269\u7406\u8106\u5f31\u6027\u548c\u6b7b\u4ea1\u7387\u88ab\u89c6\u4e3aAI\u53d1\u5c55\u7684\u969c\u788d\uff0c\u800c\u751f\u7269\u4f53\u5374\u80fd\u8f7b\u677e\u5e94\u5bf9\u3002", "method": "\u57fa\u4e8e\u6d77\u5fb7\u683c\u5c14\u7684\u5b58\u5728\u4e3b\u4e49\u73b0\u8c61\u5b66\u5b9a\u4e49\u4e24\u4e2a\u6700\u5c0f\u6761\u4ef6\uff1a\u5728\u4e16\u5b58\u5728\uff08\u667a\u80fd\u4f53\u662f\u73af\u5883\u7684\u4e00\u90e8\u5206\uff09\u548c\u5411\u6b7b\u800c\u751f\uff08\u667a\u80fd\u4f53\u8d8b\u5411\u7ec8\u672b\u72b6\u6001\uff09\u3002\u7ed3\u5408\u5c3c\u91c7\u7684\u6743\u529b\u610f\u5fd7\u6982\u5ff5\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u5f62\u5f0f\u5316\u8fd9\u4e9b\u6982\u5ff5\u3002", "result": "\u4ece\u8fd9\u4e9b\u57fa\u672c\u6761\u4ef6\u53ef\u4ee5\u63a8\u5bfc\u51fa\u7a33\u6001\u9a71\u52a8\u529b\uff08\u7ef4\u6301\u5b8c\u6574\u6027\u3001\u907f\u514d\u6b7b\u4ea1\uff09\u548c\u5185\u5728\u9a71\u52a8\u529b\uff08\u5c3d\u53ef\u80fd\u591a\u5730\u6301\u7eed\u884c\u52a8\uff09\u3002\u901a\u8fc7\u6700\u5927\u5316\u5bf9\u672a\u6765\u72b6\u6001\u7684\u63a7\u5236\uff08\u5982\u8d4b\u6743\uff09\uff0c\u667a\u80fd\u4f53\u80fd\u591f\u589e\u5f3a\u7ef4\u6301\u7269\u7406\u5b8c\u6574\u6027\u7684\u80fd\u529b\u3002", "conclusion": "\u5185\u5728\u9a71\u52a8\u7684\u5177\u8eab\u667a\u80fd\u4f53\u5728\u5f00\u653e\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u5b66\u4e60\uff0c\u53ef\u4ee5\u57f9\u517b\u5f00\u653e\u6027\u548c\u5173\u6000\u7684\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3AI\u5bf9\u9f50\u548c\u9002\u5e94\u6027\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.07161", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07161", "abs": "https://arxiv.org/abs/2510.07161", "authors": ["Ali Norouzifar", "Humam Kourani", "Marcus Dees", "Wil van der Aalst"], "title": "Integrating Domain Knowledge into Process Discovery Using Large Language Models", "comment": "This paper is currently under review for publication in a journal", "summary": "Process discovery aims to derive process models from event logs, providing\ninsights into operational behavior and forming a foundation for conformance\nchecking and process improvement. However, models derived solely from event\ndata may not accurately reflect the real process, as event logs are often\nincomplete or affected by noise, and domain knowledge, an important\ncomplementary resource, is typically disregarded. As a result, the discovered\nmodels may lack reliability for downstream tasks. We propose an interactive\nframework that incorporates domain knowledge, expressed in natural language,\ninto the process discovery pipeline using Large Language Models (LLMs). Our\napproach leverages LLMs to extract declarative rules from textual descriptions\nprovided by domain experts. These rules are used to guide the IMr discovery\nalgorithm, which recursively constructs process models by combining insights\nfrom both the event log and the extracted rules, helping to avoid problematic\nprocess structures that contradict domain knowledge. The framework coordinates\ninteractions among the LLM, domain experts, and a set of backend services. We\npresent a fully implemented tool that supports this workflow and conduct an\nextensive evaluation of multiple LLMs and prompt engineering strategies. Our\nempirical study includes a case study based on a real-life event log with the\ninvolvement of domain experts, who assessed the usability and effectiveness of\nthe framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6d41\u7a0b\u53d1\u73b0\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u9886\u57df\u4e13\u5bb6\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u63d0\u53d6\u58f0\u660e\u6027\u89c4\u5219\uff0c\u6307\u5bfc\u6d41\u7a0b\u6a21\u578b\u53d1\u73b0\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u56e0\u5ffd\u7565\u9886\u57df\u77e5\u8bc6\u800c\u5bfc\u81f4\u6a21\u578b\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6d41\u7a0b\u53d1\u73b0\u65b9\u6cd5\u4ec5\u57fa\u4e8e\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u4f46\u4e8b\u4ef6\u65e5\u5fd7\u5f80\u5f80\u4e0d\u5b8c\u6574\u6216\u5305\u542b\u566a\u58f0\uff0c\u4e14\u5ffd\u7565\u4e86\u9886\u57df\u77e5\u8bc6\u8fd9\u4e00\u91cd\u8981\u8865\u5145\u8d44\u6e90\uff0c\u5bfc\u81f4\u53d1\u73b0\u7684\u6a21\u578b\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7f3a\u4e4f\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u9886\u57df\u4e13\u5bb6\u7684\u6587\u672c\u63cf\u8ff0\u4e2d\u63d0\u53d6\u58f0\u660e\u6027\u89c4\u5219\uff0c\u8fd9\u4e9b\u89c4\u5219\u6307\u5bfcIMr\u53d1\u73b0\u7b97\u6cd5\u9012\u5f52\u6784\u5efa\u6d41\u7a0b\u6a21\u578b\uff0c\u7ed3\u5408\u4e8b\u4ef6\u65e5\u5fd7\u548c\u63d0\u53d6\u7684\u89c4\u5219\uff0c\u907f\u514d\u4e0e\u9886\u57df\u77e5\u8bc6\u76f8\u77db\u76fe\u7684\u95ee\u9898\u6d41\u7a0b\u7ed3\u6784\u3002", "result": "\u5f00\u53d1\u4e86\u5b8c\u5168\u5b9e\u73b0\u7684\u5de5\u5177\u652f\u6301\u8be5\u5de5\u4f5c\u6d41\uff0c\u5e76\u5bf9\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u548c\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\u3002\u5b9e\u8bc1\u7814\u7a76\u5305\u62ec\u57fa\u4e8e\u771f\u5b9e\u4e8b\u4ef6\u65e5\u5fd7\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u9886\u57df\u4e13\u5bb6\u8bc4\u4f30\u4e86\u6846\u67b6\u7684\u53ef\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u4ea4\u4e92\u5f0f\u6846\u67b6\u6210\u529f\u5730\u5c06\u9886\u57df\u77e5\u8bc6\u6574\u5408\u5230\u6d41\u7a0b\u53d1\u73b0\u6d41\u7a0b\u4e2d\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u58f0\u660e\u6027\u89c4\u5219\uff0c\u63d0\u9ad8\u4e86\u6d41\u7a0b\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.07172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07172", "abs": "https://arxiv.org/abs/2510.07172", "authors": ["Tianshi Zheng", "Kelvin Kiu-Wai Tam", "Newt Hue-Nam K. Nguyen", "Baixuan Xu", "Zhaowei Wang", "Jiayang Cheng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Tianqing Fang", "Yangqiu Song", "Ginny Y. Wong", "Simon See"], "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents", "comment": "60 pages, 18 figures, 13 tables", "summary": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery.", "AI": {"tldr": "\u725b\u987f\u57fa\u51c6\u662f\u4e00\u4e2a\u5305\u542b324\u4e2a\u79d1\u5b66\u5b9a\u5f8b\u53d1\u73b0\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5143\u7269\u7406\u53d8\u6362\u89e3\u51b3\u8bc4\u4f30\u4e09\u96be\u95ee\u9898\uff0c\u5c06\u8bc4\u4f30\u4ece\u9759\u6001\u51fd\u6570\u62df\u5408\u63d0\u5347\u5230\u4ea4\u4e92\u5f0f\u6a21\u578b\u53d1\u73b0\uff0c\u63ed\u793a\u4e86\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u53d1\u73b0\u80fd\u529b\u5b58\u5728\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u79d1\u5b66\u53d1\u73b0\u57fa\u51c6\u5b58\u5728\u65b9\u6cd5\u5b66\u4e09\u96be\u95ee\u9898\uff08\u79d1\u5b66\u76f8\u5173\u6027\u3001\u53ef\u6269\u5c55\u6027\u3001\u6297\u8bb0\u5fc6\u6027\uff09\uff0c\u4e14\u8fc7\u5ea6\u7b80\u5316\u53d1\u73b0\u8fc7\u7a0b\u4e3a\u9759\u6001\u51fd\u6570\u62df\u5408\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u7684\u79d1\u5b66\u63a2\u7d22\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u5143\u7269\u7406\u53d8\u6362\uff08\u5bf9\u6807\u51c6\u5b9a\u5f8b\u7684\u7cfb\u7edf\u6027\u4fee\u6539\uff09\u751f\u6210\u5927\u91cf\u95ee\u9898\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u901a\u8fc7\u5b9e\u9a8c\u63a2\u7d22\u6a21\u62df\u590d\u6742\u7cfb\u7edf\u6765\u53d1\u73b0\u9690\u85cf\u539f\u7406\uff0c\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u6a21\u578b\u53d1\u73b0\u3002", "result": "\u524d\u6cbfLLMs\u5c55\u73b0\u51fa\u6e05\u6670\u4f46\u8106\u5f31\u7684\u53d1\u73b0\u80fd\u529b\uff1a\u8be5\u80fd\u529b\u968f\u7cfb\u7edf\u590d\u6742\u6027\u589e\u52a0\u800c\u6025\u5267\u4e0b\u964d\uff0c\u5bf9\u89c2\u6d4b\u566a\u58f0\u6781\u5ea6\u654f\u611f\uff1b\u5de5\u5177\u8f85\u52a9\u5b58\u5728\u6096\u8bba\u6548\u5e94\uff0c\u4ee3\u7801\u89e3\u91ca\u5668\u53ef\u80fd\u963b\u788d\u66f4\u6709\u80fd\u529b\u7684\u6a21\u578b\u3002", "conclusion": "\u5728\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u3001\u53ef\u6cdb\u5316\u7684\u53d1\u73b0\u4ecd\u662f\u6838\u5fc3\u6311\u6218\uff0c\u725b\u987f\u57fa\u51c6\u4e3a\u8861\u91cf\u771f\u5b9e\u8fdb\u5c55\u548c\u5f00\u53d1\u771f\u6b63\u79d1\u5b66\u53d1\u73b0\u80fd\u529b\u7684\u4e0b\u4e00\u4ee3AI\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\u3002"}}
{"id": "2510.07276", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.07276", "abs": "https://arxiv.org/abs/2510.07276", "authors": ["Pulkit Rustagi", "Kyle Hollins Wray", "Sandhya Saisubramanian"], "title": "Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences", "comment": "8 pages, 7 figures", "summary": "Many real-world scenarios require multiple agents to coordinate in shared\nenvironments, while balancing trade-offs between multiple, potentially\ncompeting objectives. Current multi-objective multi-agent path finding\n(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto\nfrontiers. They do not explicitly optimize for user-defined preferences, even\nwhen the preferences are available, and scale poorly with the number of\nobjectives. We propose a lexicographic framework for modeling MO-MAPF, along\nwith an algorithm \\textit{Lexicographic Conflict-Based Search} (LCBS) that\ndirectly computes a single solution aligned with a lexicographic preference\nover objectives. LCBS integrates a priority-aware low-level $A^*$ search with\nconflict-based search, avoiding Pareto frontier construction and enabling\nefficient planning guided by preference over objectives. We provide insights\ninto optimality and scalability, and empirically demonstrate that LCBS computes\noptimal solutions while scaling to instances with up to ten objectives -- far\nbeyond the limits of existing MO-MAPF methods. Evaluations on standard and\nrandomized MAPF benchmarks show consistently higher success rates against\nstate-of-the-art baselines, especially with increasing number of objectives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bcd\u5178\u5e8f\u7684\u591a\u76ee\u6807\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u6846\u67b6\u548cLCBS\u7b97\u6cd5\uff0c\u76f4\u63a5\u8ba1\u7b97\u7b26\u5408\u76ee\u6807\u4f18\u5148\u7ea7\u6392\u5e8f\u7684\u5355\u4e00\u89e3\uff0c\u907f\u514d\u6784\u5efaPareto\u524d\u6cbf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709MO-MAPF\u7b97\u6cd5\u901a\u5e38\u8ba1\u7b97Pareto\u524d\u6cbf\u800c\u4e0d\u4f18\u5316\u7528\u6237\u5b9a\u4e49\u504f\u597d\uff0c\u4e14\u76ee\u6807\u6570\u91cf\u589e\u52a0\u65f6\u6269\u5c55\u6027\u5dee\u3002", "method": "LCBS\u7b97\u6cd5\u7ed3\u5408\u4f18\u5148\u7ea7\u611f\u77e5\u7684\u4f4e\u5c42A*\u641c\u7d22\u548c\u57fa\u4e8e\u51b2\u7a81\u7684\u641c\u7d22\uff0c\u76f4\u63a5\u6839\u636e\u76ee\u6807\u4f18\u5148\u7ea7\u8fdb\u884c\u89c4\u5212\u3002", "result": "LCBS\u80fd\u8ba1\u7b97\u6700\u4f18\u89e3\u5e76\u6269\u5c55\u523010\u4e2a\u76ee\u6807\uff0c\u8fdc\u8d85\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u7387\u663e\u8457\u9ad8\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u3002", "conclusion": "\u8bcd\u5178\u5e8f\u6846\u67b6\u548cLCBS\u7b97\u6cd5\u4e3a\u591a\u76ee\u6807\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.07297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07297", "abs": "https://arxiv.org/abs/2510.07297", "authors": ["Henry Wang", "Sirajus Salekin", "Jake Lee", "Ross Claytor", "Shinan Zhang", "Michael Chi"], "title": "Agentic generative AI for media content discovery at the national football league", "comment": "13 pages, 7 figures, International Sports Analytics Conference and\n  Exhibition", "summary": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u5de5\u4f5c\u6d41\uff0c\u8ba9NFL\u5a92\u4f53\u7814\u7a76\u4eba\u5458\u80fd\u591f\u7528\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5386\u53f2\u6bd4\u8d5b\u7247\u6bb5\uff0c\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u5c06\u67e5\u627e\u89c6\u9891\u7684\u5e73\u5747\u65f6\u95f4\u4ece10\u5206\u949f\u7f29\u77ed\u523030\u79d2\u3002", "motivation": "\u4f20\u7edf\u7b5b\u9009\u70b9\u51fb\u754c\u9762\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u76f4\u89c2\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u65b9\u5f0f\u6765\u63d0\u9ad8\u5a92\u4f53\u7814\u7a76\u4eba\u5458\u7684\u5de5\u4f5c\u6548\u7387\uff0c\u8ba9\u4ed6\u4eec\u80fd\u4e13\u6ce8\u4e8e\u521b\u610f\u5185\u5bb9\u5236\u4f5c\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u667a\u80fd\u5de5\u4f5c\u6d41\uff0c\u5c06\u7528\u6237\u67e5\u8be2\u5206\u89e3\u4e3a\u5143\u7d20\u5e76\u8f6c\u6362\u4e3a\u5e95\u5c42\u6570\u636e\u5e93\u67e5\u8be2\u8bed\u8a00\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bed\u4e49\u7f13\u5b58\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u54cd\u5e94\u901f\u5ea6\u3002", "result": "\u7cfb\u7edf\u8fbe\u5230\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\uff0c\u5e73\u5747\u67e5\u627e\u65f6\u95f4\u4ece10\u5206\u949f\u51cf\u5c11\u523030\u79d2\uff0c\u663e\u8457\u63d0\u5347\u4e86NFL\u7684\u8fd0\u8425\u6548\u7387\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5de5\u4f5c\u6d41\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u5a92\u4f53\u5185\u5bb9\u53d1\u73b0\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u4e3a\u4f53\u80b2\u5a92\u4f53\u884c\u4e1a\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5185\u5bb9\u67e5\u8be2\u89e3\u51b3\u65b9\u6848\u3002"}}
