<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
*Zhanke Zhou,Chentao Cao,Xiao Feng,Xuan Li,Zongze Li,Xiangyu Lu,Jiangchao Yao,Weikai Huang,Linrui Xu,Tian Cheng,Guanyu Jiang,Yiming Zheng,Brando Miranda,Tongliang Liu,Sanmi Koyejo,Masashi Sugiyama,Bo Han*

Main category: cs.AI

TL;DR: AlphaApollo是一个自演化的智能推理系统，通过协调多个模型和专业工具来解决基础模型推理能力有限和测试时迭代不可靠的问题，在AIME 2024/2025评估中显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型推理能力有限和测试时迭代不可靠两个瓶颈问题，提升模型的推理能力和可靠性。

Method: 通过协调多个模型与专业工具（计算工具和检索工具）进行可验证推理，使用共享状态地图支持多轮多模型解决方案演化，包括候选方案、可执行检查和迭代反馈。

Result: 在AIME 2024/2025评估中，Qwen2.5-14B-Instruct模型平均提升5.15%，通过率提升23.34%；Llama-3.3-70B-Instruct模型平均提升8.91%，通过率提升26.67%。超过80%的工具调用成功执行。

Conclusion: AlphaApollo系统通过工具使用和协作推理，有效提升了基础模型的能力上限，在多个模型上都取得了显著的性能提升。

Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to
address two bottlenecks in foundation model (FM) reasoning-limited
model-intrinsic capacity and unreliable test-time iteration. AlphaApollo
orchestrates multiple models with professional tools to enable deliberate,
verifiable reasoning. It couples (i) a computation tool (Python with numerical
and symbolic libraries) and (ii) a retrieval tool (task-relevant external
information) to execute exact calculations and ground decisions. The system
further supports multi-round, multi-model solution evolution via a shared state
map that records candidates, executable checks, and feedback for iterative
refinement. In evaluations on AIME 2024/2025 across multiple models,
AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32
for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for
Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool
calls are successfully executed, with consistent outperformance of non-tool
baselines, thereby lifting the capability ceiling of FMs. More empirical
results and implementation details will be updated at
https://github.com/tmlr-group/AlphaApollo.

</details>


### [2] [Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization](https://arxiv.org/abs/2510.06274)
*Mohammad Mahdi Samiei Paqaleh,Arash Marioriyad,Arman Tahmasebi-Zadeh,Mohamadreza Fereydooni,Mahdi Ghaznavai,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: 提出了复杂性分布外泛化框架来定义和衡量推理能力，强调当测试实例所需的最小解决方案复杂性超过训练示例时，模型仍能保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI在模式识别任务上取得进展，但缺乏对推理能力的明确定义和度量标准，需要建立系统性框架来评估真实推理能力。

Method: 通过解决方案描述Kolmogorov复杂性和操作代理来形式化复杂性，区分复杂性OoD与长度和组合OoD，并将该视角转化为实践建议。

Result: 建立了统一学习和推理的框架，表明许多在低复杂性下可用System1处理的情况在复杂性压力下需要System2，而System2可视为对解决方案结构的泛化。

Conclusion: 复杂性OoD不能仅通过扩展数据解决，需要专门针对复杂性的架构和训练机制来推进稳健推理能力的发展。

Abstract: Recent progress has pushed AI frontiers from pattern recognition tasks toward
problems that require step by step, System2 style reasoning, especially with
large language models. Yet, unlike learning, where generalization and out of
distribution (OoD) evaluation concepts are well formalized, there is no clear,
consistent definition or metric for reasoning ability. We propose Complexity
Out of Distribution (Complexity OoD) generalization as a framework and problem
setting to define and measure reasoning. A model exhibits Complexity OoD
generalization when it maintains performance on test instances whose minimal
required solution complexity, either representational (richer solution
structure) or computational (more reasoning steps/program length), exceeds that
of all training examples. We formalize complexity via solution description
Kolmogorov complexity and operational proxies (e.g., object/relation counts;
reasoning step counts), clarifying how Complexity OoD differs from length and
compositional OoD. This lens unifies learning and reasoning: many cases
solvable with System1 like processing at low complexity become System2 like
under complexity pressure, while System2 can be viewed as generalization over
solution structures. We translate this perspective into practice with
recommendations for operationalizing Complexity OoD across the stack:
incorporating complexity into benchmark and evaluation metric design,
rethinking supervision to target solution traces, seeking and designing
inductive biases for Complexity OoD generalization, addressing learning to
reason spillovers such as spurious shortcuts, semantic robustness, catastrophic
forgetting, and step wise calibration. Because Complexity OoD cannot be solved
by scaling data alone, progress toward robust reasoning will require
architectures and training regimes that explicitly model and allocate
computation with respect to complexity.

</details>


### [3] [BuilderBench -- A benchmark for generalist agents](https://arxiv.org/abs/2510.06288)
*Raj Ghugare,Catherine Ji,Kathryn Wantlin,Jin Schofield,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: 提出了BuilderBench基准，用于评估智能体通过开放式探索学习构建结构的能力，包含42个多样化目标结构，测试物理理解、数学和长时程规划。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型主要通过模仿学习，难以解决超出数据范围的新问题。需要开发能够通过交互经验学习的智能体，但可扩展的学习机制仍是一个开放问题。

Method: BuilderBench包含硬件加速的机器人模拟器和42个目标结构任务。智能体在训练时无监督探索环境，在评估时构建未见过的目标结构。

Result: 实验表明当前算法在这些任务上仍面临挑战，因此提供了"训练轮"协议和6种算法的单文件实现作为参考。

Conclusion: BuilderBench旨在加速基于开放式探索的智能体预训练研究，需要一种体现在行动中的具身推理能力，而不仅仅是语言表达。

Abstract: Today's AI models learn primarily through mimicry and sharpening, so it is
not surprising that they struggle to solve problems beyond the limits set by
existing data. To solve novel problems, agents should acquire skills for
exploring and learning through experience. Finding a scalable learning
mechanism for developing agents that learn through interaction remains a major
open problem. In this work, we introduce BuilderBench, a benchmark to
accelerate research into agent pre-training that centers open-ended
exploration. BuilderBench requires agents to learn how to build any structure
using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated
simulator of a robotic agent interacting with various physical blocks, and
$(2)$ a task-suite with over 42 diverse target structures that are carefully
curated to test an understanding of physics, mathematics, and long-horizon
planning. During training, agents have to explore and learn general principles
about the environment without any external supervision. During evaluation,
agents have to build the unseen target structures from the task suite. Solving
these tasks requires a sort of \emph{embodied reasoning} that is not reflected
in words but rather in actions, experimenting with different strategies and
piecing them together. Our experiments show that many of these tasks challenge
the current iteration of algorithms. Hence, we also provide a ``training
wheels'' protocol, in which agents are trained and evaluated to build a single
target structure from the task suite. Finally, we provide single-file
implementations of six different algorithms as a reference point for
researchers.

</details>


### [4] [Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration](https://arxiv.org/abs/2510.06302)
*Ksenija Lace,Marite Kirikova*

Main category: cs.AI

TL;DR: 本文探讨如何通过游戏化学习设计改进并购后信息系统集成的方法培训，解决现有方法学习曲线高和动机低的问题。


<details>
  <summary>Details</summary>
Motivation: 并购后信息系统集成面临独特挑战，现有AMILI和AMILP方法在实际应用中存在学习曲线高和学习者动机低的问题，需要更有效的培训方式。

Method: 分析基础学习理论、认知负荷与动机模型、严肃游戏设计框架，识别游戏化学习设计框架的关键要求，包括转换过程和最终学习体验两个组成部分。

Result: 提出了一个专门针对并购后信息系统集成的游戏化学习设计框架，并制定了通过迭代设计和实际验证来开发和评估该框架的计划。

Conclusion: 游戏化学习设计能够将静态方法培训转变为引人入胜的学习体验，有望解决现有信息系统集成培训中的动机和学习曲线问题。

Abstract: Post-merger integration states unique challenges for professionals
responsible for information system integration aimed on alignment and
combination diverse system architectures of merging organizations. Although the
theoretical and practical guidance exists for post-merger integration on the
business level, there is a significant gap in training for information system
integration in this context. In prior research specific methods AMILI (Support
method for informed decision identification) and AMILP (Support method for
informed decision-making) were introduced for the support of information system
integration decisions in the post-merger integration. But during the practical
application was reported high learning curve and low learner motivation. This
paper explores how game-based learning design can address these limitations by
transforming static method training into engaging learning experience. The
study analyzes foundational learning theories, cognitive load and motivation
models, and serious game design frameworks to identify the essential
requirements for a game-based learning design framework tailored to information
system integration in post-merger integration. Requirements are structured in
two components: the transformation process and resulting learning experience.
The paper concludes with a plan for developing and evaluating the proposed
framework through iterative design and real-world validation.

</details>


### [5] [Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks](https://arxiv.org/abs/2510.06307)
*Wentao Deng,Jiahuan Pei,Zhiwei Xu,Zhaochun Ren,Zhumin Chen,Pengjie Ren*

Main category: cs.AI

TL;DR: 提出了BCCS框架，通过选择最优合作者和校准共识判断来解决多智能体系统中共识不稳定问题，在MATH和MMLU基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有共识寻求方法依赖投票机制，忽视了系统内部信念的矛盾，且智能体与所有其他智能体无差别协作，无法找到最优合作者，阻碍稳定共识的形成。

Method: 提出了理论框架来选择最大化共识稳定性的最优合作者，并基于此开发了BCCS框架，通过选择最优合作者和校准系统内部信念的共识判断来促进稳定共识。

Result: 在MATH和MMLU基准数据集上的实验结果显示，BCCS框架在挑战性任务上的准确率分别比现有最佳结果提高了2.23%和3.95%。

Conclusion: BCCS框架通过优化合作者选择和共识判断校准，有效解决了多智能体系统中的共识不稳定问题，显著提升了复杂NLP任务的性能。

Abstract: A multi-agent system (MAS) enhances its capacity to solve complex natural
language processing (NLP) tasks through collaboration among multiple agents,
where consensus-seeking serves as a fundamental mechanism. However, existing
consensus-seeking approaches typically rely on voting mechanisms to judge
consensus, overlooking contradictions in system-internal beliefs that
destabilize the consensus. Moreover, these methods often involve agents
updating their results through indiscriminate collaboration with every other
agent. Such uniform interaction fails to identify the optimal collaborators for
each agent, hindering the emergence of a stable consensus. To address these
challenges, we provide a theoretical framework for selecting optimal
collaborators that maximize consensus stability. Based on the theorems, we
propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate
stable consensus via selecting optimal collaborators and calibrating the
consensus judgment by system-internal beliefs. Experimental results on the MATH
and MMLU benchmark datasets demonstrate that the proposed BCCS framework
outperforms the best existing results by 2.23% and 3.95% of accuracy on
challenging tasks, respectively. Our code and data are available at
https://github.com/dengwentao99/BCCS.

</details>


### [6] [Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?](https://arxiv.org/abs/2510.06410)
*Aochong Oliver Li,Tanya Goyal*

Main category: cs.AI

TL;DR: 研究发现，当前推理大语言模型在协作推理方面存在严重缺陷：更强的模型反而更容易被误导推理轨迹干扰，且所有模型都无法有效利用协作者的正确推理步骤来解决问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索标准单模型推理训练流程是否能产生有效的协作推理能力，即模型能否评估和利用其他模型的推理过程。

Method: 提出双测试框架：可恢复性测试（从误导推理中恢复）和可引导性测试（利用更强协作者的正确推理），评估15个开源LLM（1.5B-32B），并分析蒸馏教师选择、RL使用和数据选择策略的影响。

Result: 反直觉发现：基准测试更强的LLM在干扰下更脆弱；所有模型在超出自身能力的问题上利用引导步骤的解决率低于9.2%；教师模型的不佳可恢复性行为会传递给蒸馏学生。

Conclusion: 当前现成的推理LLM在协作推理方面存在根本性局限，需要专门训练来培养原生强协作推理能力。

Abstract: Reasoning LLMs are trained to verbalize their reasoning process, yielding
strong gains on complex tasks. This transparency also opens a promising
direction: multiple reasoners can directly collaborate on each other's thinking
within a shared trajectory, yielding better inference efficiency and
exploration. A key prerequisite, however, is the ability to assess the
usefulness and build on another model's partial thinking -- we call this
off-trajectory reasoning. Our paper investigates a critical question: can
standard solo-reasoning training pipelines deliver desired off-trajectory
behaviors? We propose twin tests that capture the two extremes of the
off-trajectory spectrum, namely Recoverability, which tests whether LLMs can
backtrack from "distractions" induced by misleading reasoning traces, and
Guidability, which tests their ability to build upon correct reasoning from
stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and
reveals a counterintuitive finding -- "stronger" LLMs on benchmarks are often
more fragile under distraction. Moreover, all models tested fail to effectively
leverage guiding steps from collaborators on problems beyond their inherent
capabilities with solve rates remaining under 9.2%. Finally, we conduct control
studies to isolate the effects of three factors in post-training on these
behaviors: the choice of distillation teacher, the use of RL, and data
selection strategy. Our results provide actionable insights for training
natively strong reasoning collaborators; e.g., we find that suboptimal
recoverability behaviors of teacher models are transferred to distilled
students even if the distillation trajectories are correct. Taken together,
this work lays the groundwork for evaluating multi-model collaborations in
shared reasoning trajectories and highlights the limitations of off-the-shelf
reasoning LLMs.

</details>


### [7] [Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health](https://arxiv.org/abs/2510.06433)
*Aryan Singh Dalal,Yinglun Zhang,Duru Doğan,Atalay Mert İleri,Hande Küçük McGinty*

Main category: cs.AI

TL;DR: 该研究创建了一个知识图谱，通过语义网络标准化表示食物与健康之间的关系，重点关注食物中的黄酮类化合物含量与癌症之间的联系。


<details>
  <summary>Details</summary>
Motivation: 目前很少有研究使用标准化的机器可读格式来表示食物与健康之间的关系，这限制了有效利用这些知识的能力。

Method: 使用KNARM方法，结合USDA数据库中的食物黄酮类化合物含量数据和文献中的癌症关联信息，构建知识图谱。

Result: 成功创建了一个机器可操作的知识图谱，能够探索饮食选择与疾病管理之间的复杂相互作用。

Conclusion: 该知识图谱为研究人员提供了一个范例，未来工作将扩展图谱范围，添加更多相关数据并进行推理以发现隐藏关系。

Abstract: The focus on "food as medicine" is gaining traction in the field of health
and several studies conducted in the past few years discussed this aspect of
food in the literature. However, very little research has been done on
representing the relationship between food and health in a standardized,
machine-readable format using a semantic web that can help us leverage this
knowledge effectively. To address this gap, this study aims to create a
knowledge graph to link food and health through the knowledge graph's ability
to combine information from various platforms focusing on flavonoid contents of
food found in the USDA databases and cancer connections found in the
literature. We looked closely at these relationships using KNARM methodology
and represented them in machine-operable format. The proposed knowledge graph
serves as an example for researchers, enabling them to explore the complex
interplay between dietary choices and disease management. Future work for this
study involves expanding the scope of the knowledge graph by capturing nuances,
adding more related data, and performing inferences on the acquired knowledge
to uncover hidden relationships.

</details>


### [8] [PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles](https://arxiv.org/abs/2510.06475)
*Yitao Long,Yuru Jiang,Hongjun Liu,Yilun Zhao,Jingchen Sun,Yiqiu Shen,Chen Zhao,Arman Cohan,Dennis Shasha*

Main category: cs.AI

TL;DR: PuzzlePlex是一个评估基础模型推理和规划能力的基准测试，包含15种不同类型的谜题游戏，支持指令式和代码式两种执行方式，研究发现推理模型在指令式设置中表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究基础模型在复杂动态环境中的推理和规划能力及其可扩展性，需要开发一个全面的评估基准。

Method: 开发PuzzlePlex基准测试，包含15种不同类型的谜题游戏（确定性和随机性、单人和双人场景），实现定制化游戏策略进行比较，并开发细粒度性能指标。

Result: 推理模型在指令式设置中表现优于其他模型，而代码式执行虽然更具挑战性但提供了可扩展和高效的替代方案。

Conclusion: PuzzlePlex能够进行针对性评估，指导基础模型在推理、规划和泛化能力方面的未来改进。

Abstract: This work investigates the reasoning and planning capabilities of foundation
models and their scalability in complex, dynamic environments. We introduce
PuzzlePlex, a benchmark designed to assess these capabilities through a diverse
set of puzzles. PuzzlePlex consists of 15 types of puzzles, including
deterministic and stochastic games of varying difficulty, as well as
single-player and two-player scenarios. The PuzzlePlex framework provides a
comprehensive environment for each game, and supports extensibility to generate
more challenging instances as foundation models evolve. Additionally, we
implement customized game-playing strategies for comparison. Building on this
benchmark, we develop fine-grained metrics to measure performance and conduct
an in-depth analysis of frontier foundation models across two settings:
instruction-based and code-based. Furthermore, we systematically investigate
their scaling limits. Our findings show that reasoning models outperform others
in instruction-based settings, while code-based execution presents greater
challenges but offers a scalable and efficient alternative. PuzzlePlex enables
targeted evaluation and guides future improvements in reasoning, planning, and
generalization for foundation models.

</details>


### [9] [Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them](https://arxiv.org/abs/2510.06534)
*Jiahe Jin,Abhijay Paladugu,Chenyan Xiong*

Main category: cs.AI

TL;DR: 提出了行为引导技术，通过识别四种有益推理行为（信息验证、权威评估、自适应搜索、错误恢复）来训练更有效的智能搜索模型，在多个基准测试中实现超过35%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 智能搜索利用LLMs解释复杂用户信息需求并执行多步骤搜索过程，这对LLMs的推理和智能能力提出了独特挑战，需要研究有效的推理行为模式。

Method: 提出推理驱动的LLM管道分析成功搜索轨迹，识别四种有益推理行为，然后通过行为引导技术合成展现这些行为的轨迹，通过监督微调和强化学习训练模型。

Result: 在三个基准测试（GAIA、WebWalker、HLE）上，行为引导使Llama3.2-3B和Qwen3-1.7B相比直接RL训练获得超过35%的性能提升，且SFT数据中期望的推理行为比最终答案正确性更重要。

Conclusion: 引入的推理行为赋予模型更有效的探索能力和测试时扩展能力，为强化学习提供了坚实基础，证明了推理行为质量对最终性能的关键作用。

Abstract: Agentic search leverages large language models (LLMs) to interpret complex
user information needs and execute a multi-step process of planning, searching,
and synthesizing information to provide answers. This paradigm introduces
unique challenges for LLMs' reasoning and agentic capabilities when interacting
with retrieval systems and the broader web. In this paper, we propose a
reasoning-driven LLM-based pipeline to study effective reasoning behavior
patterns in agentic search. Using this pipeline, we analyze successful agentic
search trajectories and identify four beneficial reasoning behaviors:
Information Verification, Authority Evaluation, Adaptive Search, and Error
Recovery. Based on these findings, we propose a technique called Behavior
Priming to train more effective agentic search models. It synthesizes agentic
search trajectories that exhibit these four behaviors and integrates them into
the agentic search model through supervised fine-tuning (SFT), followed by
standard reinforcement learning (RL). Experiments on three benchmarks (GAIA,
WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in
Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models
with RL. Crucially, we demonstrate that the desired reasoning behaviors in the
SFT data, rather than the correctness of the final answer, is the critical
factor for achieving strong final performance after RL: fine-tuning on
trajectories with desirable reasoning behaviors but incorrect answers leads to
better performance than fine-tuning on trajectories with correct answers. Our
analysis further reveals the underlying mechanism: the introduced reasoning
behaviors endow models with more effective exploration (higher pass@k and
entropy) and test-time scaling (longer trajectories) capabilities, providing a
strong foundation for RL. Our code will be released as open source.

</details>


### [10] [Auto-Prompt Ensemble for LLM Judge](https://arxiv.org/abs/2510.06538)
*Jiajie Li,Huayi Zhang,Peng Lin,Jinjun Xiong,Wei Xu*

Main category: cs.AI

TL;DR: 提出Auto-Prompt Ensemble (APE)框架，通过自动学习评估维度来提升LLM评判者的可靠性


<details>
  <summary>Details</summary>
Motivation: 现有LLM评判者经常忽略关键评估维度，因为它们无法识别人类评估背后的隐含标准

Method: APE框架包含自适应学习评估维度的机制和基于置信度的集成方法，采用Collective Confidence置信度估计方法

Result: APE在Reward Bench上将GPT-4o的零样本设置下的一致性率从87.2%提升到90.5%

Conclusion: APE为LLM评判者提供了利用测试时计算的原理性方法，弥合了人类与LLM评判者之间的评估差距

Abstract: We present a novel framework that improves the reliability of LLM judges by
selectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM
judges often miss crucial evaluation dimensions because they fail to recognize
the implicit standards underlying human assessments. To address this challenge,
we propose the Auto-Prompt Ensemble (APE), an adaptive framework that
automatically learns evaluation dimensions from its failure cases. APE
incorporates a confidence-based ensemble mechanism to decide when to adopt the
judgments from additional evaluation dimensions through a novel confidence
estimation approach called Collective Confidence. Extensive experiments
demonstrate that APE improves the reliability of LLM Judge across diverse
standard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward
Bench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a
principled approach for LLM Judge to leverage test-time computation, and bridge
the evaluation gap between human and LLM judges.

</details>


### [11] [WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks](https://arxiv.org/abs/2510.06587)
*Jingbo Yang,Bairu Hou,Wei Wei,Shiyu Chang,Yujia Bao*

Main category: cs.AI

TL;DR: WebDART是一个通用框架，通过动态任务分解和持续重规划，使单个大语言模型能够处理复杂的网页任务，在WebChoreArena上比之前最先进代理提升了13.7个百分点的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代理在简单网页任务上表现良好，但在需要长视野导航、大规模信息提取和约束推理的复杂目标上仍然存在困难。

Method: WebDART框架（i）将每个目标动态分解为三个专注子任务：导航、信息提取和执行；（ii）随着新网页的揭示持续重规划分解，利用新发现的过滤器或捷径，避免冗余探索。

Result: 在WebChoreArena上，WebDART比之前最先进代理提升了13.7个百分点的成功率，在更简单的WebArena套件上表现相当，完成任务时导航步骤减少了14.7步。

Conclusion: WebDART框架通过动态任务分解和持续重规划，显著提升了单个大语言模型处理复杂网页任务的能力。

Abstract: Large language model (LLM) agents are becoming competent at straightforward
web tasks, such as opening an item page or submitting a form, but still
struggle with objectives that require long horizon navigation, large scale
information extraction, and reasoning under constraints. We present WebDART, a
general framework that enables a single LLM to handle such complex chores.
WebDART (i) dynamically decomposes each objective into three focused subtasks:
navigation, information extraction, and execution, so the model concentrates on
one skill at a time, and (ii) continuously replans the decomposition as new
webpages are revealed, taking advantage of newly discovered filters or
shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,
WebDART lifts success rates by up to 13.7 percentage points over previous SOTA
agents, while matching their performance on the easier WebArena suite and
completing tasks with up to 14.7 fewer navigation steps.

</details>


### [12] [Fine-Grained Emotion Recognition via In-Context Learning](https://arxiv.org/abs/2510.06600)
*Zhaochun Ren,Zhou Yang,Chenglong Ye,Haizhou Sun,Chao Chen,Xiaofei Zhu,Xiangwen Liao*

Main category: cs.AI

TL;DR: 本文提出EICL方法，通过引入情感相似示例和动态软标签策略来改进细粒度情感识别中的推理和决策过程，显著优于传统ICL方法。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法虽然增强了情感识别的推理过程，但忽略了决策过程。语义相似示例常引入情感差异，阻碍准确的情感表示。

Method: 提出EICL方法：引入情感相似示例，使用动态软标签策略改进查询表示，并采用两阶段排除策略从多角度评估相似性。

Result: 在多个数据集上的广泛实验表明，EICL显著优于ICL方法。

Conclusion: EICL通过改进情感表示和决策过程，有效解决了ICL在细粒度情感识别中的局限性。

Abstract: Fine-grained emotion recognition aims to identify the emotional type in
queries through reasoning and decision-making processes, playing a crucial role
in various systems. Recent methods use In-Context Learning (ICL), enhancing the
representation of queries in the reasoning process through semantically similar
examples, while further improving emotion recognition by explaining the
reasoning mechanisms. However, these methods enhance the reasoning process but
overlook the decision-making process. This paper investigates decision-making
in fine-grained emotion recognition through prototype theory. We show that ICL
relies on similarity matching between query representations and emotional
prototypes within the model, where emotion-accurate representations are
critical. However, semantically similar examples often introduce emotional
discrepancies, hindering accurate representations and causing errors. To
address this, we propose Emotion In-Context Learning (EICL), which introduces
emotionally similar examples and uses a dynamic soft-label strategy to improve
query representations in the emotion reasoning process. A two-stage exclusion
strategy is then employed to assess similarity from multiple angles, further
optimizing the decision-making process. Extensive experiments show that EICL
significantly outperforms ICL on multiple datasets.

</details>


### [13] [Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support](https://arxiv.org/abs/2510.06674)
*Cen,Zhao,Tiantian Zhang,Hanchen Su,Yufeng,Zhang,Shaowei Su,Mingzhi Xu,Yu,Liu,Wei Han,Jeremy Werner,Claire Na Cheng,Yashar Mehdad*

Main category: cs.AI

TL;DR: 提出了Agent-in-the-Loop框架，通过实时反馈循环持续改进基于LLM的客服系统，将重训练周期从数月缩短至数周。


<details>
  <summary>Details</summary>
Motivation: 传统的离线批量标注方法效率低下，需要更高效的实时反馈机制来持续改进客服系统。

Method: AITL框架整合四种关键标注类型到实时客服运营中：响应偏好对比、客服采纳与理由、知识相关性检查、缺失知识识别。

Result: 生产试点显示检索准确率显著提升（召回率+11.7%，精确率+14.8%），生成质量提升（帮助性+8.4%），客服采纳率提升（+4.5%）。

Conclusion: 将人工反馈循环直接嵌入运营工作流程能有效持续改进基于LLM的客服系统。

Abstract: We introduce an Agent-in-the-Loop (AITL) framework that implements a
continuous data flywheel for iteratively improving an LLM-based customer
support system. Unlike standard offline approaches that rely on batch
annotations, AITL integrates four key types of annotations directly into live
customer operations: (1) pairwise response preferences, (2) agent adoption and
rationales, (3) knowledge relevance checks, and (4) identification of missing
knowledge. These feedback signals seamlessly feed back into models' updates,
reducing retraining cycles from months to weeks. Our production pilot involving
US-based customer support agents demonstrated significant improvements in
retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality
(+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore
the effectiveness of embedding human feedback loops directly into operational
workflows to continuously refine LLM-based customer support system.

</details>


### [14] [Inefficiencies of Meta Agents for Agent Design](https://arxiv.org/abs/2510.06711)
*Batu El,Mert Yuksekgonul,James Zou*

Main category: cs.AI

TL;DR: 本文分析了元代理在自动化设计代理系统时面临的三个关键挑战：跨迭代学习效率低、行为多样性不足以及经济可行性有限。


<details>
  <summary>Details</summary>
Motivation: 研究元代理在自动化设计代理系统时存在的实际问题，包括学习效率、行为多样性和经济成本方面的挑战。

Method: 通过实验分析元代理的跨迭代学习策略、评估设计代理的行为多样性，以及计算自动化设计的经济成本效益。

Result: 发现简单扩展上下文的方法效果不如忽略先前设计；设计代理行为多样性低；仅在少数情况下自动化设计比人工设计更经济。

Conclusion: 当前元代理方法在大多数情况下并不经济可行，需要改进学习策略和增加行为多样性才能实现实用价值。

Abstract: Recent works began to automate the design of agentic systems using
meta-agents that propose and iteratively refine new agent architectures. In
this paper, we examine three key challenges in a common class of meta-agents.
First, we investigate how a meta-agent learns across iterations and find that
simply expanding the context with all previous agents, as proposed by previous
works, performs worse than ignoring prior designs entirely. We show that the
performance improves with an evolutionary approach. Second, although the
meta-agent designs multiple agents during training, it typically commits to a
single agent at test time. We find that the designed agents have low behavioral
diversity, limiting the potential for their complementary use. Third, we assess
when automated design is economically viable. We find that only in a few
cases--specifically, two datasets--the overall cost of designing and deploying
the agents is lower than that of human-designed agents when deployed on over
15,000 examples. In contrast, the performance gains for other datasets do not
justify the design cost, regardless of scale.

</details>


### [15] [MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2510.06742)
*Ali Sarabadani,Kheirolah Rahsepar Fard*

Main category: cs.AI

TL;DR: MultiCNKG是一个融合认知神经科学知识图谱、基因本体和疾病本体的创新框架，利用大语言模型进行实体对齐和图增强，构建连接基因机制、神经系统疾病和认知功能的统一知识图谱。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在捕捉基因、疾病和认知过程之间复杂语义联系方面存在局限，需要利用大语言模型来整合多个知识源，构建更全面的生物医学知识图谱。

Method: 整合三个关键知识源：认知神经科学知识图谱（CNKG）、基因本体（GO）和疾病本体（DO），利用GPT-4等大语言模型进行实体对齐、语义相似度计算和图增强。

Result: 构建的MultiCNKG包含6.9K个节点和11.3K条边，在精度（85.20%）、召回率（87.30%）、覆盖率（92.18%）等指标上表现优异，链接预测评估显示与基准数据集相比具有竞争力。

Conclusion: 该知识图谱推动了个性化医疗、认知障碍诊断和认知神经科学假设制定等应用，为从分子到行为领域的多层级研究提供了支持。

Abstract: The advent of large language models (LLMs) has revolutionized the integration
of knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming
limitations in traditional machine learning methods for capturing intricate
semantic links among genes, diseases, and cognitive processes. We introduce
MultiCNKG, an innovative framework that merges three key knowledge sources: the
Cognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges
across 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes
and 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)
comprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.
Leveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity
computation, and graph augmentation to create a cohesive KG that interconnects
genetic mechanisms, neurological disorders, and cognitive functions. The
resulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,
Diseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,
Associated with, Regulates), facilitating a multi-layered view from molecular
to behavioral domains. Assessments using metrics such as precision (85.20%),
recall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty
detection (40.28%), and expert validation (89.50%) affirm its robustness and
coherence. Link prediction evaluations with models like TransE (MR: 391, MRR:
0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against
benchmarks like FB15k-237 and WN18RR. This KG advances applications in
personalized medicine, cognitive disorder diagnostics, and hypothesis
formulation in cognitive neuroscience.

</details>


### [16] [Verifying Memoryless Sequential Decision-making of Large Language Models](https://arxiv.org/abs/2510.06756)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: 开发了一个用于自动验证基于LLM策略的工具，在无记忆顺序决策任务中检查安全属性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在顺序决策任务中的应用增多，需要严格验证其策略是否满足安全要求，确保系统可靠性。

Method: 给定MDP、LLM策略和PCTL安全要求，增量构建可达状态空间，将状态编码为自然语言提示，解析LLM响应为动作，用Storm模型检查器验证安全属性。

Result: 实验显示开源LLM在确定性种子下可验证，但性能通常不如深度强化学习基线。

Conclusion: 该工具为验证日益强大的LLM提供了实用基础，支持与Ollama集成和PRISM指定任务。

Abstract: We introduce a tool for rigorous and automated verification of large language
model (LLM)- based policies in memoryless sequential decision-making tasks.
Given a Markov decision process (MDP) representing the sequential
decision-making task, an LLM policy, and a safety requirement expressed as a
PCTL formula, our approach incrementally constructs only the reachable portion
of the MDP guided by the LLM's chosen actions. Each state is encoded as a
natural language prompt, the LLM's response is parsed into an action, and
reachable successor states by the policy are expanded. The resulting formal
model is checked with Storm to determine whether the policy satisfies the
specified safety property. In experiments on standard grid world benchmarks, we
show that open source LLMs accessed via Ollama can be verified when
deterministically seeded, but generally underperform deep reinforcement
learning baselines. Our tool natively integrates with Ollama and supports
PRISM-specified tasks, enabling continuous benchmarking in user-specified
sequential decision-making tasks and laying a practical foundation for formally
verifying increasingly capable LLMs.

</details>


### [17] [Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration](https://arxiv.org/abs/2510.06761)
*Zhi Zhang,Yan Liu,Zhejing Hu,Gong Chen,Sheng-hua Zhong,Jiannong Cao*

Main category: cs.AI

TL;DR: 提出了一种双循环多智能体框架（DLMA），通过领导循环（教授智能体）进化研究计划，跟随循环（博士学生智能体）执行最佳计划，实现自动化科学研究。


<details>
  <summary>Details</summary>
Motivation: 自动化端到端科学研究面临双重挑战：需要生成新颖且合理的高层计划，并在动态不确定条件下正确执行这些计划。

Method: DLMA框架包含领导循环和跟随循环。领导循环使用进化算法通过参与、改进和整合会议迭代生成和优化研究提案；跟随循环通过事前和事后会议动态调整计划执行。

Result: 在ACLAward和Laboratory基准测试中，DLMA生成的研究论文在自动化评估中达到最先进分数，显著优于强基线方法。

Conclusion: 消融研究证实两个循环都至关重要，进化驱动新颖性，执行确保合理性，DLMA成功解决了自动化科学研究的双层挑战。

Abstract: Automating the end-to-end scientific research process poses a fundamental
challenge: it requires both evolving high-level plans that are novel and sound,
and executing these plans correctly amidst dynamic and uncertain conditions. To
address this bilevel challenge, we propose a novel Double-Loop Multi-Agent
(DLMA) framework to solve the given research problem automatically. The leader
loop, composed of professor agents, is responsible for evolving research plans.
It employs an evolutionary algorithm through involvement, improvement, and
integration meetings to iteratively generate and refine a pool of research
proposals, exploring the solution space effectively. The follower loop,
composed of doctoral student agents, is responsible for executing the
best-evolved plan. It dynamically adjusts the plan during implementation via
pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is
well-supported by contextual and external observations. Extensive experiments
on benchmarks like ACLAward and Laboratory show that DLMA generates research
papers that achieve state-of-the-art scores in automated evaluation,
significantly outperforming strong baselines. Ablation studies confirm the
critical roles of both loops, with evolution driving novelty and execution
ensuring soundness.

</details>


### [18] [Autoformalizer with Tool Feedback](https://arxiv.org/abs/2510.06857)
*Qi Guo,Jianing Wang,Jianfei Zhang,Deyang Kong,Xiangzhou Huang,Xiangyu Xi,Wei Wang,Jingang Wang,Xunliang Cai,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 提出ATF方法，通过集成语法修正和一致性验证工具反馈来改进自动形式化过程，显著提升了形式化陈述的语法有效性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有形式化器在生成符合语法有效性和语义一致性的有效陈述方面仍存在困难，需要改进自动形式化的质量。

Method: 结合Lean 4编译器进行语法修正，采用多LLMs作为评判者进行一致性验证，通过工具反馈自适应优化生成陈述。训练包括冷启动阶段、专家迭代阶段和直接偏好优化。

Result: ATF显著优于一系列基线形式化模型，人类评估进一步验证其优越性能，并展现出优秀的推理扩展特性。

Conclusion: ATF方法通过工具反馈有效提升了自动形式化的质量，开源Numina-ATF数据集将促进自动形式化和自动定理证明研究的发展。

Abstract: Autoformalization addresses the scarcity of data for Automated Theorem
Proving (ATP) by translating mathematical problems from natural language into
formal statements. Efforts in recent work shift from directly prompting large
language models to training an end-to-end formalizer model from scratch,
achieving remarkable advancements. However, existing formalizer still struggles
to consistently generate valid statements that meet syntactic validity and
semantic consistency. To address this issue, we propose the Autoformalizer with
Tool Feedback (ATF), a novel approach that incorporates syntactic and
consistency information as tools into the formalization process. By integrating
Lean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge
approach for consistency validation, the model is able to adaptively refine
generated statements according to the tool feedback, enhancing both syntactic
validity and semantic consistency. The training of ATF involves a cold-start
phase on synthetic tool-calling data, an expert iteration phase to improve
formalization capabilities, and Direct Preference Optimization to alleviate
ineffective revisions. Experimental results show that ATF markedly outperforms
a range of baseline formalizer models, with its superior performance further
validated by human evaluations. Subsequent analysis reveals that ATF
demonstrates excellent inference scaling properties. Moreover, we open-source
Numina-ATF, a dataset containing 750K synthetic formal statements to facilitate
advancements in autoformalization and ATP research.

</details>


### [19] [TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs](https://arxiv.org/abs/2510.06878)
*Daria Ozerova,Ekaterina Trofimova*

Main category: cs.AI

TL;DR: TGPR是一个结合GRPO和Thompson采样的树搜索框架，通过主动探索失败和成功的精化路径来改进LLM的迭代精化能力，在代码调试任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有迭代精化方法依赖预定义启发式，面临探索-利用困境且无法根据历史精化结果自适应调整，需要更有效的搜索策略来处理庞大的精化空间。

Method: 提出TGPR框架，将GRPO与基于Thompson采样的树搜索相结合，主动探索失败和成功的精化路径，生成更密集的训练轨迹和自适应策略。

Result: 在HumanEval、MBPP和APPS基准测试中，相比GRPO基线，pass@1在MBPP上提升4.2个百分点，pass@10在APPS上提升12.51个百分点。

Conclusion: TGPR为结合学习策略与结构化搜索方法提供了原则性方法，是增强LLM迭代精化和状态推理能力的通用框架。

Abstract: Iterative refinement has been a promising paradigm to enable large language
models (LLMs) to resolve difficult reasoning and problem-solving tasks. One of
the key challenges, however, is how to effectively search through the enormous
search space of possible refinements. Existing methods typically fall back on
predefined heuristics, which are troubled by the exploration-exploitation
dilemma and cannot adapt based on past refinement outcomes. We introduce
Tree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with
a Thompson-Sampling-based tree search. TGPR explores both failed and successful
refinement paths actively, with denser training trajectories and more adaptive
policies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to
+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to
+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to
a competitive GRPO baseline. Apart from debugging code, TGPR focuses on a
principled approach to combining learned policies with structured search
methods, offering a general framework for enhancing iterative refinement and
stateful reasoning in LLMs.

</details>


### [20] [LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN](https://arxiv.org/abs/2510.06911)
*Hacane Hechehouche,Andre Antakli,Matthias Klusch*

Main category: cs.AI

TL;DR: 提出了一个集成开发环境来解决AJAN框架中RDF/RDFS和SPARQL建模的困难，并利用大型语言模型扩展用户群体


<details>
  <summary>Details</summary>
Motivation: AJAN框架虽然基于语义Web标准构建多智能体系统，但RDF/RDFS和SPARQL的行为定义在实践中存在很大障碍，如URI处理容易出错、复杂SPARQL查询学习曲线高等问题

Method: 开发了一个集成开发环境，通过利用大型语言模型来辅助智能体工程，降低建模难度

Result: 该环境能够克服AJAN智能体建模的障碍，同时扩展了AJAN的用户群体

Conclusion: 通过集成开发环境和LLM的结合，可以有效解决语义Web标准在多智能体系统建模中的实践难题

Abstract: There are many established semantic Web standards for implementing
multi-agent driven applications. The AJAN framework allows to engineer
multi-agent systems based on these standards. In particular, agent knowledge is
represented in RDF/RDFS and OWL, while agent behavior models are defined with
Behavior Trees and SPARQL to access and manipulate this knowledge. However, the
appropriate definition of RDF/RDFS and SPARQL-based agent behaviors still
remains a major hurdle not only for agent modelers in practice. For example,
dealing with URIs is very error-prone regarding typos and dealing with complex
SPARQL queries in large-scale environments requires a high learning curve. In
this paper, we present an integrated development environment to overcome such
hurdles of modeling AJAN agents and at the same time to extend the user
community for AJAN by the possibility to leverage Large Language Models for
agent engineering.

</details>


### [21] [Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.06953)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.AI

TL;DR: 本文重新审视统一信息密度假说在LLM推理轨迹中的应用，发现步骤级信息密度均匀性可预测推理质量，选择更均匀的推理轨迹能显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 探索统一信息密度假说是否适用于大语言模型推理轨迹，验证步骤级信息密度均匀性是否能反映推理质量。

Method: 提出基于熵的逐步信息密度度量方法，引入局部和全局均匀性评分两种互补的均匀性衡量标准。

Result: 在六个推理基准测试中，步骤级均匀性不仅提供理论视角，还带来实际性能提升：选择信息密度更均匀的推理轨迹在AIME2025上相对基线提升准确率10-32%。正确推理轨迹避免信息密度尖峰，错误轨迹则显示不规则信息爆发。

Conclusion: UID启发的信息密度度量优于其他内部信号作为推理质量预测指标，信息密度均匀性可作为构建更可靠准确推理系统的稳健诊断和选择标准。

Abstract: The Uniform Information Density (UID) hypothesis suggests that effective
communication maintains a stable flow of information. In this work, we revisit
this principle in the context of large language model (LLM) reasoning traces,
asking whether step-level uniformity reflects reasoning quality. To this end,
we propose an entropy-based stepwise information density metric and introduce
two complementary measures of uniformity, local and global uniformity scores.
Across the experiments on six different reasoning benchmarks, we find that
step-level uniformity not only provides a strong theoretical lens but also
yields practical performance benefits; for example, selecting reasoning traces
with more uniform information density at the step-level improves accuracy by
10-32\% relative gains over baselines at AIME2025. Our analysis further reveals
that correct reasoning traces tend to avoid sharp information density spikes,
while incorrect traces exhibit irregular information bursts. These results
demonstrate that UID-inspired information density measures outperform
alternative internal signals as predictors of reasoning quality. Results
highlight the uniformity of the information density as a robust diagnostic and
selection criterion for building more reliable and accurate reasoning systems.

</details>


### [22] [Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning](https://arxiv.org/abs/2510.07038)
*Wenxun Wu,Yuanyang Li,Guhan Chen,Linyue Wang,Hongyang Chen*

Main category: cs.AI

TL;DR: 提出了TAPO框架，通过强化学习将多跳推理与自适应工具调用能力相结合，在需要外部知识和数学计算的任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要最新知识或计算工具（如计算器和代码解释器）的复杂算术运算任务上表现不佳，需要克服这些限制。

Method: 使用改进的动态采样策略优化（DAPO）强化学习框架，专门针对工具调用场景进行适配，使模型能够动态交错复杂推理与按需工具使用。

Result: 在Qwen2.5-3B和Qwen2.5-7B模型上的实验表明，该方法在需要外部知识和数学计算的任务上实现了最先进的性能，比基线方法更有效地利用工具，同时防止因奖励攻击导致的过度调用。

Conclusion: 结合高级推理与工具使用在知识密集型和计算密集型任务中具有显著潜力，能够有效提升模型性能。

Abstract: Recent advances in large language models (LLMs) have popularized test-time
scaling, where models generate additional reasoning tokens before producing
final answers. These approaches have demonstrated significant performance
improvements on benchmarks involving mathematical reasoning. However, language
models relying solely on direct inference still struggle with tasks demanding
up-to-date knowledge or computational tools such as calculators and code
interpreters for complex arithmetic operations. To overcome these limitations,
we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement
learning framework that systematically integrates multi-hop reasoning with
adaptive tool-calling capabilities. Our approach employs a modified version of
Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,
which we adapt specifically for tool invocation scenarios, enabling models to
dynamically interleave complex reasoning with on-demand tool usage (including
search APIs and Python interpreters).
  To support this research, we introduce two new datasets: TAPO-easy-60K and
TAPO-hard-18K, specifically designed to train and evaluate both fact-based
reasoning and mathematical calculation capabilities. Our experiments on
Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,
with both models achieving state-of-the-art performance on tasks requiring
external knowledge and mathematical computation among methods with comparable
parameters. Notably, TAPO achieves more efficient tool utilization than
baseline methods while preventing excessive calls caused by reward hacking.
These results highlight the significant potential of combining advanced
reasoning with tool usage to enhance model performance in knowledge-intensive
and computationally demanding tasks.

</details>


### [23] [Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations](https://arxiv.org/abs/2510.07064)
*Manh Hung Nguyen,Sebastian Tschiatschek,Adish Singla*

Main category: cs.AI

TL;DR: 提出一个新颖框架，通过构建一组LLM代理来集体捕捉人类群体的多样性，每个代理通过上下文学习在少量人类演示上进行条件化，使用子模优化方法选择代表性代理集。


<details>
  <summary>Details</summary>
Motivation: 由于获取大规模人类响应的困难和成本，LLMs成为人类行为的替代品，但现有LLMs输出同质化，无法捕捉人类观点和行为的丰富多样性。

Method: 构建多个LLM代理，每个代理通过上下文学习在少量人类演示（任务-响应对）上进行条件化，使用子模优化方法从指数级大的可能代理空间中选择代表性代理集。

Result: 在众包和教育领域的广泛实验表明，该方法构建的代理比基线方法更有效地代表人类群体，行为分析显示这些代理能够再现目标学生和标注者的行为模式和观点。

Conclusion: 该方法成功构建了能够有效代表人类群体多样性的LLM代理集合，在保持人类行为模式的同时解决了LLMs输出同质化的问题。

Abstract: The difficulty and expense of obtaining large-scale human responses make
Large Language Models (LLMs) an attractive alternative and a promising proxy
for human behavior. However, prior work shows that LLMs often produce
homogeneous outputs that fail to capture the rich diversity of human
perspectives and behaviors. Thus, rather than trying to capture this diversity
with a single LLM agent, we propose a novel framework to construct a set of
agents that collectively capture the diversity of a given human population.
Each agent is an LLM whose behavior is steered by conditioning on a small set
of human demonstrations (task-response pairs) through in-context learning. The
central challenge is therefore to select a representative set of LLM agents
from the exponentially large space of possible agents. We tackle this selection
problem from the lens of submodular optimization. In particular, we develop
methods that offer different trade-offs regarding time complexity and
performance guarantees. Extensive experiments in crowdsourcing and educational
domains demonstrate that our approach constructs agents that more effectively
represent human populations compared to baselines. Moreover, behavioral
analyses on new tasks show that these agents reproduce the behavior patterns
and perspectives of the students and annotators they are designed to represent.

</details>


### [24] [Inductive Learning for Possibilistic Logic Programs Under Stable Models](https://arxiv.org/abs/2510.07069)
*Hongbo Hu,Yisong Wang,Yi Huang,Kewen Wang*

Main category: cs.AI

TL;DR: 本文提出了从背景程序和示例中提取可能性逻辑程序的方法，定义了归纳任务概念，开发了ilpsm和ilpsmmin算法，并在普通逻辑程序上验证了其优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 可能性逻辑程序在稳定模型下的归纳推理问题尚未被研究，需要开发从背景知识和示例中学习可能性逻辑程序的方法。

Method: 定义了归纳任务的形式化概念，提出了ilpsm和ilpsmmin两种计算归纳解的算法，并实现了ilpsmmin的原型系统。

Result: 实验结果表明，当输入为普通逻辑程序时，该原型在随机生成的数据集上优于主要的基于稳定模型的正常逻辑程序归纳学习系统。

Conclusion: 成功开发了可能性逻辑程序的归纳学习方法，验证了所提算法在普通逻辑程序情况下的有效性。

Abstract: Possibilistic logic programs (poss-programs) under stable models are a major
variant of answer set programming (ASP). While its semantics (possibilistic
stable models) and properties have been well investigated, the problem of
inductive reasoning has not been investigated yet. This paper presents an
approach to extracting poss-programs from a background program and examples
(parts of intended possibilistic stable models). To this end, the notion of
induction tasks is first formally defined, its properties are investigated and
two algorithms ilpsm and ilpsmmin for computing induction solutions are
presented. An implementation of ilpsmmin is also provided and experimental
results show that when inputs are ordinary logic programs, the prototype
outperforms a major inductive learning system for normal logic programs from
stable models on the datasets that are randomly generated.

</details>


### [25] [VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems](https://arxiv.org/abs/2510.07073)
*André Hottung,Federico Berto,Chuanbo Hua,Nayeli Gast Zepeda,Daniel Wetzel,Michael Römer,Haoran Ye,Davide Zago,Michael Poli,Stefano Massaroli,Jinkyoo Park,Kevin Tierney*

Main category: cs.AI

TL;DR: VRPAgent是一个将LLM生成的组件集成到元启发式算法中，并通过遗传搜索进行优化的框架，用于自动发现车辆路径问题的高性能启发式算法。


<details>
  <summary>Details</summary>
Motivation: 设计高性能的车辆路径问题启发式算法需要领域知识和直觉，而现有的LLM代码生成方法还无法产生能与人类专家相媲美的启发式算法。

Method: 使用LLM生成问题特定的操作符，并将其嵌入到通用元启发式框架中，通过新颖的遗传搜索进行优化，确保任务可控性和正确性。

Result: 在多个车辆路径问题变体上，VRPAgent发现的启发式操作符优于手工方法和近期基于学习的方法，且仅需单个CPU核心。

Conclusion: VRPAgent是首个在车辆路径问题上推进最先进技术的LLM范式，为自动启发式发现展示了有前景的未来。

Abstract: Designing high-performing heuristics for vehicle routing problems (VRPs) is a
complex task that requires both intuition and deep domain knowledge. Large
language model (LLM)-based code generation has recently shown promise across
many domains, but it still falls short of producing heuristics that rival those
crafted by human experts. In this paper, we propose VRPAgent, a framework that
integrates LLM-generated components into a metaheuristic and refines them
through a novel genetic search. By using the LLM to generate problem-specific
operators, embedded within a generic metaheuristic framework, VRPAgent keeps
tasks manageable, guarantees correctness, and still enables the discovery of
novel and powerful strategies. Across multiple problems, including the
capacitated VRP, the VRP with time windows, and the prize-collecting VRP, our
method discovers heuristic operators that outperform handcrafted methods and
recent learning-based approaches while requiring only a single CPU core. To our
knowledge, \VRPAgent is the first LLM-based paradigm to advance the
state-of-the-art in VRPs, highlighting a promising future for automated
heuristics discovery.

</details>


### [26] [The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas](https://arxiv.org/abs/2510.07091)
*Baixuan Xu,Tianshi Zheng,Zhaowei Wang,Hong Ting Tsang,Weiqi Wang,Tianqing Fang,Yangqiu Song*

Main category: cs.AI

TL;DR: 该论文系统研究了长视野任务中两种动作表示方法：基于动作的规划（PwA）和基于模式的规划（PwS），发现在动作空间组合爆炸时PwS更具可扩展性，并提出了认知带宽框架来理解这两种表示的区别。


<details>
  <summary>Details</summary>
Motivation: 当环境动作空间组合爆炸时（如开放世界），传统基于动作列表的规划方法变得不切实际，需要寻找最优的动作表示方法来实现可扩展的长视野自主性。

Method: 系统比较PwA（提供可执行动作列表）和PwS（将动作模式实例化为动作列表）两种表示方法，提出认知带宽框架，在ALFWorld（~35动作）和SciWorld（~500动作）上进行实证研究。

Result: 观察到在ALFWorld和SciWorld之间存在表示选择的拐点，证明了可扩展表示的必要性。更强的规划能力使拐点右移，更好的模式实例化使拐点左移。

Conclusion: PwS在动作空间扩展时更具优势，但当前PwS代理性能欠佳，需要构建更强大的PwS代理以实现更好的可扩展自主性。

Abstract: Enabling LLMs to effectively operate long-horizon task which requires
long-term planning and multiple interactions is essential for open-world
autonomy. Conventional methods adopt planning with actions where a executable
action list would be provided as reference. However, this action representation
choice would be impractical when the environment action space is combinatorial
exploded (e.g., open-ended real world). This naturally leads to a question: As
environmental action space scales, what is the optimal action representation
for long-horizon agents? In this paper, we systematically study the
effectiveness of two different action representations. The first one is
conventional planning with actions (PwA) which is predominantly adopted for its
effectiveness on existing benchmarks. The other one is planning with schemas
(PwS) which instantiate an action schema into action lists (e.g., "move [OBJ]
to [OBJ]" -> "move apple to desk") to ensure concise action space and reliable
scalability. This alternative is motivated by its alignment with human
cognition and its compliance with environment-imposed action format
restriction. We propose cognitive bandwidth perspective as a conceptual
framework to qualitatively understand the differences between these two action
representations and empirically observe a representation-choice inflection
point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve
as evidence of the need for scalable representations. We further conduct
controlled experiments to study how the location of this inflection point
interacts with different model capacities: stronger planning proficiency shifts
the inflection rightward, whereas better schema instantiation shifts it
leftward. Finally, noting the suboptimal performance of PwS agents, we provide
an actionable guide for building more capable PwS agents for better scalable
autonomy.

</details>


### [27] [The Contingencies of Physical Embodiment Allow for Open-Endedness and Care](https://arxiv.org/abs/2510.07117)
*Leonardo Christov-Moore,Arthur Juliani,Alex Kiefer,Nicco Reggente,B. Scott Rousse,Adam Safron,Nicol'as Hinrichs,Daniel Polani,Antonio Damasio*

Main category: cs.AI

TL;DR: 该论文从海德格尔和尼采的存在主义哲学出发，提出了物理具身智能体的两个基本条件：在世存在和向死而生，并探讨如何从这些条件中衍生出维持生命的内在驱动力。


<details>
  <summary>Details</summary>
Motivation: 理解生物体在开放环境中生存和相互关爱的能力，以开发更鲁棒、自适应和关爱的AI智能体。物理脆弱性和死亡率被视为AI发展的障碍，而生物体却能轻松应对。

Method: 基于海德格尔的存在主义现象学定义两个最小条件：在世存在（智能体是环境的一部分）和向死而生（智能体趋向终末状态）。结合尼采的权力意志概念，在强化学习框架中形式化这些概念。

Result: 从这些基本条件可以推导出稳态驱动力（维持完整性、避免死亡）和内在驱动力（尽可能多地持续行动）。通过最大化对未来状态的控制（如赋权），智能体能够增强维持物理完整性的能力。

Conclusion: 内在驱动的具身智能体在开放多智能体环境中学习，可以培养开放性和关怀的能力，为解决AI对齐和适应性挑战提供了新视角。

Abstract: Physical vulnerability and mortality are often seen as obstacles to be
avoided in the development of artificial agents, which struggle to adapt to
open-ended environments and provide aligned care. Meanwhile, biological
organisms survive, thrive, and care for each other in an open-ended physical
world with relative ease and efficiency. Understanding the role of the
conditions of life in this disparity can aid in developing more robust,
adaptive, and caring artificial agents. Here we define two minimal conditions
for physical embodiment inspired by the existentialist phenomenology of Martin
Heidegger: being-in-the-world (the agent is a part of the environment) and
being-towards-death (unless counteracted, the agent drifts toward terminal
states due to the second law of thermodynamics). We propose that from these
conditions we can obtain both a homeostatic drive - aimed at maintaining
integrity and avoiding death by expending energy to learn and act - and an
intrinsic drive to continue to do so in as many ways as possible. Drawing
inspiration from Friedrich Nietzsche's existentialist concept of will-to-power,
we examine how intrinsic drives to maximize control over future states, e.g.,
empowerment, allow agents to increase the probability that they will be able to
meet their future homeostatic needs, thereby enhancing their capacity to
maintain physical integrity. We formalize these concepts within a reinforcement
learning framework, which enables us to examine how intrinsically driven
embodied agents learning in open-ended multi-agent environments may cultivate
the capacities for open-endedness and care.ov

</details>


### [28] [Integrating Domain Knowledge into Process Discovery Using Large Language Models](https://arxiv.org/abs/2510.07161)
*Ali Norouzifar,Humam Kourani,Marcus Dees,Wil van der Aalst*

Main category: cs.AI

TL;DR: 提出了一个交互式流程发现框架，利用大语言模型从领域专家的自然语言描述中提取声明性规则，指导流程模型发现过程，解决传统方法因忽略领域知识而导致模型不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 传统流程发现方法仅基于事件日志，但事件日志往往不完整或包含噪声，且忽略了领域知识这一重要补充资源，导致发现的模型对下游任务缺乏可靠性。

Method: 使用大语言模型从领域专家的文本描述中提取声明性规则，这些规则指导IMr发现算法递归构建流程模型，结合事件日志和提取的规则，避免与领域知识相矛盾的问题流程结构。

Result: 开发了完全实现的工具支持该工作流，并对多种大语言模型和提示工程策略进行了广泛评估。实证研究包括基于真实事件日志的案例研究，领域专家评估了框架的可用性和有效性。

Conclusion: 该交互式框架成功地将领域知识整合到流程发现流程中，通过大语言模型提取声明性规则，提高了流程模型的可靠性和实用性。

Abstract: Process discovery aims to derive process models from event logs, providing
insights into operational behavior and forming a foundation for conformance
checking and process improvement. However, models derived solely from event
data may not accurately reflect the real process, as event logs are often
incomplete or affected by noise, and domain knowledge, an important
complementary resource, is typically disregarded. As a result, the discovered
models may lack reliability for downstream tasks. We propose an interactive
framework that incorporates domain knowledge, expressed in natural language,
into the process discovery pipeline using Large Language Models (LLMs). Our
approach leverages LLMs to extract declarative rules from textual descriptions
provided by domain experts. These rules are used to guide the IMr discovery
algorithm, which recursively constructs process models by combining insights
from both the event log and the extracted rules, helping to avoid problematic
process structures that contradict domain knowledge. The framework coordinates
interactions among the LLM, domain experts, and a set of backend services. We
present a fully implemented tool that supports this workflow and conduct an
extensive evaluation of multiple LLMs and prompt engineering strategies. Our
empirical study includes a case study based on a real-life event log with the
involvement of domain experts, who assessed the usability and effectiveness of
the framework.

</details>


### [29] [NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents](https://arxiv.org/abs/2510.07172)
*Tianshi Zheng,Kelvin Kiu-Wai Tam,Newt Hue-Nam K. Nguyen,Baixuan Xu,Zhaowei Wang,Jiayang Cheng,Hong Ting Tsang,Weiqi Wang,Jiaxin Bai,Tianqing Fang,Yangqiu Song,Ginny Y. Wong,Simon See*

Main category: cs.AI

TL;DR: 牛顿基准是一个包含324个科学定律发现任务的基准测试，通过元物理变换解决评估三难问题，将评估从静态函数拟合提升到交互式模型发现，揭示了前沿大语言模型在复杂系统中的发现能力存在脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有科学发现基准存在方法学三难问题（科学相关性、可扩展性、抗记忆性），且过度简化发现过程为静态函数拟合，无法捕捉真实的科学探索过程。

Method: 使用元物理变换（对标准定律的系统性修改）生成大量问题，要求智能体通过实验探索模拟复杂系统来发现隐藏原理，实现交互式模型发现。

Result: 前沿LLMs展现出清晰但脆弱的发现能力：该能力随系统复杂性增加而急剧下降，对观测噪声极度敏感；工具辅助存在悖论效应，代码解释器可能阻碍更有能力的模型。

Conclusion: 在复杂交互环境中实现稳健、可泛化的发现仍是核心挑战，牛顿基准为衡量真实进展和开发真正科学发现能力的下一代AI智能体提供了关键工具。

Abstract: Large language models are emerging as powerful tools for scientific law
discovery, a foundational challenge in AI-driven science. However, existing
benchmarks for this task suffer from a fundamental methodological trilemma,
forcing a trade-off between scientific relevance, scalability, and resistance
to memorization. Furthermore, they oversimplify discovery as static function
fitting, failing to capture the authentic scientific process of uncovering
embedded laws through the interactive exploration of complex model systems. To
address these critical gaps, we introduce NewtonBench, a benchmark comprising
324 scientific law discovery tasks across 12 physics domains. Our design
mitigates the evaluation trilemma by using metaphysical shifts - systematic
alterations of canonical laws - to generate a vast suite of problems that are
scalable, scientifically relevant, and memorization-resistant. Moreover, we
elevate the evaluation from static function fitting to interactive model
discovery, requiring agents to experimentally probe simulated complex systems
to uncover hidden principles. Our extensive experiment reveals a clear but
fragile capability for discovery in frontier LLMs: this ability degrades
precipitously with increasing system complexity and exhibits extreme
sensitivity to observational noise. Notably, we uncover a paradoxical effect of
tool assistance: providing a code interpreter can hinder more capable models by
inducing a premature shift from exploration to exploitation, causing them to
satisfice on suboptimal solutions. These results demonstrate that robust,
generalizable discovery in complex, interactive environments remains the core
challenge. By providing a scalable, robust, and scientifically authentic
testbed, NewtonBench offers a crucial tool for measuring true progress and
guiding the development of next-generation AI agents capable of genuine
scientific discovery.

</details>


### [30] [Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences](https://arxiv.org/abs/2510.07276)
*Pulkit Rustagi,Kyle Hollins Wray,Sandhya Saisubramanian*

Main category: cs.AI

TL;DR: 提出了一种基于词典序的多目标多智能体路径规划框架和LCBS算法，直接计算符合目标优先级排序的单一解，避免构建Pareto前沿，显著提升了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有MO-MAPF算法通常计算Pareto前沿而不优化用户定义偏好，且目标数量增加时扩展性差。

Method: LCBS算法结合优先级感知的低层A*搜索和基于冲突的搜索，直接根据目标优先级进行规划。

Result: LCBS能计算最优解并扩展到10个目标，远超现有方法，在标准基准测试中成功率显著高于最先进基线。

Conclusion: 词典序框架和LCBS算法为多目标多智能体路径规划提供了高效且可扩展的解决方案。

Abstract: Many real-world scenarios require multiple agents to coordinate in shared
environments, while balancing trade-offs between multiple, potentially
competing objectives. Current multi-objective multi-agent path finding
(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto
frontiers. They do not explicitly optimize for user-defined preferences, even
when the preferences are available, and scale poorly with the number of
objectives. We propose a lexicographic framework for modeling MO-MAPF, along
with an algorithm \textit{Lexicographic Conflict-Based Search} (LCBS) that
directly computes a single solution aligned with a lexicographic preference
over objectives. LCBS integrates a priority-aware low-level $A^*$ search with
conflict-based search, avoiding Pareto frontier construction and enabling
efficient planning guided by preference over objectives. We provide insights
into optimality and scalability, and empirically demonstrate that LCBS computes
optimal solutions while scaling to instances with up to ten objectives -- far
beyond the limits of existing MO-MAPF methods. Evaluations on standard and
randomized MAPF benchmarks show consistently higher success rates against
state-of-the-art baselines, especially with increasing number of objectives.

</details>


### [31] [Agentic generative AI for media content discovery at the national football league](https://arxiv.org/abs/2510.07297)
*Henry Wang,Sirajus Salekin,Jake Lee,Ross Claytor,Shinan Zhang,Michael Chi*

Main category: cs.AI

TL;DR: 开发了一个基于生成式AI的工作流，让NFL媒体研究人员能够用自然语言查询历史比赛片段，准确率超过95%，将查找视频的平均时间从10分钟缩短到30秒。


<details>
  <summary>Details</summary>
Motivation: 传统筛选点击界面效率低下，需要更直观的自然语言查询方式来提高媒体研究人员的工作效率，让他们能专注于创意内容制作。

Method: 采用基于生成式AI的智能工作流，将用户查询分解为元素并转换为底层数据库查询语言，通过精心设计的语义缓存提高准确性和响应速度。

Result: 系统达到超过95%的准确率，平均查找时间从10分钟减少到30秒，显著提升了NFL的运营效率。

Conclusion: 生成式AI工作流成功解决了传统媒体内容发现效率低下的问题，为体育媒体行业提供了高效的内容查询解决方案。

Abstract: Generative AI has unlocked new possibilities in content discovery and
management. Through collaboration with the National Football League (NFL), we
demonstrate how a generative-AI based workflow enables media researchers and
analysts to query relevant historical plays using natural language rather than
traditional filter-and-click interfaces. The agentic workflow takes a user
query as input, breaks it into elements, and translates them into the
underlying database query language. Accuracy and latency are further improved
through carefully designed semantic caching. The solution achieves over 95
percent accuracy and reduces the average time to find relevant videos from 10
minutes to 30 seconds, significantly increasing the NFL's operational
efficiency and allowing users to focus on producing creative content and
engaging storylines.

</details>
