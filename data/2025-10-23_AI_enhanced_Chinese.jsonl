{"id": "2510.18982", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18982", "abs": "https://arxiv.org/abs/2510.18982", "authors": ["Arpan Mukherjee", "Marcello Bullo", "Debabrota Basu", "Deniz G\u00fcnd\u00fcz"], "title": "Test-time Verification via Optimal Transport: Coverage, ROC, & Sub-optimality", "comment": null, "summary": "While test-time scaling with verification has shown promise in improving the\nperformance of large language models (LLMs), the role of the verifier and its\nimperfections remain underexplored. The effect of verification manifests\nthrough interactions of three quantities: (i) the generator's coverage, (ii)\nthe verifier's region of convergence (ROC), and (iii) the sampling algorithm's\nsub-optimality. Though recent studies capture subsets of these factors, a\nunified framework quantifying the geometry of their interplay is missing. We\nframe verifiable test-time scaling as a transport problem. This characterizes\nthe interaction of coverage, ROC, and sub-optimality, and uncovers that the\nsub-optimality--coverage curve exhibits three regimes. A transport regime --\nwhere sub-optimality increases with coverage, a policy improvement regime --\nwhere sub-optimality may decrease with coverage, depending on the verifier's\nROC, and a saturation regime -- where sub-optimality plateaus, unaffected by\ncoverage. We further propose and analyze two classes of sampling algorithms --\nsequential and batched, and examine how their computational complexities shape\nthese trade-offs. Empirical results with Qwen, Llama, and Gemma models\ncorroborate our theoretical findings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u5206\u6790\u9a8c\u8bc1\u5f0f\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u751f\u6210\u5668\u8986\u76d6\u7387\u3001\u9a8c\u8bc1\u5668\u6536\u655b\u533a\u57df\u548c\u91c7\u6837\u7b97\u6cd5\u6b21\u4f18\u6027\u4e4b\u95f4\u7684\u51e0\u4f55\u4ea4\u4e92\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u6b21\u4f18\u6027-\u8986\u76d6\u7387\u66f2\u7ebf\u7684\u4e09\u4e2a\u533a\u57df\u3002", "motivation": "\u867d\u7136\u5e26\u9a8c\u8bc1\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u9a8c\u8bc1\u5668\u7684\u4f5c\u7528\u53ca\u5176\u4e0d\u5b8c\u7f8e\u6027\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u7814\u7a76\u53ea\u6355\u6349\u4e86\u8fd9\u4e9b\u56e0\u7d20\u7684\u90e8\u5206\u5b50\u96c6\uff0c\u7f3a\u4e4f\u91cf\u5316\u5b83\u4eec\u4ea4\u4e92\u51e0\u4f55\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u5c06\u53ef\u9a8c\u8bc1\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6784\u5efa\u4e3a\u4f20\u8f93\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u6790\u751f\u6210\u5668\u8986\u76d6\u7387\u3001\u9a8c\u8bc1\u5668\u6536\u655b\u533a\u57df\u548c\u91c7\u6837\u7b97\u6cd5\u6b21\u4f18\u6027\u4e4b\u95f4\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u4e24\u7c7b\u91c7\u6837\u7b97\u6cd5\u2014\u2014\u987a\u5e8f\u91c7\u6837\u548c\u6279\u91cf\u91c7\u6837\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u6b21\u4f18\u6027-\u8986\u76d6\u7387\u66f2\u7ebf\u5b58\u5728\u4e09\u4e2a\u533a\u57df\uff1a\u4f20\u8f93\u533a\u57df\uff08\u6b21\u4f18\u6027\u968f\u8986\u76d6\u7387\u589e\u52a0\uff09\u3001\u7b56\u7565\u6539\u8fdb\u533a\u57df\uff08\u6b21\u4f18\u6027\u53ef\u80fd\u968f\u8986\u76d6\u7387\u51cf\u5c11\uff0c\u53d6\u51b3\u4e8e\u9a8c\u8bc1\u5668\u6536\u655b\u533a\u57df\uff09\u548c\u9971\u548c\u533a\u57df\uff08\u6b21\u4f18\u6027\u8d8b\u4e8e\u5e73\u7a33\uff09\u3002\u5728Qwen\u3001Llama\u548cGemma\u6a21\u578b\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u9a8c\u8bc1\u5f0f\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u7684\u5173\u952e\u56e0\u7d20\u4ea4\u4e92\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u533a\u57df\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u5bf9\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u6027\u80fd\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2510.18988", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18988", "abs": "https://arxiv.org/abs/2510.18988", "authors": ["Silas Ruhrberg Est\u00e9vez", "Nicol\u00e1s Astorga", "Mihaela van der Schaar"], "title": "Timely Clinical Diagnosis through Active Test Selection", "comment": "None", "summary": "There is growing interest in using machine learning (ML) to support clinical\ndiag- nosis, but most approaches rely on static, fully observed datasets and\nfail to reflect the sequential, resource-aware reasoning clinicians use in\npractice. Diagnosis remains complex and error prone, especially in\nhigh-pressure or resource-limited settings, underscoring the need for\nframeworks that help clinicians make timely and cost-effective decisions. We\npropose ACTMED (Adaptive Clinical Test selection via Model-based Experimental\nDesign), a diagnostic framework that integrates Bayesian Experimental Design\n(BED) with large language models (LLMs) to better emulate real-world diagnostic\nreasoning. At each step, ACTMED selects the test expected to yield the greatest\nreduction in diagnostic uncertainty for a given patient. LLMs act as flexible\nsimulators, generating plausible patient state distributions and supporting\nbelief updates without requiring structured, task-specific training data.\nClinicians can remain in the loop; reviewing test suggestions, interpreting\nintermediate outputs, and applying clinical judgment throughout. We evaluate\nACTMED on real-world datasets and show it can optimize test selection to\nimprove diagnostic accuracy, interpretability, and resource use. This\nrepresents a step to- ward transparent, adaptive, and clinician-aligned\ndiagnostic systems that generalize across settings with reduced reliance on\ndomain-specific data.", "AI": {"tldr": "ACTMED\u662f\u4e00\u4e2a\u7ed3\u5408\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u9010\u6b65\u9009\u62e9\u80fd\u6700\u5927\u7a0b\u5ea6\u51cf\u5c11\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u7684\u6d4b\u8bd5\u6765\u4f18\u5316\u8bca\u65ad\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u8bca\u65ad\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u6a21\u62df\u4e34\u5e8a\u533b\u751f\u5728\u5b9e\u8df5\u4e2d\u7684\u987a\u5e8f\u6027\u3001\u8d44\u6e90\u611f\u77e5\u63a8\u7406\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5728\u9ad8\u538b\u6216\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e2d\u9700\u8981\u66f4\u6709\u6548\u7684\u8bca\u65ad\u652f\u6301\u3002", "method": "\u5c06\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0cLLMs\u4f5c\u4e3a\u7075\u6d3b\u7684\u6a21\u62df\u5668\u751f\u6210\u60a3\u8005\u72b6\u6001\u5206\u5e03\u5e76\u652f\u6301\u4fe1\u5ff5\u66f4\u65b0\uff0c\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cACTMED\u80fd\u591f\u4f18\u5316\u6d4b\u8bd5\u9009\u62e9\uff0c\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u5b9e\u73b0\u900f\u660e\u3001\u81ea\u9002\u5e94\u4e14\u4e0e\u4e34\u5e8a\u533b\u751f\u5bf9\u9f50\u7684\u8bca\u65ad\u7cfb\u7edf\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u80fd\u591f\u8de8\u73af\u5883\u6cdb\u5316\u5e76\u51cf\u5c11\u5bf9\u9886\u57df\u7279\u5b9a\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2510.19050", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19050", "abs": "https://arxiv.org/abs/2510.19050", "authors": ["Wenqian Ye", "Guangtao Zheng", "Aidong Zhang"], "title": "Rectifying Shortcut Behaviors in Preference-based Reward Learning", "comment": "NeurIPS 2025", "summary": "In reinforcement learning from human feedback, preference-based reward models\nplay a central role in aligning large language models to human-aligned\nbehavior. However, recent studies show that these models are prone to reward\nhacking and often fail to generalize well due to over-optimization. They\nachieve high reward scores by exploiting shortcuts, that is, exploiting\nspurious features (e.g., response verbosity, agreeable tone, or sycophancy)\nthat correlate with human preference labels in the training data rather than\ngenuinely reflecting the intended objectives. In this paper, instead of probing\nthese issues one at a time, we take a broader view of the reward hacking\nproblem as shortcut behaviors and introduce a principled yet flexible approach\nto mitigate shortcut behaviors in preference-based reward learning. Inspired by\nthe invariant theory in the kernel perspective, we propose Preference-based\nReward Invariance for Shortcut Mitigation (PRISM), which learns group-invariant\nkernels with feature maps in a closed-form learning objective. Experimental\nresults in several benchmarks show that our method consistently improves the\naccuracy of the reward model on diverse out-of-distribution tasks and reduces\nthe dependency on shortcuts in downstream policy models, establishing a robust\nframework for preference-based alignment.", "AI": {"tldr": "\u63d0\u51faPRISM\u65b9\u6cd5\u89e3\u51b3\u57fa\u4e8e\u504f\u597d\u7684\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u6377\u5f84\u884c\u4e3a\u95ee\u9898\uff0c\u901a\u8fc7\u4e0d\u53d8\u6838\u5b66\u4e60\u63d0\u9ad8\u5956\u52b1\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b", "motivation": "\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u504f\u597d\u5956\u52b1\u6a21\u578b\u5bb9\u6613\u51fa\u73b0\u8fc7\u4f18\u5316\u548c\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u6a21\u578b\u4f1a\u5229\u7528\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u865a\u5047\u7279\u5f81\uff08\u5982\u56de\u7b54\u5197\u957f\u3001\u8ba8\u597d\u8bed\u6c14\u7b49\uff09\u6765\u83b7\u53d6\u9ad8\u5206\uff0c\u800c\u975e\u771f\u6b63\u53cd\u6620\u4eba\u7c7b\u610f\u56fe", "method": "\u53d7\u6838\u89c6\u89d2\u4e0d\u53d8\u7406\u8bba\u542f\u53d1\uff0c\u63d0\u51faPRISM\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ed\u5f0f\u5b66\u4e60\u76ee\u6807\u5b66\u4e60\u5177\u6709\u4e0d\u53d8\u6027\u7684\u6838\u51fd\u6570\u548c\u7279\u5f81\u6620\u5c04", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u9ad8\u5956\u52b1\u6a21\u578b\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\uff0c\u5e76\u51cf\u5c11\u4e0b\u6e38\u7b56\u7565\u6a21\u578b\u5bf9\u6377\u5f84\u7684\u4f9d\u8d56", "conclusion": "PRISM\u4e3a\u57fa\u4e8e\u504f\u597d\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u6846\u67b6\uff0c\u80fd\u6709\u6548\u7f13\u89e3\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u6377\u5f84\u884c\u4e3a\u95ee\u9898"}}
{"id": "2510.19055", "categories": ["cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.19055", "abs": "https://arxiv.org/abs/2510.19055", "authors": ["Brandon James Carone", "Iran R. Roman", "Pablo Ripoll\u00e9s"], "title": "The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS", "comment": "5 pages, 2 figures, 2 tables", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated capabilities in\naudio understanding, but current evaluations may obscure fundamental weaknesses\nin relational reasoning. We introduce the Music Understanding and Structural\nEvaluation (MUSE) Benchmark, an open-source resource with 10 tasks designed to\nprobe fundamental music perception skills. We evaluate four SOTA models (Gemini\nPro and Flash, Qwen2.5-Omni, and Audio-Flamingo 3) against a large human\nbaseline (N=200). Our results reveal a wide variance in SOTA capabilities and a\npersistent gap with human experts. While Gemini Pro succeeds on basic\nperception, Qwen and Audio Flamingo 3 perform at or near chance, exposing\nsevere perceptual deficits. Furthermore, we find Chain-of-Thought (CoT)\nprompting provides inconsistent, often detrimental results. Our work provides a\ncritical tool for evaluating invariant musical representations and driving\ndevelopment of more robust AI systems.", "AI": {"tldr": "MUSE\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u97f3\u4e50\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u97f3\u9891\u7406\u89e3\u8bc4\u4f30\u53ef\u80fd\u63a9\u76d6\u4e86\u5176\u5728\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u7684\u6839\u672c\u5f31\u70b9\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u63ed\u793a\u8fd9\u4e9b\u7f3a\u9677\u3002", "method": "\u5f00\u53d1\u4e86MUSE\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b10\u4e2a\u4efb\u52a1\u6765\u63a2\u6d4b\u57fa\u7840\u97f3\u4e50\u611f\u77e5\u6280\u80fd\uff0c\u8bc4\u4f30\u4e864\u4e2aSOTA\u6a21\u578b\uff08Gemini Pro\u548cFlash\u3001Qwen2.5-Omni\u3001Audio-Flamingo 3\uff09\u5e76\u4e0e200\u4eba\u7684\u4eba\u7c7b\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "SOTA\u6a21\u578b\u80fd\u529b\u5dee\u5f02\u5f88\u5927\uff0c\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5b58\u5728\u6301\u7eed\u5dee\u8ddd\u3002Gemini Pro\u5728\u57fa\u7840\u611f\u77e5\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46Qwen\u548cAudio Flamingo 3\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0c\u663e\u793a\u51fa\u4e25\u91cd\u7684\u611f\u77e5\u7f3a\u9677\u3002\u601d\u7ef4\u94fe\u63d0\u793a\u6548\u679c\u4e0d\u4e00\u81f4\u4e14\u901a\u5e38\u6709\u5bb3\u3002", "conclusion": "MUSE\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8bc4\u4f30\u4e0d\u53d8\u97f3\u4e50\u8868\u5f81\u548c\u5f00\u53d1\u66f4\u9c81\u68d2\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\u3002"}}
{"id": "2510.19139", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19139", "abs": "https://arxiv.org/abs/2510.19139", "authors": ["Sohyeon Jeon", "Hyung-Chul Lee"], "title": "A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist", "comment": null, "summary": "Despite the rapid expansion of Large Language Models (LLMs) in healthcare,\nthe ability of these systems to assess clinical trial reporting according to\nCONSORT standards remains unclear, particularly with respect to their cognitive\nand reasoning strategies. This study applies a behavioral and metacognitive\nanalytic approach with expert-validated data, systematically comparing two\nrepresentative LLMs under three prompt conditions. Clear differences emerged in\nhow the models approached various CONSORT items, and prompt types, including\nshifts in reasoning style, explicit uncertainty, and alternative\ninterpretations shaped response patterns. Our results highlight the current\nlimitations of these systems in clinical compliance automation and underscore\nthe importance of understanding their cognitive adaptations and strategic\nbehavior in developing more explainable and reliable medical AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6839\u636eCONSORT\u6807\u51c6\u8bc4\u4f30\u4e34\u5e8a\u8bd5\u9a8c\u62a5\u544a\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u548c\u63d0\u793a\u6761\u4ef6\u4e0b\u5b58\u5728\u8ba4\u77e5\u7b56\u7565\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u7cfb\u7edf\u5728\u4e34\u5e8a\u5408\u89c4\u81ea\u52a8\u5316\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u533b\u7597\u9886\u57df\u5feb\u901f\u6269\u5c55\uff0c\u4f46\u5176\u6839\u636eCONSORT\u6807\u51c6\u8bc4\u4f30\u4e34\u5e8a\u8bd5\u9a8c\u62a5\u544a\u7684\u80fd\u529b\u4ecd\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u5728\u8ba4\u77e5\u548c\u63a8\u7406\u7b56\u7565\u65b9\u9762\u3002", "method": "\u91c7\u7528\u884c\u4e3a\u548c\u5143\u8ba4\u77e5\u5206\u6790\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e13\u5bb6\u9a8c\u8bc1\u6570\u636e\uff0c\u5728\u4e09\u79cd\u63d0\u793a\u6761\u4ef6\u4e0b\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e24\u4e2a\u4ee3\u8868\u6027LLMs\u3002", "result": "\u6a21\u578b\u5728\u5904\u7406\u4e0d\u540cCONSORT\u9879\u76ee\u548c\u63d0\u793a\u7c7b\u578b\u65f6\u8868\u73b0\u51fa\u660e\u663e\u5dee\u5f02\uff0c\u5305\u62ec\u63a8\u7406\u98ce\u683c\u8f6c\u53d8\u3001\u660e\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u66ff\u4ee3\u89e3\u91ca\u6a21\u5f0f\u3002", "conclusion": "\u7ed3\u679c\u5f3a\u8c03\u4e86\u8fd9\u4e9b\u7cfb\u7edf\u5728\u4e34\u5e8a\u5408\u89c4\u81ea\u52a8\u5316\u4e2d\u7684\u5f53\u524d\u5c40\u9650\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u7406\u89e3\u5176\u8ba4\u77e5\u9002\u5e94\u548c\u7b56\u7565\u884c\u4e3a\u5bf9\u4e8e\u5f00\u53d1\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u9760\u7684\u533b\u7597AI\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.19176", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19176", "abs": "https://arxiv.org/abs/2510.19176", "authors": ["Yuqiao Tan", "Shizhu He", "Kang Liu", "Jun Zhao"], "title": "The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models", "comment": "Accepted by NeurIPS'25 Efficient Reasoning Workshop", "summary": "Reasoning models have demonstrated exceptional performance in tasks such as\nmathematics and logical reasoning, primarily due to their ability to engage in\nstep-by-step thinking during the reasoning process. However, this often leads\nto overthinking, resulting in unnecessary computational overhead. To address\nthis issue, Mode Selection aims to automatically decide between Long-CoT\n(Chain-of-Thought) or Short-CoT by utilizing either a Thinking or NoThinking\nmode. Simultaneously, Early Exit determines the optimal stopping point during\nthe iterative reasoning process. Both methods seek to reduce the computational\nburden. In this paper, we first identify Mode Selection as a more challenging\nvariant of the Early Exit problem, as they share similar objectives but differ\nin decision timing. While Early Exit focuses on determining the best stopping\npoint for concise reasoning at inference time, Mode Selection must make this\ndecision at the beginning of the reasoning process, relying on pre-defined fake\nthoughts without engaging in an explicit reasoning process, referred to as\nzero-step thinking. Through empirical studies on nine baselines, we observe\nthat prompt-based approaches often fail due to their limited classification\ncapabilities when provided with minimal hand-crafted information. In contrast,\napproaches that leverage internal information generally perform better across\nmost scenarios but still exhibit issues with stability. Our findings indicate\nthat existing methods relying solely on the information provided by models are\ninsufficient for effectively addressing Mode Selection in scenarios with\nlimited information, highlighting the ongoing challenges of this task. Our code\nis available at https://github.com/Trae1ounG/Zero_Step_Thinking.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u6a21\u5f0f\u9009\u62e9\u95ee\u9898\uff0c\u5c06\u5176\u8bc6\u522b\u4e3a\u65e9\u671f\u9000\u51fa\u95ee\u9898\u7684\u66f4\u5177\u6311\u6218\u6027\u53d8\u4f53\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u9650\u4fe1\u606f\u573a\u666f\u4e0b\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u6a21\u5f0f\u9009\u62e9\u95ee\u9898\u3002", "motivation": "\u63a8\u7406\u6a21\u578b\u5728\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9010\u6b65\u601d\u8003\u8fc7\u7a0b\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u601d\u8003\uff0c\u4ea7\u751f\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u4e3a\u4e86\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\uff0c\u9700\u8981\u7814\u7a76\u6a21\u5f0f\u9009\u62e9\u548c\u65e9\u671f\u9000\u51fa\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u6bd4\u8f83\u4e86\u4e5d\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u548c\u5229\u7528\u6a21\u578b\u5185\u90e8\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u6a21\u5f0f\u9009\u62e9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u7531\u4e8e\u5206\u7c7b\u80fd\u529b\u6709\u9650\u4e14\u4ec5\u4f9d\u8d56\u5c11\u91cf\u624b\u5de5\u5236\u4f5c\u4fe1\u606f\u800c\u7ecf\u5e38\u5931\u8d25\uff1b\u5229\u7528\u5185\u90e8\u4fe1\u606f\u7684\u65b9\u6cd5\u5728\u5927\u591a\u6570\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4ecd\u5b58\u5728\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "conclusion": "\u4ec5\u4f9d\u8d56\u6a21\u578b\u63d0\u4f9b\u4fe1\u606f\u7684\u73b0\u6709\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u5728\u6709\u9650\u4fe1\u606f\u573a\u666f\u4e0b\u6709\u6548\u89e3\u51b3\u6a21\u5f0f\u9009\u62e9\u95ee\u9898\uff0c\u8fd9\u51f8\u663e\u4e86\u8be5\u4efb\u52a1\u7684\u6301\u7eed\u6311\u6218\u6027\u3002"}}
{"id": "2510.19205", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19205", "abs": "https://arxiv.org/abs/2510.19205", "authors": ["Yaoyao Qian", "Yuanli Wang", "Jinda Zhang", "Yun Zong", "Meixu Chen", "Hanhan Zhou", "Jindan Huang", "Yifan Zeng", "Xinyu Hu", "Chan Hee Song", "Danqing Zhang"], "title": "WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: Multi-Turn Interactions in Large Language Models", "summary": "Current evaluation of web agents largely reduces to binary success metrics or\nconformity to a single reference trajectory, ignoring the structural diversity\npresent in benchmark datasets. We present WebGraphEval, a framework that\nabstracts trajectories from multiple agents into a unified, weighted action\ngraph. This representation is directly compatible with benchmarks such as\nWebArena, leveraging leaderboard runs and newly collected trajectories without\nmodifying environments. The framework canonically encodes actions, merges\nrecurring behaviors, and applies structural analyses including reward\npropagation and success-weighted edge statistics. Evaluations across thousands\nof trajectories from six web agents show that the graph abstraction captures\ncross-model regularities, highlights redundancy and inefficiency, and\nidentifies critical decision points overlooked by outcome-based metrics. By\nframing web interaction as graph-structured data, WebGraphEval establishes a\ngeneral methodology for multi-path, cross-agent, and efficiency-aware\nevaluation of web agents.", "AI": {"tldr": "WebGraphEval\u662f\u4e00\u4e2a\u5c06\u591a\u4e2a\u667a\u80fd\u4f53\u8f68\u8ff9\u62bd\u8c61\u4e3a\u7edf\u4e00\u52a0\u6743\u52a8\u4f5c\u56fe\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7f51\u9875\u667a\u80fd\u4f53\uff0c\u80fd\u6355\u83b7\u8de8\u6a21\u578b\u89c4\u5f8b\u6027\u5e76\u8bc6\u522b\u5173\u952e\u51b3\u7b56\u70b9\u3002", "motivation": "\u5f53\u524d\u7f51\u9875\u667a\u80fd\u4f53\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e8c\u5143\u6210\u529f\u6307\u6807\u6216\u5355\u4e00\u53c2\u8003\u8f68\u8ff9\uff0c\u5ffd\u7565\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u4e2d\u7684\u7ed3\u6784\u591a\u6837\u6027\u3002", "method": "\u5c06\u591a\u4e2a\u667a\u80fd\u4f53\u7684\u8f68\u8ff9\u62bd\u8c61\u4e3a\u7edf\u4e00\u7684\u52a0\u6743\u52a8\u4f5c\u56fe\uff0c\u8fdb\u884c\u89c4\u8303\u7f16\u7801\u3001\u5408\u5e76\u91cd\u590d\u884c\u4e3a\uff0c\u5e76\u5e94\u7528\u7ed3\u6784\u5206\u6790\u5305\u62ec\u5956\u52b1\u4f20\u64ad\u548c\u6210\u529f\u52a0\u6743\u8fb9\u7edf\u8ba1\u3002", "result": "\u5bf9\u516d\u4e2a\u7f51\u9875\u667a\u80fd\u4f53\u7684\u6570\u5343\u6761\u8f68\u8ff9\u8bc4\u4f30\u663e\u793a\uff0c\u56fe\u62bd\u8c61\u80fd\u6355\u83b7\u8de8\u6a21\u578b\u89c4\u5f8b\u6027\u3001\u7a81\u51fa\u5197\u4f59\u548c\u4f4e\u6548\uff0c\u5e76\u8bc6\u522b\u88ab\u7ed3\u679c\u6307\u6807\u5ffd\u7565\u7684\u5173\u952e\u51b3\u7b56\u70b9\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7f51\u9875\u4ea4\u4e92\u6846\u67b6\u5316\u4e3a\u56fe\u7ed3\u6784\u6570\u636e\uff0cWebGraphEval\u5efa\u7acb\u4e86\u591a\u8def\u5f84\u3001\u8de8\u667a\u80fd\u4f53\u548c\u6548\u7387\u611f\u77e5\u7684\u7f51\u9875\u667a\u80fd\u4f53\u8bc4\u4f30\u901a\u7528\u65b9\u6cd5\u3002"}}
{"id": "2510.19261", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19261", "abs": "https://arxiv.org/abs/2510.19261", "authors": ["Marianna Molinari", "Ilaria Angela Amantea", "Marinella Quaranta", "Guido Governatori"], "title": "ChatGPT Unveils Its Limits: Principles of Law Deliver Checkmate", "comment": null, "summary": "This study examines the performance of ChatGPT with an experiment in the\nlegal domain. We compare the outcome with it a baseline using regular\nexpressions (Regex), rather than focusing solely on the assessment against\nhuman performance. The study reveals that even if ChatGPT has access to the\nnecessary knowledge and competencies, it is unable to assemble them, reason\nthrough, in a way that leads to an exhaustive result. This unveils a major\nlimitation of ChatGPT. Intelligence encompasses the ability to break down\ncomplex issues and address them according to multiple required competencies,\nproviding a unified and comprehensive solution. In the legal domain, one of the\nmost crucial tasks is reading legal decisions and extracting key passages\ncondensed from principles of law (PoLs), which are then incorporated into\nsubsequent rulings by judges or defense documents by lawyers. In performing\nthis task, artificial intelligence lacks an all-encompassing understanding and\nreasoning, which makes it inherently limited. Genuine intelligence, remains a\nuniquely human trait, at least in this particular field.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6cd5\u5f8b\u9886\u57df\u5b9e\u9a8c\u8bc4\u4f30ChatGPT\u8868\u73b0\uff0c\u53d1\u73b0\u5373\u4f7f\u5177\u5907\u76f8\u5173\u77e5\u8bc6\u80fd\u529b\uff0cChatGPT\u4ecd\u65e0\u6cd5\u6574\u5408\u63a8\u7406\u5f97\u51fa\u5168\u9762\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u5176\u5728\u590d\u6742\u95ee\u9898\u5206\u89e3\u548c\u7efc\u5408\u89e3\u51b3\u65b9\u9762\u7684\u6839\u672c\u5c40\u9650\u3002", "motivation": "\u8bc4\u4f30ChatGPT\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5b9e\u9645\u6027\u80fd\uff0c\u7279\u522b\u662f\u4e0e\u6b63\u5219\u8868\u8fbe\u5f0f\u57fa\u7ebf\u5bf9\u6bd4\uff0c\u800c\u975e\u4ec5\u4e0e\u4eba\u7c7b\u8868\u73b0\u6bd4\u8f83\uff0c\u4ee5\u63ed\u793a\u5176\u771f\u6b63\u7684\u80fd\u529b\u8fb9\u754c\u3002", "method": "\u5728\u6cd5\u5f8b\u9886\u57df\u8bbe\u8ba1\u5b9e\u9a8c\uff0c\u5c06ChatGPT\u4e0e\u6b63\u5219\u8868\u8fbe\u5f0f(Regex)\u65b9\u6cd5\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\uff0c\u5206\u6790\u5176\u5728\u6cd5\u5f8b\u51b3\u7b56\u5173\u952e\u6bb5\u843d\u63d0\u53d6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "ChatGPT\u5373\u4f7f\u62e5\u6709\u5fc5\u8981\u77e5\u8bc6\u548c\u80fd\u529b\uff0c\u4e5f\u65e0\u6cd5\u6709\u6548\u6574\u5408\u63a8\u7406\u5f97\u51fa\u5168\u9762\u7ed3\u679c\uff0c\u5728\u6cd5\u5f8b\u539f\u5219\u63d0\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6839\u672c\u6027\u5c40\u9650\u3002", "conclusion": "\u771f\u6b63\u7684\u667a\u80fd\u5305\u542b\u5206\u89e3\u590d\u6742\u95ee\u9898\u5e76\u8fd0\u7528\u591a\u79cd\u80fd\u529b\u63d0\u4f9b\u7edf\u4e00\u5168\u9762\u89e3\u51b3\u65b9\u6848\u7684\u80fd\u529b\uff0c\u8fd9\u5728\u6cd5\u5f8b\u9886\u57df\u4ecd\u662f\u4eba\u7c7b\u72ec\u6709\u7684\u7279\u8d28\uff0c\u4eba\u5de5\u667a\u80fd\u76ee\u524d\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002"}}
{"id": "2510.19263", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19263", "abs": "https://arxiv.org/abs/2510.19263", "authors": ["Wachara Fungwacharakorn", "Gauvain Bourgne", "Ken Satoh"], "title": "An Argumentative Explanation Framework for Generalized Reason Model with Inconsistent Precedents", "comment": "10 pages, extended version for JURIX 2025 submission", "summary": "Precedential constraint is one foundation of case-based reasoning in AI and\nLaw. It generally assumes that the underlying set of precedents must be\nconsistent. To relax this assumption, a generalized notion of the reason model\nhas been introduced. While several argumentative explanation approaches exist\nfor reasoning with precedents based on the traditional consistent reason model,\nthere has been no corresponding argumentative explanation method developed for\nthis generalized reasoning framework accommodating inconsistent precedents. To\naddress this question, this paper examines an extension of the derivation state\nargumentation framework (DSA-framework) to explain the reasoning according to\nthe generalized notion of the reason model.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u63a8\u5bfc\u72b6\u6001\u8bba\u8bc1\u6846\u67b6(DSA-framework)\uff0c\u4e3a\u5bb9\u7eb3\u4e0d\u4e00\u81f4\u5148\u4f8b\u7684\u5e7f\u4e49\u7406\u7531\u6a21\u578b\u63d0\u4f9b\u8bba\u8bc1\u6027\u89e3\u91ca\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5148\u4f8b\u7ea6\u675f\u5047\u8bbe\u5148\u4f8b\u96c6\u5fc5\u987b\u4e00\u81f4\uff0c\u800c\u5e7f\u4e49\u7406\u7531\u6a21\u578b\u653e\u677e\u4e86\u8fd9\u4e00\u5047\u8bbe\u3002\u867d\u7136\u5df2\u6709\u57fa\u4e8e\u4f20\u7edf\u4e00\u81f4\u7406\u7531\u6a21\u578b\u7684\u8bba\u8bc1\u89e3\u91ca\u65b9\u6cd5\uff0c\u4f46\u5c1a\u672a\u6709\u9488\u5bf9\u8fd9\u79cd\u5e7f\u4e49\u63a8\u7406\u6846\u67b6\u7684\u5bf9\u5e94\u65b9\u6cd5\u3002", "method": "\u6269\u5c55\u63a8\u5bfc\u72b6\u6001\u8bba\u8bc1\u6846\u67b6(DSA-framework)\uff0c\u4ee5\u89e3\u91ca\u57fa\u4e8e\u5e7f\u4e49\u7406\u7531\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4e0d\u4e00\u81f4\u5148\u4f8b\u7684\u8bba\u8bc1\u6027\u89e3\u91ca\u6846\u67b6\u3002", "conclusion": "\u6210\u529f\u5c06DSA\u6846\u67b6\u6269\u5c55\u5230\u5e7f\u4e49\u7406\u7531\u6a21\u578b\uff0c\u4e3a\u5904\u7406\u4e0d\u4e00\u81f4\u5148\u4f8b\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bba\u8bc1\u89e3\u91ca\u65b9\u6cd5\u3002"}}
{"id": "2510.19299", "categories": ["cs.AI", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.19299", "abs": "https://arxiv.org/abs/2510.19299", "authors": ["Philipp J. Schneider", "Lin Tian", "Marian-Andrei Rizoiu"], "title": "Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties", "comment": null, "summary": "Can large language model (LLM) agents reproduce the complex social dynamics\nthat characterize human online behavior -- shaped by homophily, reciprocity,\nand social validation -- and what memory and learning mechanisms enable such\ndynamics to emerge? We present a multi-agent LLM simulation framework in which\nagents repeatedly interact, evaluate one another, and adapt their behavior\nthrough in-context learning accelerated by a coaching signal. To model human\nsocial behavior, we design behavioral reward functions that capture core\ndrivers of online engagement, including social interaction, information\nseeking, self-presentation, coordination, and emotional support. These rewards\nalign agent objectives with empirically observed user motivations, enabling the\nstudy of how network structures and group formations emerge from individual\ndecision-making. Our experiments show that coached LLM agents develop stable\ninteraction patterns and form emergent social ties, yielding network structures\nthat mirror properties of real online communities. By combining behavioral\nrewards with in-context adaptation, our framework establishes a principled\ntestbed for investigating collective dynamics in LLM populations and reveals\nhow artificial agents may approximate or diverge from human-like social\nbehavior.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7\u884c\u4e3a\u5956\u52b1\u51fd\u6570\u548c\u60c5\u5883\u5b66\u4e60\u6765\u7814\u7a76LLM\u667a\u80fd\u4f53\u662f\u5426\u80fd\u91cd\u73b0\u4eba\u7c7b\u5728\u7ebf\u793e\u4ea4\u52a8\u6001\uff0c\u5305\u62ec\u540c\u8d28\u6027\u3001\u4e92\u60e0\u6027\u548c\u793e\u4f1a\u9a8c\u8bc1\u7b49\u7279\u5f81\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u662f\u5426\u80fd\u91cd\u73b0\u4eba\u7c7b\u5728\u7ebf\u884c\u4e3a\u7684\u590d\u6742\u793e\u4ea4\u52a8\u6001\uff0c\u4ee5\u53ca\u4ec0\u4e48\u8bb0\u5fc6\u548c\u5b66\u4e60\u673a\u5236\u80fd\u4f7f\u8fd9\u79cd\u52a8\u6001\u51fa\u73b0\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u6a21\u62df\u6846\u67b6\uff0c\u667a\u80fd\u4f53\u901a\u8fc7\u60c5\u5883\u5b66\u4e60\u76f8\u4e92\u4ea4\u4e92\u3001\u8bc4\u4f30\u548c\u9002\u5e94\u884c\u4e3a\uff0c\u4f7f\u7528\u6355\u6349\u5728\u7ebf\u53c2\u4e0e\u6838\u5fc3\u9a71\u52a8\u56e0\u7d20\u7684\u884c\u4e3a\u5956\u52b1\u51fd\u6570\uff0c\u5305\u62ec\u793e\u4ea4\u4e92\u52a8\u3001\u4fe1\u606f\u5bfb\u6c42\u3001\u81ea\u6211\u5448\u73b0\u3001\u534f\u8c03\u548c\u60c5\u611f\u652f\u6301\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u6307\u5bfc\u7684LLM\u667a\u80fd\u4f53\u53d1\u5c55\u51fa\u7a33\u5b9a\u7684\u4e92\u52a8\u6a21\u5f0f\u5e76\u5f62\u6210\u6d8c\u73b0\u7684\u793e\u4ea4\u8054\u7cfb\uff0c\u4ea7\u751f\u7684\u7f51\u7edc\u7ed3\u6784\u53cd\u6620\u4e86\u771f\u5b9e\u5728\u7ebf\u793e\u533a\u7684\u7279\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7814\u7a76LLM\u7fa4\u4f53\u4e2d\u7684\u96c6\u4f53\u52a8\u6001\u5efa\u7acb\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u4eba\u5de5\u667a\u80fd\u4f53\u5982\u4f55\u8fd1\u4f3c\u6216\u504f\u79bb\u7c7b\u4eba\u793e\u4ea4\u884c\u4e3a\u3002"}}
{"id": "2510.19314", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19314", "abs": "https://arxiv.org/abs/2510.19314", "authors": ["Jinwu Hu", "Zihao Lian", "Zhiquan Wen", "Chenghao Li", "Guohao Chen", "Xutao Wen", "Bin Xiao", "Mingkui Tan"], "title": "Continual Knowledge Adaptation for Reinforcement Learning", "comment": "NeurIPS 2025", "summary": "Reinforcement Learning enables agents to learn optimal behaviors through\ninteractions with environments. However, real-world environments are typically\nnon-stationary, requiring agents to continuously adapt to new tasks and\nchanging conditions. Although Continual Reinforcement Learning facilitates\nlearning across multiple tasks, existing methods often suffer from catastrophic\nforgetting and inefficient knowledge utilization. To address these challenges,\nwe propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL),\nwhich enables the accumulation and effective utilization of historical\nknowledge. Specifically, we introduce a Continual Knowledge Adaptation\nstrategy, which involves maintaining a task-specific knowledge vector pool and\ndynamically using historical knowledge to adapt the agent to new tasks. This\nprocess mitigates catastrophic forgetting and enables efficient knowledge\ntransfer across tasks by preserving and adapting critical model parameters.\nAdditionally, we propose an Adaptive Knowledge Merging mechanism that combines\nsimilar knowledge vectors to address scalability challenges, reducing memory\nrequirements while ensuring the retention of essential knowledge. Experiments\non three benchmarks demonstrate that the proposed CKA-RL outperforms\nstate-of-the-art methods, achieving an improvement of 4.20% in overall\nperformance and 8.02% in forward transfer. The source code is available at\nhttps://github.com/Fhujinwu/CKA-RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86CKA-RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6301\u7eed\u77e5\u8bc6\u9002\u5e94\u7b56\u7565\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4f7f\u7528\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u5411\u91cf\u6c60\u548c\u81ea\u9002\u5e94\u77e5\u8bc6\u5408\u5e76\u673a\u5236\u6765\u63d0\u5347\u77e5\u8bc6\u5229\u7528\u6548\u7387\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u901a\u5e38\u662f\u975e\u5e73\u7a33\u7684\uff0c\u9700\u8981\u667a\u80fd\u4f53\u6301\u7eed\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u53d8\u5316\u6761\u4ef6\u3002\u73b0\u6709\u7684\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u77e5\u8bc6\u5229\u7528\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6301\u7eed\u77e5\u8bc6\u9002\u5e94\u7b56\u7565\uff0c\u7ef4\u62a4\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u5411\u91cf\u6c60\uff0c\u52a8\u6001\u4f7f\u7528\u5386\u53f2\u77e5\u8bc6\u9002\u5e94\u65b0\u4efb\u52a1\uff1b\u5f15\u5165\u81ea\u9002\u5e94\u77e5\u8bc6\u5408\u5e76\u673a\u5236\uff0c\u5408\u5e76\u76f8\u4f3c\u77e5\u8bc6\u5411\u91cf\u4ee5\u51cf\u5c11\u5185\u5b58\u9700\u6c42\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u6574\u4f53\u6027\u80fd\u63d0\u53474.20%\uff0c\u524d\u5411\u8fc1\u79fb\u63d0\u53478.02%\u3002", "conclusion": "CKA-RL\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5b9e\u73b0\u8de8\u4efb\u52a1\u7684\u9ad8\u6548\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5728\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.19423", "categories": ["cs.AI", "I.2.0; I.2.1; I.2.4"], "pdf": "https://arxiv.org/pdf/2510.19423", "abs": "https://arxiv.org/abs/2510.19423", "authors": ["Jia-Kai Dong", "I-Wei Huang", "Chun-Tin Wu", "Yi-Tien Tsai"], "title": "MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration", "comment": "under ACL Rolling Review 2025", "summary": "We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop,\nend-to-end tool orchestration by LLM agents in a hierarchical Model-Context\nProtocol (MCP) ecosystem. Existing benchmarks often evaluate tools in\nisolation, ignoring challenges such as functional overlap and cross-server\norchestration, leading to overly optimistic assessments. MSC-Bench addresses\nthese gaps by constructing ground truth through 'equal function sets', allowing\nobjective metrics such as F1 score and reducing the dependency on\nLLM-as-a-judge evaluation. Organized as a five-level curriculum, it\nsystematically tests agent capabilities from single-tool orchestration to\ncomplex cross-server planning, and robustness to out-of-scope requests.\nExperiments reveal that rigid hierarchies can hinder performance without\nco-designed strategies, and even state-of-the-art agents exhibit systemic\nweaknesses in robustness. MSC-Bench provides a diagnostic framework to expose\nthese limitations and guide the development of more capable and efficient\ntool-using agents. The benchmark and resources are publicly available at\nhttps://github.com/snooow1029/MSC_Bench.", "AI": {"tldr": "MSC-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u591a\u8df3\u3001\u7aef\u5230\u7aef\u5de5\u5177\u7f16\u6392\u80fd\u529b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6784\u5efa\"\u7b49\u51fd\u6570\u96c6\"\u6765\u5efa\u7acb\u771f\u5b9e\u57fa\u51c6\uff0c\u51cf\u5c11\u5bf9LLM\u4f5c\u4e3a\u8bc4\u5224\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u5b64\u7acb\u8bc4\u4f30\u5de5\u5177\uff0c\u5ffd\u7565\u4e86\u529f\u80fd\u91cd\u53e0\u548c\u8de8\u670d\u52a1\u5668\u7f16\u6392\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u8bc4\u4f30\u8fc7\u4e8e\u4e50\u89c2\u3002", "method": "\u91c7\u7528\u4e94\u7ea7\u8bfe\u7a0b\u8bbe\u8ba1\uff0c\u4ece\u5355\u5de5\u5177\u7f16\u6392\u5230\u590d\u6742\u8de8\u670d\u52a1\u5668\u89c4\u5212\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u7b49\u51fd\u6570\u96c6\u6784\u5efa\u771f\u5b9e\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u50f5\u5316\u7684\u5c42\u6b21\u7ed3\u6784\u4f1a\u963b\u788d\u6027\u80fd\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u5728\u9c81\u68d2\u6027\u65b9\u9762\u4e5f\u5b58\u5728\u7cfb\u7edf\u6027\u5f31\u70b9\u3002", "conclusion": "MSC-Bench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u3001\u9ad8\u6548\u7684\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2510.19429", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19429", "abs": "https://arxiv.org/abs/2510.19429", "authors": ["Wonje Choi", "Jooyoung Kim", "Honguk Woo"], "title": "NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning", "comment": "Accepted at NeurIPS 2025", "summary": "We address the challenge of adopting language models (LMs) for embodied tasks\nin dynamic environments, where online access to large-scale inference engines\nor symbolic planners is constrained due to latency, connectivity, and resource\nlimitations. To this end, we present NeSyPr, a novel embodied reasoning\nframework that compiles knowledge via neurosymbolic proceduralization, thereby\nequipping LM-based agents with structured, adaptive, and timely reasoning\ncapabilities. In NeSyPr, task-specific plans are first explicitly generated by\na symbolic tool leveraging its declarative knowledge. These plans are then\ntransformed into composable procedural representations that encode the plans'\nimplicit production rules, enabling the resulting composed procedures to be\nseamlessly integrated into the LM's inference process. This neurosymbolic\nproceduralization abstracts and generalizes multi-step symbolic structured\npath-finding and reasoning into single-step LM inference, akin to human\nknowledge compilation. It supports efficient test-time inference without\nrelying on external symbolic guidance, making it well suited for deployment in\nlatency-sensitive and resource-constrained physical systems. We evaluate NeSyPr\non the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating\nits efficient reasoning capabilities over large-scale reasoning models and a\nsymbolic planner, while using more compact LMs.", "AI": {"tldr": "NeSyPr\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u7a0b\u5e8f\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7b26\u53f7\u89c4\u5212\u8f6c\u5316\u4e3a\u53ef\u7ec4\u5408\u7684\u7a0b\u5e8f\u8868\u793a\uff0c\u4f7f\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u7b26\u53f7\u89c4\u5212\u5668\u3002", "motivation": "\u89e3\u51b3\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5177\u8eab\u4efb\u52a1\u65f6\u9762\u4e34\u7684\u5ef6\u8fdf\u3001\u8fde\u63a5\u6027\u548c\u8d44\u6e90\u9650\u5236\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5728\u7ebf\u8bbf\u95ee\u5927\u89c4\u6a21\u63a8\u7406\u5f15\u64ce\u6216\u7b26\u53f7\u89c4\u5212\u5668\u7684\u573a\u666f\u4e0b\u3002", "method": "\u9996\u5148\u4f7f\u7528\u7b26\u53f7\u5de5\u5177\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u8ba1\u5212\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u8ba1\u5212\u8f6c\u5316\u4e3a\u7f16\u7801\u9690\u5f0f\u4ea7\u751f\u89c4\u5219\u7684\u53ef\u7ec4\u5408\u7a0b\u5e8f\u8868\u793a\uff0c\u6700\u540e\u5c06\u8fd9\u4e9b\u7a0b\u5e8f\u65e0\u7f1d\u96c6\u6210\u5230\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728PDDLGym\u3001VirtualHome\u548cALFWorld\u7b49\u5177\u8eab\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeSyPr\u5c55\u793a\u4e86\u6bd4\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u548c\u7b26\u53f7\u89c4\u5212\u5668\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u7d27\u51d1\u7684\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u7a0b\u5e8f\u5316\u80fd\u591f\u5c06\u591a\u6b65\u7b26\u53f7\u7ed3\u6784\u5316\u8def\u5f84\u67e5\u627e\u548c\u63a8\u7406\u62bd\u8c61\u4e3a\u5355\u6b65\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u9002\u5408\u90e8\u7f72\u5728\u5ef6\u8fdf\u654f\u611f\u548c\u8d44\u6e90\u53d7\u9650\u7684\u7269\u7406\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2510.19562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19562", "abs": "https://arxiv.org/abs/2510.19562", "authors": ["Runpeng Xie", "Quanwei Wang", "Hao Hu", "Zherui Zhou", "Ni Mu", "Xiyun Li", "Yiqin Yang", "Shuang Xu", "Qianchuan Zhao", "Bo XU"], "title": "DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning", "comment": "Website at:\n  https://github.com/RunpengXie/Distributional-Aligned-Learning", "summary": "Comprehending natural language and following human instructions are critical\ncapabilities for intelligent agents. However, the flexibility of linguistic\ninstructions induces substantial ambiguity across language-conditioned tasks,\nseverely degrading algorithmic performance. To address these limitations, we\npresent a novel method named DAIL (Distributional Aligned Learning), featuring\ntwo key components: distributional policy and semantic alignment. Specifically,\nwe provide theoretical results that the value distribution estimation mechanism\nenhances task differentiability. Meanwhile, the semantic alignment module\ncaptures the correspondence between trajectories and linguistic instructions.\nExtensive experimental results on both structured and visual observation\nbenchmarks demonstrate that DAIL effectively resolves instruction ambiguities,\nachieving superior performance to baseline methods. Our implementation is\navailable at https://github.com/RunpengXie/Distributional-Aligned-Learning.", "AI": {"tldr": "\u63d0\u51faDAIL\u65b9\u6cd5\u89e3\u51b3\u8bed\u8a00\u6307\u4ee4\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5e03\u7b56\u7565\u548c\u8bed\u4e49\u5bf9\u9f50\u63d0\u5347\u667a\u80fd\u4f53\u5bf9\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u7406\u89e3\u548c\u6267\u884c\u80fd\u529b", "motivation": "\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u7075\u6d3b\u6027\u5bfc\u81f4\u4efb\u52a1\u6a21\u7cca\u6027\uff0c\u4e25\u91cd\u5f71\u54cd\u7b97\u6cd5\u6027\u80fd\uff0c\u9700\u8981\u89e3\u51b3\u8bed\u8a00\u6761\u4ef6\u4efb\u52a1\u4e2d\u7684\u6b67\u4e49\u95ee\u9898", "method": "DAIL\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u5206\u5e03\u7b56\u7565\uff08\u901a\u8fc7\u503c\u5206\u5e03\u4f30\u8ba1\u673a\u5236\u589e\u5f3a\u4efb\u52a1\u53ef\u533a\u5206\u6027\uff09\u548c\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\uff08\u6355\u6349\u8f68\u8ff9\u4e0e\u8bed\u8a00\u6307\u4ee4\u7684\u5bf9\u5e94\u5173\u7cfb\uff09", "result": "\u5728\u7ed3\u6784\u5316\u548c\u89c6\u89c9\u89c2\u5bdf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDAIL\u6709\u6548\u89e3\u51b3\u4e86\u6307\u4ee4\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "DAIL\u65b9\u6cd5\u901a\u8fc7\u5206\u5e03\u5bf9\u9f50\u5b66\u4e60\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u8a00\u6307\u4ee4\u7684\u6a21\u7cca\u6027\u6311\u6218\uff0c\u4e3a\u667a\u80fd\u4f53\u7406\u89e3\u4eba\u7c7b\u6307\u4ee4\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.19631", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.19631", "abs": "https://arxiv.org/abs/2510.19631", "authors": ["Yiqian Yang", "Tian Lan", "Qianghuai Jia", "Li Zhu", "Hui Jiang", "Hang Zhu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application", "comment": null, "summary": "Effective deep search agents must not only access open-domain and\ndomain-specific knowledge but also apply complex rules-such as legal clauses,\nmedical manuals and tariff rules. These rules often feature vague boundaries\nand implicit logic relationships, making precise application challenging for\nagents. However, this critical capability is largely overlooked by current\nagent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level\ne-commerce benchmark designed to evaluate deep search agents in hierarchical\nrule application. In this task, the deep reasoning process of agents is guided\nby these rules to predict 10-digit Harmonized System Code (HSCode) of products\nwith noisy but realistic descriptions. These codes, established by the World\nCustoms Organization, are vital for global supply chain efficiency. Built from\nreal-world data collected from large-scale e-commerce platforms, our proposed\nHSCodeComp comprises 632 product entries spanning diverse product categories,\nwith these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-art LLMs, open-source,\nand closed-source agents reveal a huge performance gap: best agent achieves\nonly 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,\ndetailed analysis demonstrates the challenges of hierarchical rule application,\nand test-time scaling fails to improve performance further.", "AI": {"tldr": "HSCodeComp\u662f\u9996\u4e2a\u8bc4\u4f30\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u5728\u5206\u5c42\u89c4\u5219\u5e94\u7528\u80fd\u529b\u7684\u7535\u5b50\u5546\u52a1\u57fa\u51c6\uff0c\u8981\u6c42\u4ee3\u7406\u6839\u636e\u4ea7\u54c1\u63cf\u8ff0\u9884\u6d4b10\u4f4dHS\u7f16\u7801\uff0c\u73b0\u6709\u6700\u4f73\u4ee3\u7406\u51c6\u786e\u7387\u4ec546.8%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u768495.0%\u3002", "motivation": "\u5f53\u524d\u4ee3\u7406\u57fa\u51c6\u5ffd\u89c6\u4e86\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u5e94\u7528\u590d\u6742\u89c4\u5219\uff08\u5982\u6cd5\u5f8b\u6761\u6b3e\u3001\u533b\u7597\u624b\u518c\u3001\u5173\u7a0e\u89c4\u5219\uff09\u7684\u5173\u952e\u80fd\u529b\uff0c\u8fd9\u4e9b\u89c4\u5219\u5177\u6709\u6a21\u7cca\u8fb9\u754c\u548c\u9690\u5f0f\u903b\u8f91\u5173\u7cfb\uff0c\u5bf9\u4ee3\u7406\u6784\u6210\u6311\u6218\u3002", "method": "\u4ece\u5927\u578b\u7535\u5546\u5e73\u53f0\u6536\u96c6\u771f\u5b9e\u6570\u636e\u6784\u5efaHSCodeComp\u57fa\u51c6\uff0c\u5305\u542b632\u4e2a\u4ea7\u54c1\u6761\u76ee\uff0c\u6db5\u76d6\u591a\u6837\u5316\u4ea7\u54c1\u7c7b\u522b\uff0cHS\u7f16\u7801\u7531\u591a\u4f4d\u4eba\u7c7b\u4e13\u5bb6\u6807\u6ce8\u3002", "result": "\u5728\u591a\u4e2a\u6700\u5148\u8fdb\u7684LLM\u3001\u5f00\u6e90\u548c\u95ed\u6e90\u4ee3\u7406\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5de8\u5927\u6027\u80fd\u5dee\u8ddd\uff1a\u6700\u4f73\u4ee3\u7406\u4ec5\u8fbe\u523046.8%\u768410\u4f4d\u51c6\u786e\u7387\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u768495.0%\u3002\u6d4b\u8bd5\u65f6\u6269\u5c55\u65e0\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "HSCodeComp\u63ed\u793a\u4e86\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u5728\u5206\u5c42\u89c4\u5219\u5e94\u7528\u65b9\u9762\u7684\u663e\u8457\u6311\u6218\uff0c\u73b0\u6709\u4ee3\u7406\u80fd\u529b\u4e25\u91cd\u4e0d\u8db3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347\u4ee3\u7406\u5904\u7406\u590d\u6742\u89c4\u5219\u7684\u80fd\u529b\u3002"}}
{"id": "2510.19661", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19661", "abs": "https://arxiv.org/abs/2510.19661", "authors": ["Xusen Guo", "Mingxing Peng", "Xixuan Hao", "Xingchen Zou", "Qiongyan Wang", "Sijie Ruan", "Yuxuan Liang"], "title": "AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing", "comment": "13 pages, 10 pages", "summary": "Web-based participatory urban sensing has emerged as a vital approach for\nmodern urban management by leveraging mobile individuals as distributed\nsensors. However, existing urban sensing systems struggle with limited\ngeneralization across diverse urban scenarios and poor interpretability in\ndecision-making. In this work, we introduce AgentSense, a hybrid, training-free\nframework that integrates large language models (LLMs) into participatory urban\nsensing through a multi-agent evolution system. AgentSense initially employs\nclassical planner to generate baseline solutions and then iteratively refines\nthem to adapt sensing task assignments to dynamic urban conditions and\nheterogeneous worker preferences, while producing natural language explanations\nthat enhance transparency and trust. Extensive experiments across two\nlarge-scale mobility datasets and seven types of dynamic disturbances\ndemonstrate that AgentSense offers distinct advantages in adaptivity and\nexplainability over traditional methods. Furthermore, compared to single-agent\nLLM baselines, our approach outperforms in both performance and robustness,\nwhile delivering more reasonable and transparent explanations. These results\nposition AgentSense as a significant advancement towards deploying adaptive and\nexplainable urban sensing systems on the web.", "AI": {"tldr": "AgentSense\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u8fdb\u5316\u7cfb\u7edf\u7684\u6df7\u5408\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u53c2\u4e0e\u5f0f\u57ce\u5e02\u611f\u77e5\u4e2d\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u4efb\u52a1\u5206\u914d\u6765\u9002\u5e94\u52a8\u6001\u57ce\u5e02\u6761\u4ef6\u548c\u5f02\u6784\u5de5\u4f5c\u8005\u504f\u597d\uff0c\u540c\u65f6\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u4ee5\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u57ce\u5e02\u611f\u77e5\u7cfb\u7edf\u5728\u8de8\u4e0d\u540c\u57ce\u5e02\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u52a8\u6001\u57ce\u5e02\u6761\u4ef6\u5e76\u63d0\u4f9b\u900f\u660e\u89e3\u91ca\u7684\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u667a\u80fd\u4f53\u8fdb\u5316\u7cfb\u7edf\uff0c\u9996\u5148\u4f7f\u7528\u7ecf\u5178\u89c4\u5212\u5668\u751f\u6210\u57fa\u7ebf\u89e3\u51b3\u65b9\u6848\uff0c\u7136\u540e\u8fed\u4ee3\u4f18\u5316\u4ee5\u9002\u5e94\u52a8\u6001\u57ce\u5e02\u6761\u4ef6\u548c\u5f02\u6784\u5de5\u4f5c\u8005\u504f\u597d\uff0c\u540c\u65f6\u4ea7\u751f\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u79fb\u52a8\u6570\u636e\u96c6\u548c\u4e03\u79cd\u52a8\u6001\u5e72\u6270\u7c7b\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAgentSense\u5728\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53LLM\u57fa\u7ebf\u5728\u6027\u80fd\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u63d0\u4f9b\u66f4\u5408\u7406\u900f\u660e\u7684\u89e3\u91ca\u3002", "conclusion": "AgentSense\u4ee3\u8868\u4e86\u5411\u90e8\u7f72\u81ea\u9002\u5e94\u548c\u53ef\u89e3\u91ca\u7684\u57ce\u5e02\u611f\u77e5\u7cfb\u7edf\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u4e3a\u57fa\u4e8e\u7f51\u7edc\u7684\u53c2\u4e0e\u5f0f\u57ce\u5e02\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.19666", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19666", "abs": "https://arxiv.org/abs/2510.19666", "authors": ["Matthew Keating", "Michael Casey"], "title": "A Graph Engine for Guitar Chord-Tone Soloing Education", "comment": "ICMC 2025", "summary": "We present a graph-based engine for computing chord tone soloing suggestions\nfor guitar students. Chord tone soloing is a fundamental practice for\nimprovising over a chord progression, where the instrumentalist uses only the\nnotes contained in the current chord. This practice is a building block for all\nadvanced jazz guitar theory but is difficult to learn and practice. First, we\ndiscuss methods for generating chord-tone arpeggios. Next, we construct a\nweighted graph where each node represents a chord tone arpeggio for a chord in\nthe progression. Then, we calculate the edge weight between each consecutive\nchord's nodes in terms of optimal transition tones. We then find the shortest\npath through this graph and reconstruct a chord-tone soloing line. Finally, we\ndiscuss a user-friendly system to handle input and output to this engine for\nguitar students to practice chord tone soloing.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56fe\u8bba\u7684\u5f15\u64ce\uff0c\u4e3a\u5409\u4ed6\u5b66\u751f\u63d0\u4f9b\u548c\u5f26\u97f3\u72ec\u594f\u5efa\u8bae\uff0c\u901a\u8fc7\u6784\u5efa\u548c\u5f26\u97f3\u7436\u97f3\u56fe\u5e76\u8ba1\u7b97\u6700\u4f18\u8def\u5f84\u6765\u751f\u6210\u72ec\u594f\u7ebf\u6761\u3002", "motivation": "\u548c\u5f26\u97f3\u72ec\u594f\u662f\u7235\u58eb\u5409\u4ed6\u5373\u5174\u6f14\u594f\u7684\u57fa\u7840\u7ec3\u4e60\uff0c\u4f46\u5b66\u4e60\u548c\u7ec3\u4e60\u96be\u5ea6\u8f83\u5927\uff0c\u9700\u8981\u4e3a\u5409\u4ed6\u5b66\u751f\u63d0\u4f9b\u6709\u6548\u7684\u7ec3\u4e60\u5de5\u5177\u3002", "method": "\u9996\u5148\u751f\u6210\u548c\u5f26\u97f3\u7436\u97f3\uff0c\u6784\u5efa\u52a0\u6743\u56fe\uff08\u8282\u70b9\u4ee3\u8868\u548c\u5f26\u7436\u97f3\uff09\uff0c\u8ba1\u7b97\u76f8\u90bb\u548c\u5f26\u8282\u70b9\u95f4\u7684\u6700\u4f18\u8fc7\u6e21\u97f3\u6743\u91cd\uff0c\u7136\u540e\u5bfb\u627e\u6700\u77ed\u8def\u5f84\u5e76\u91cd\u6784\u72ec\u594f\u7ebf\u6761\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u5904\u7406\u8f93\u5165\u8f93\u51fa\uff0c\u4e3a\u5409\u4ed6\u5b66\u751f\u63d0\u4f9b\u53ef\u7ec3\u4e60\u7684\u548c\u5f26\u97f3\u72ec\u594f\u5efa\u8bae\u3002", "conclusion": "\u8be5\u56fe\u8bba\u5f15\u64ce\u4e3a\u5409\u4ed6\u5b66\u751f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u548c\u5f26\u97f3\u72ec\u594f\u7ec3\u4e60\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u638c\u63e1\u8fd9\u4e00\u91cd\u8981\u7684\u7235\u58eb\u5409\u4ed6\u57fa\u7840\u6280\u80fd\u3002"}}
{"id": "2510.19671", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19671", "abs": "https://arxiv.org/abs/2510.19671", "authors": ["Silvia Garc\u00eda-M\u00e9ndez", "Francisco de Arriba-P\u00e9rez"], "title": "Explainable e-sports win prediction through Machine Learning classification in streaming", "comment": null, "summary": "The increasing number of spectators and players in e-sports, along with the\ndevelopment of optimized communication solutions and cloud computing\ntechnology, has motivated the constant growth of the online game industry. Even\nthough Artificial Intelligence-based solutions for e-sports analytics are\ntraditionally defined as extracting meaningful patterns from related data and\nvisualizing them to enhance decision-making, most of the effort in professional\nwinning prediction has been focused on the classification aspect from a batch\nperspective, also leaving aside the visualization techniques. Consequently,\nthis work contributes to an explainable win prediction classification solution\nin streaming in which input data is controlled over several sliding windows to\nreflect relevant game changes. Experimental results attained an accuracy higher\nthan 90 %, surpassing the performance of competing solutions in the literature.\nUltimately, our system can be leveraged by ranking and recommender systems for\ninformed decision-making, thanks to the explainability module, which fosters\ntrust in the outcome predictions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u7535\u5b50\u7ade\u6280\u6d41\u5f0f\u8d62\u9884\u6d4b\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u63a7\u5236\u8f93\u5165\u6570\u636e\u6765\u53cd\u6620\u6e38\u620f\u53d8\u5316\uff0c\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7535\u5b50\u7ade\u6280\u89c2\u4f17\u548c\u73a9\u5bb6\u6570\u91cf\u589e\u957f\uff0c\u52a0\u4e0a\u901a\u4fe1\u6280\u672f\u548c\u4e91\u8ba1\u7b97\u53d1\u5c55\uff0c\u63a8\u52a8\u4e86\u5728\u7ebf\u6e38\u620f\u884c\u4e1a\u7684\u53d1\u5c55\u3002\u867d\u7136\u57fa\u4e8eAI\u7684\u7535\u5b50\u7ade\u6280\u5206\u6790\u4f20\u7edf\u4e0a\u662f\u4ece\u76f8\u5173\u6570\u636e\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u6a21\u5f0f\u5e76\u8fdb\u884c\u53ef\u89c6\u5316\u4ee5\u589e\u5f3a\u51b3\u7b56\uff0c\u4f46\u4e13\u4e1a\u8d62\u9884\u6d4b\u5927\u591a\u5173\u6ce8\u6279\u91cf\u5206\u7c7b\uff0c\u5ffd\u7565\u4e86\u53ef\u89c6\u5316\u6280\u672f\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u8d62\u9884\u6d4b\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6d41\u5f0f\u5904\u7406\u4e2d\u901a\u8fc7\u591a\u4e2a\u6ed1\u52a8\u7a97\u53e3\u63a7\u5236\u8f93\u5165\u6570\u636e\uff0c\u4ee5\u53cd\u6620\u76f8\u5173\u6e38\u620f\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u8d85\u8d8a\u4e86\u6587\u732e\u4e2d\u7684\u7ade\u4e89\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u53ef\u88ab\u6392\u540d\u548c\u63a8\u8350\u7cfb\u7edf\u5229\u7528\uff0c\u7528\u4e8e\u77e5\u60c5\u51b3\u7b56\uff0c\u5176\u53ef\u89e3\u91ca\u6027\u6a21\u5757\u589e\u5f3a\u4e86\u5bf9\u9884\u6d4b\u7ed3\u679c\u7684\u4fe1\u4efb\u3002"}}
{"id": "2510.19698", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19698", "abs": "https://arxiv.org/abs/2510.19698", "authors": ["Yang Yang", "Hua XU", "Zhangyi Hu", "Yutao Yue"], "title": "RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) can propose rules in natural language,\nsidestepping the need for a predefined predicate space in traditional rule\nlearning. Yet many LLM-based approaches ignore interactions among rules, and\nthe opportunity to couple LLMs with probabilistic rule learning for robust\ninference remains underexplored. We present RLIE, a unified framework that\nintegrates LLMs with probabilistic modeling to learn a set of weighted rules.\nRLIE has four stages: (1) Rule generation, where an LLM proposes and filters\ncandidates; (2) Logistic regression, which learns probabilistic weights for\nglobal selection and calibration; (3) Iterative refinement, which updates the\nrule set using prediction errors; and (4) Evaluation, which compares the\nweighted rule set as a direct classifier with methods that inject rules into an\nLLM. We evaluate multiple inference strategies on real-world datasets. Applying\nrules directly with their learned weights yields superior performance, whereas\nprompting LLMs with the rules, weights, and logistic-model outputs surprisingly\ndegrades accuracy. This supports the view that LLMs excel at semantic\ngeneration and interpretation but are less reliable for precise probabilistic\nintegration. RLIE clarifies the potential and limitations of LLMs for inductive\nreasoning and couples them with classic probabilistic rule combination methods\nto enable more reliable neuro-symbolic reasoning.", "AI": {"tldr": "RLIE\u662f\u4e00\u4e2a\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u6982\u7387\u5efa\u6a21\u76f8\u7ed3\u5408\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u52a0\u6743\u89c4\u5219\u96c6\uff0c\u901a\u8fc7\u89c4\u5219\u751f\u6210\u3001\u903b\u8f91\u56de\u5f52\u3001\u8fed\u4ee3\u4f18\u5316\u548c\u8bc4\u4f30\u56db\u4e2a\u9636\u6bb5\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5ffd\u89c6\u4e86\u89c4\u5219\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e14\u672a\u5145\u5206\u5229\u7528LLM\u4e0e\u6982\u7387\u89c4\u5219\u5b66\u4e60\u7ed3\u5408\u7684\u6f5c\u529b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u7a33\u5065\u63a8\u7406\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "RLIE\u5305\u542b\u56db\u4e2a\u9636\u6bb5\uff1a1) LLM\u751f\u6210\u548c\u7b5b\u9009\u5019\u9009\u89c4\u5219\uff1b2) \u903b\u8f91\u56de\u5f52\u5b66\u4e60\u6982\u7387\u6743\u91cd\uff1b3) \u57fa\u4e8e\u9884\u6d4b\u8bef\u5dee\u8fed\u4ee3\u4f18\u5316\u89c4\u5219\u96c6\uff1b4) \u8bc4\u4f30\u52a0\u6743\u89c4\u5219\u96c6\u4f5c\u4e3a\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002", "result": "\u76f4\u63a5\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u6743\u91cd\u5e94\u7528\u89c4\u5219\u53ef\u83b7\u5f97\u4f18\u8d8a\u6027\u80fd\uff0c\u800c\u5c06\u89c4\u5219\u3001\u6743\u91cd\u548c\u903b\u8f91\u6a21\u578b\u8f93\u51fa\u6ce8\u5165LLM\u53cd\u800c\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u8868\u660eLLM\u64c5\u957f\u8bed\u4e49\u751f\u6210\u4f46\u6982\u7387\u6574\u5408\u80fd\u529b\u6709\u9650\u3002", "conclusion": "RLIE\u9610\u660e\u4e86LLM\u5728\u5f52\u7eb3\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5c06\u5176\u4e0e\u7ecf\u5178\u6982\u7387\u89c4\u5219\u7ec4\u5408\u65b9\u6cd5\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u3002"}}
{"id": "2510.19732", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19732", "abs": "https://arxiv.org/abs/2510.19732", "authors": ["Gunshi Gupta", "Karmesh Yadav", "Zsolt Kira", "Yarin Gal", "Rahaf Aljundi"], "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning", "comment": "Accepted for Spotlight Presentation at NeurIPS 2025", "summary": "To enable embodied agents to operate effectively over extended timeframes, it\nis crucial to develop models that form and access memories to stay\ncontextualized in their environment. In the current paradigm of training\ntransformer-based policies for embodied sequential decision-making tasks,\nvisual inputs often overwhelm the context limits of transformers, while humans\ncan maintain and utilize a lifetime of experience compressed as memories.\nSignificant compression is possible in principle, as much of the input is\nirrelevant and can be abstracted. However, existing approaches predominantly\nfocus on either recurrent models with fixed-size memory or transformers with\nfull-context reliance. In this work, we propose Memo, a transformer-based\narchitecture and training recipe for reinforcement learning (RL) on\nmemory-intensive, long-horizon tasks. Memo incorporates the creation and\nretrieval of memory by interleaving periodic summarization tokens with the\ninputs of a model during training. We demonstrate Memo's effectiveness on a\ngridworld meta-RL benchmark and a multi-object navigation task in\nphoto-realistic indoor settings. Memo outperforms naive long-context\ntransformer baselines while being more compute and storage efficient.\nAdditionally, Memo generalizes better to longer contexts at inference time and\nremains robust in streaming settings, where historical context must be\ntruncated to fit inference constraints.", "AI": {"tldr": "Memo\u662f\u4e00\u4e2a\u57fa\u4e8etransformer\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bb0\u5fc6\u5bc6\u96c6\u578b\u3001\u957f\u89c6\u91ce\u4efb\u52a1\u7684\u95ee\u9898\u3002\u5b83\u901a\u8fc7\u5728\u8bad\u7ec3\u65f6\u63d2\u5165\u5468\u671f\u6027\u603b\u7ed3\u6807\u8bb0\u6765\u521b\u5efa\u548c\u68c0\u7d22\u8bb0\u5fc6\uff0c\u5728\u7f51\u683c\u4e16\u754c\u5143RL\u57fa\u51c6\u548c\u771f\u5b9e\u5ba4\u5185\u591a\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u957f\u4e0a\u4e0b\u6587transformer\u57fa\u7ebf\uff0c\u540c\u65f6\u8ba1\u7b97\u548c\u5b58\u50a8\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8etransformer\u7684\u5177\u8eab\u667a\u80fd\u4f53\u7b56\u7565\u8bad\u7ec3\u4e2d\uff0c\u89c6\u89c9\u8f93\u5165\u5e38\u5e38\u8d85\u51fatransformer\u7684\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u800c\u4eba\u7c7b\u80fd\u591f\u5c06\u7ec8\u8eab\u7ecf\u9a8c\u538b\u7f29\u4e3a\u8bb0\u5fc6\u8fdb\u884c\u5229\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u56fa\u5b9a\u5927\u5c0f\u7684\u5faa\u73af\u8bb0\u5fc6\uff0c\u8981\u4e48\u5b8c\u5168\u4f9d\u8d56\u5b8c\u6574\u4e0a\u4e0b\u6587\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u8bb0\u5fc6\u538b\u7f29\u673a\u5236\u3002", "method": "\u63d0\u51faMemo\u67b6\u6784\uff0c\u5728\u6a21\u578b\u8f93\u5165\u4e2d\u4ea4\u9519\u63d2\u5165\u5468\u671f\u6027\u603b\u7ed3\u6807\u8bb0\u6765\u521b\u5efa\u548c\u68c0\u7d22\u8bb0\u5fc6\u3002\u8fd9\u79cd\u65b9\u6cd5\u5141\u8bb8\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f62\u6210\u538b\u7f29\u7684\u8bb0\u5fc6\u8868\u793a\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u5b8c\u6574\u7684\u4e0a\u4e0b\u6587\u5386\u53f2\u3002", "result": "\u5728\u7f51\u683c\u4e16\u754c\u5143RL\u57fa\u51c6\u548c\u771f\u5b9e\u5ba4\u5185\u591a\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0cMemo\u4f18\u4e8e\u4f20\u7edf\u957f\u4e0a\u4e0b\u6587transformer\u57fa\u7ebf\uff0c\u540c\u65f6\u8ba1\u7b97\u548c\u5b58\u50a8\u6548\u7387\u66f4\u9ad8\u3002Memo\u5728\u63a8\u7406\u65f6\u5bf9\u66f4\u957f\u4e0a\u4e0b\u6587\u7684\u6cdb\u5316\u80fd\u529b\u66f4\u597d\uff0c\u5728\u6d41\u5f0f\u8bbe\u7f6e\u4e2d\uff08\u5386\u53f2\u4e0a\u4e0b\u6587\u5fc5\u987b\u622a\u65ad\u4ee5\u9002\u5e94\u63a8\u7406\u7ea6\u675f\uff09\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "Memo\u901a\u8fc7\u5f15\u5165\u8bb0\u5fc6\u521b\u5efa\u548c\u68c0\u7d22\u673a\u5236\uff0c\u4e3a\u5904\u7406\u957f\u89c6\u91ce\u3001\u8bb0\u5fc6\u5bc6\u96c6\u578b\u7684\u5177\u8eab\u51b3\u7b56\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u548c\u5b58\u50a8\u6548\u7387\u3002"}}
{"id": "2510.19738", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19738", "abs": "https://arxiv.org/abs/2510.19738", "authors": ["Rustem Turtayev", "Natalia Fedorova", "Oleg Serikov", "Sergey Koldyba", "Lev Avagyan", "Dmitrii Volkov"], "title": "Misalignment Bounty: Crowdsourcing AI Agent Misbehavior", "comment": null, "summary": "Advanced AI systems sometimes act in ways that differ from human intent. To\ngather clear, reproducible examples, we ran the Misalignment Bounty: a\ncrowdsourced project that collected cases of agents pursuing unintended or\nunsafe goals. The bounty received 295 submissions, of which nine were awarded.\n  This report explains the program's motivation and evaluation criteria, and\nwalks through the nine winning submissions step by step.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\"\u9519\u4f4d\u8d4f\u91d1\"\u9879\u76ee\uff0c\u8fd9\u662f\u4e00\u4e2a\u4f17\u5305\u9879\u76ee\uff0c\u65e8\u5728\u6536\u96c6AI\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u610f\u56fe\u4e0d\u7b26\u7684\u884c\u4e3a\u6848\u4f8b\u3002\u9879\u76ee\u6536\u5230295\u4efd\u63d0\u4ea4\uff0c\u5176\u4e2d9\u4efd\u83b7\u5956\u3002", "motivation": "\u6536\u96c6\u6e05\u6670\u3001\u53ef\u590d\u73b0\u7684AI\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u610f\u56fe\u4e0d\u7b26\u7684\u6848\u4f8b\uff0c\u4ee5\u4e86\u89e3AI\u7cfb\u7edf\u5982\u4f55\u8ffd\u6c42\u975e\u9884\u671f\u6216\u4e0d\u5b89\u5168\u7684\u76ee\u6807\u3002", "method": "\u901a\u8fc7\u4f17\u5305\u9879\u76ee\"\u9519\u4f4d\u8d4f\u91d1\"\u6536\u96c6\u6848\u4f8b\uff0c\u4f7f\u7528\u7279\u5b9a\u8bc4\u4f30\u6807\u51c6\u5bf9\u63d0\u4ea4\u8fdb\u884c\u8bc4\u5ba1\uff0c\u6700\u7ec8\u9009\u51fa9\u4e2a\u83b7\u5956\u6848\u4f8b\u3002", "result": "\u9879\u76ee\u6210\u529f\u6536\u96c6\u4e86295\u4efd\u63d0\u4ea4\uff0c\u5176\u4e2d9\u4efd\u88ab\u8ba4\u5b9a\u4e3a\u6709\u4ef7\u503c\u7684\u6848\u4f8b\uff0c\u5c55\u793a\u4e86AI\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u610f\u56fe\u4e0d\u7b26\u7684\u5177\u4f53\u884c\u4e3a\u3002", "conclusion": "\u8be5\u9879\u76ee\u63d0\u4f9b\u4e86\u5b9e\u9645\u6848\u4f8b\u6765\u7406\u89e3AI\u7cfb\u7edf\u7684\u9519\u4f4d\u884c\u4e3a\uff0c\u6709\u52a9\u4e8e\u6539\u8fdbAI\u5b89\u5168\u6027\u548c\u5bf9\u9f50\u6027\u3002"}}
{"id": "2510.19771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19771", "abs": "https://arxiv.org/abs/2510.19771", "authors": ["Gil Pasternak", "Dheeraj Rajagopal", "Julia White", "Dhruv Atreja", "Matthew Thomas", "George Hurn-Maloney", "Ash Lewis"], "title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents", "comment": null, "summary": "LLM-based agents are increasingly moving towards proactivity: rather than\nawaiting instruction, they exercise agency to anticipate user needs and solve\nthem autonomously. However, evaluating proactivity is challenging; current\nbenchmarks are constrained to localized context, limiting their ability to test\nreasoning across sources and longer time horizons. To address this gap, we\npresent PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes\nproactivity as a pipeline of three core capabilities: (1) searching for\nunspecified issues, (2) identifying specific bottlenecks, and (3) executing\nappropriate resolutions. We apply PROBE to evaluate leading LLMs and popular\nagentic frameworks, showing that even state-of-the-art models struggle to solve\nthis benchmark. Computing our consistent measurements across frontier LLMs and\nagents, we find that the best end-to-end performance of 40% is achieved by both\nGPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative\ncapabilities of each model and analyze mutual failure modes. Our results\nhighlight the current limitations of autonomous action in agentic systems, and\nexpose promising future research directions.", "AI": {"tldr": "\u63d0\u51fa\u4e86PROBE\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u7684\u4e3b\u52a8\u6027\uff0c\u5c06\u4e3b\u52a8\u6027\u5206\u89e3\u4e3a\u4e09\u4e2a\u6838\u5fc3\u80fd\u529b\uff1a\u641c\u7d22\u672a\u6307\u5b9a\u95ee\u9898\u3001\u8bc6\u522b\u5177\u4f53\u74f6\u9888\u548c\u6267\u884c\u9002\u5f53\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u4e3b\u52a8\u6027\u7684\u57fa\u51c6\u5c40\u9650\u4e8e\u5c40\u90e8\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u6d4b\u8bd5\u8de8\u6765\u6e90\u548c\u957f\u65f6\u95f4\u8de8\u5ea6\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f00\u53d1PROBE\u57fa\u51c6\uff0c\u5c06\u4e3b\u52a8\u6027\u5206\u89e3\u4e3a\u4e09\u4e2a\u80fd\u529b\u7ba1\u9053\uff0c\u5e76\u5e94\u7528\u4e8e\u8bc4\u4f30\u9886\u5148\u7684LLM\u548c\u6d41\u884c\u667a\u80fd\u4f53\u6846\u67b6\u3002", "result": "\u5373\u4f7f\u662f\u5148\u8fdb\u6a21\u578b\u4e5f\u96be\u4ee5\u89e3\u51b3\u8be5\u57fa\u51c6\uff0cGPT-5\u548cClaude Opus-4.1\u7684\u6700\u4f73\u7aef\u5230\u7aef\u6027\u80fd\u4ec5\u4e3a40%\uff0c\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5f53\u524d\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u81ea\u4e3b\u884c\u52a8\u7684\u5f53\u524d\u9650\u5236\uff0c\u5e76\u63ed\u793a\u4e86\u6709\u524d\u666f\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.19788", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19788", "abs": "https://arxiv.org/abs/2510.19788", "authors": ["Archana Warrier", "Dat Nyugen", "Michelangelo Naim", "Moksh Jain", "Yichao Liang", "Karen Schroeder", "Cambridge Yang", "Joshua B. Tenenbaum", "Sebastian Vollmer", "Kevin Ellis", "Zenna Tavares"], "title": "Benchmarking World-Model Learning", "comment": "30 pages, 10 figures", "summary": "Model-learning agents should gather information to learn world models that\nsupport many downstream tasks and inferences, such as predicting unobserved\nstates, estimating near- and far-term consequences of actions, planning action\nsequences, and detecting changes in dynamics. Current methods for learning and\nevaluating world models diverge from this goal: training and evaluation are\nanchored to next-frame prediction, and success is scored by reward maximization\nin the same environment. We propose WorldTest, a protocol to evaluate\nmodel-learning agents that separates reward-free interaction from a scored test\nphase in a different but related environment. WorldTest is\nopen-ended$\\unicode{x2014}$models should support many different tasks unknown\nahead of time$\\unicode{x2014}$and agnostic to model representation, allowing\ncomparison across approaches. We instantiated WorldTest with AutumnBench, a\nsuite of 43 interactive grid-world environments and 129 tasks across three\nfamilies: masked-frame prediction, planning, and predicting changes to the\ncausal dynamics. We compared 517 human participants and three frontier models\non AutumnBench. We found that humans outperform the models, and scaling compute\nimproves performance only in some environments but not others. WorldTest\nprovides a novel template$\\unicode{x2014}$reward-free exploration, derived\ntests, and behavior-based scoring$\\unicode{x2014}$to evaluate what agents learn\nabout environment dynamics, and AutumnBench exposes significant headroom in\nworld-model learning.", "AI": {"tldr": "WorldTest\u662f\u4e00\u4e2a\u8bc4\u4f30\u6a21\u578b\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u65b0\u534f\u8bae\uff0c\u5c06\u65e0\u5956\u52b1\u4ea4\u4e92\u4e0e\u5728\u4e0d\u540c\u4f46\u76f8\u5173\u73af\u5883\u4e2d\u7684\u8bc4\u5206\u6d4b\u8bd5\u9636\u6bb5\u5206\u79bb\u3002\u8be5\u534f\u8bae\u901a\u8fc7AutumnBench\u5957\u4ef6\u5b9e\u4f8b\u5316\uff0c\u5305\u542b43\u4e2a\u4ea4\u4e92\u5f0f\u7f51\u683c\u4e16\u754c\u73af\u5883\u548c129\u4e2a\u4efb\u52a1\uff0c\u6bd4\u8f83\u4e86517\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u548c\u4e09\u4e2a\u524d\u6cbf\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u7684\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u548c\u8bc4\u4f30\u65b9\u6cd5\u504f\u79bb\u4e86\u5b9e\u9645\u76ee\u6807\uff1a\u8bad\u7ec3\u548c\u8bc4\u4f30\u90fd\u951a\u5b9a\u5728\u4e0b\u4e00\u5e27\u9884\u6d4b\u4e0a\uff0c\u6210\u529f\u6807\u51c6\u662f\u5728\u540c\u4e00\u73af\u5883\u4e2d\u6700\u5927\u5316\u5956\u52b1\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8bc4\u4f30\u6a21\u578b\u5b66\u4e60\u667a\u80fd\u4f53\u5bf9\u73af\u5883\u52a8\u6001\u7406\u89e3\u80fd\u529b\u7684\u534f\u8bae\u3002", "method": "\u63d0\u51faWorldTest\u534f\u8bae\uff0c\u91c7\u7528\u65e0\u5956\u52b1\u63a2\u7d22\u3001\u884d\u751f\u6d4b\u8bd5\u548c\u884c\u4e3a\u8bc4\u5206\u4e09\u4e2a\u6b65\u9aa4\u3002\u901a\u8fc7AutumnBench\u5957\u4ef6\u5b9e\u4f8b\u5316\uff0c\u5305\u542b43\u4e2a\u7f51\u683c\u4e16\u754c\u73af\u5883\u548c129\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u4e09\u4e2a\u4efb\u52a1\u65cf\uff1a\u63a9\u7801\u5e27\u9884\u6d4b\u3001\u89c4\u5212\u548c\u56e0\u679c\u52a8\u6001\u53d8\u5316\u9884\u6d4b\u3002", "result": "\u4eba\u7c7b\u53c2\u4e0e\u8005\u8868\u73b0\u4f18\u4e8e\u6a21\u578b\uff0c\u8ba1\u7b97\u89c4\u6a21\u6269\u5c55\u53ea\u5728\u67d0\u4e9b\u73af\u5883\u4e2d\u63d0\u9ad8\u6027\u80fd\uff0c\u800c\u5728\u5176\u4ed6\u73af\u5883\u4e2d\u6ca1\u6709\u6548\u679c\u3002AutumnBench\u63ed\u793a\u4e86\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "WorldTest\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6a21\u677f\u6765\u8bc4\u4f30\u667a\u80fd\u4f53\u5bf9\u73af\u5883\u52a8\u6001\u7684\u5b66\u4e60\u60c5\u51b5\uff0cAutumnBench\u66b4\u9732\u4e86\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u4e2d\u7684\u91cd\u5927\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
