<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 38]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem](https://arxiv.org/abs/2510.19835)
*Max B. Zhao,Fei Li*

Main category: cs.AI

TL;DR: 提出一种量子启发算法，使用矩阵乘积态和离散驱动调度解决QUBO问题，在Sudoku和MaxCut问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决二次无约束二进制优化问题，该问题在数学上等价于寻找伊辛自旋玻璃哈密顿量的基态，需要高效算法来处理工业规模的应用。

Method: 使用矩阵乘积态紧凑表示自旋构型的大叠加，结合离散驱动调度引导MPS向基态演化，通过DMRG方法迭代最小化系统能量。

Result: 算法可靠地识别全局最小值而非近似解，成功解决超过200个自旋的Sudoku问题和Biq Mac库中最多251节点、3265边的MaxCut问题。

Conclusion: 该量子启发方法具有可扩展性、通用性和适用于工业规模QUBO应用的优势。

Abstract: We propose and evaluate a quantum-inspired algorithm for solving Quadratic
Unconstrained Binary Optimization (QUBO) problems, which are mathematically
equivalent to finding ground states of Ising spin-glass Hamiltonians. The
algorithm employs Matrix Product States (MPS) to compactly represent large
superpositions of spin configurations and utilizes a discrete driving schedule
to guide the MPS toward the ground state. At each step, a driver Hamiltonian --
incorporating a transverse magnetic field -- is combined with the problem
Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is
updated using the standard Density Matrix Renormalization Group (DMRG) method,
which iteratively minimizes the system's energy via multiple sweeps across the
spin chain. Despite its heuristic nature, the algorithm reliably identifies
global minima, not merely near-optimal solutions, across diverse QUBO
instances. We first demonstrate its effectiveness on intermediate-level Sudoku
puzzles from publicly available sources, involving over $200$ Ising spins with
long-range couplings dictated by constraint satisfaction. We then apply the
algorithm to MaxCut problems from the Biq Mac library, successfully solving
instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages
of this quantum-inspired approach, including its scalability, generalizability,
and suitability for industrial-scale QUBO applications.

</details>


### [2] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: 该研究提出了分析可靠性基准（ARB），这是一个可复现的框架，用于量化应用于能源系统分析的大型语言模型的推理可靠性，填补了当前验证实践只关注预测准确性而忽略逻辑完整性的空白。


<details>
  <summary>Details</summary>
Motivation: 人工智能和机器学习在能源领域的应用日益增多，但目前缺乏标准化框架来评估这些系统的推理是否正确。现有的验证实践主要关注预测准确性或计算效率，而未测试分析结论的逻辑完整性。

Method: 研究引入了分析可靠性基准（ARB），整合了五个子指标：准确性、推理可靠性、不确定性纪律、政策一致性和透明度，并使用开放技术经济数据集（NREL ATB 2024、DOE H2A/H2New、IEA WEO 2024）在确定性、概率性和认知性场景下评估模型性能。测试了四个前沿模型（GPT-4/5、Claude 4.5 Sonnet、Gemini 2.5 Pro、Llama 3 70B）。

Result: 结果表明推理可靠性可以客观测量。GPT-4/5和Claude 4.5 Sonnet实现了持续且符合政策的推理（分析可靠性指数大于90），Gemini 2.5 Pro表现出中等稳定性，而Llama 3 70B低于专业阈值。统计验证确认这些差异显著且可复现。

Conclusion: ARB建立了能源文献中首个验证人工智能系统中因果、概率和政策驱动推理的定量方法，为全球能源转型中可信赖和透明的分析应用提供了参考框架。

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


### [3] [Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory](https://arxiv.org/abs/2510.19838)
*Shiqi He,Yue Cui,Xinyu Ma,Yaliang Li,Bolin Ding,Mosharaf Chowdhury*

Main category: cs.AI

TL;DR: Branch-and-Browse是一个细粒度的网页代理框架，通过树状结构探索、网页状态重放和页面动作记忆，显著提升了LLM驱动的自主网页代理在复杂任务中的推理深度和执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM驱动的自主网页代理在推理深度和执行效率方面存在局限：线性方法无法处理多步推理且缺乏有效回溯，而其他搜索策略则粒度粗糙且计算成本高昂。

Method: 该框架采用：(1) 显式子任务管理和树状结构探索实现可控的多分支推理；(2) 通过带背景推理的高效网页状态重放来引导探索；(3) 利用页面动作记忆在会话内外共享已探索的动作。

Result: 在WebArena基准测试中，Branch-and-Browse实现了35.8%的任务成功率，相比最先进方法执行时间减少了40.4%。

Conclusion: Branch-and-Browse是一个可靠且高效的LLM网页代理框架，显著提升了在开放网页环境中的推理能力和执行效率。

Abstract: Autonomous web agents powered by large language models (LLMs) show strong
potential for performing goal-oriented tasks such as information retrieval,
report generation, and online transactions. These agents mark a key step toward
practical embodied reasoning in open web environments. However, existing
approaches remain limited in reasoning depth and efficiency: vanilla linear
methods fail at multi-step reasoning and lack effective backtracking, while
other search strategies are coarse-grained and computationally costly. We
introduce Branch-and-Browse, a fine-grained web agent framework that unifies
structured reasoning-acting, contextual memory, and efficient execution. It (i)
employs explicit subtask management with tree-structured exploration for
controllable multi-branch reasoning, (ii) bootstraps exploration through
efficient web state replay with background reasoning, and (iii) leverages a
page action memory to share explored actions within and across sessions. On the
WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\%
and reduces execution time by up to 40.4\% relative to state-of-the-art
methods. These results demonstrate that Branch-and-Browse is a reliable and
efficient framework for LLM-based web agents.

</details>


### [4] [DAG-Math: Graph-Guided Mathematical Reasoning in LLMs](https://arxiv.org/abs/2510.19842)
*Yuanhe Zhang,Ilja Kuzborskij,Jason D. Lee,Chenlei Leng,Fanghui Liu*

Main category: cs.AI

TL;DR: 提出基于有向无环图(DAG)的框架来评估LLMs在数学推理中的逻辑一致性，引入逻辑紧密度指标，揭示PASS@k指标无法检测的推理差异。


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought(CoT)方法无法确定LLMs的成功是源于搜索、死记硬背还是规则一致推理，需要更精细的评估框架。

Method: 将CoT建模为基于规则的随机过程，在DAG上定义逻辑紧密度指标，构建DAG-MATH CoT格式和基准测试。

Result: 在标准数学推理数据集上发现代表性LLM家族间存在统计显著的推理保真度差异，即使PASS@k指标相当。

Conclusion: 该框架在自由形式CoT和形式证明系统间取得平衡，为LLMs推理评估提供可操作的诊断工具。

Abstract: Large Language Models (LLMs) demonstrate strong performance on mathematical
problems when prompted with Chain-of-Thought (CoT), yet it remains unclear
whether this success stems from search, rote procedures, or rule-consistent
reasoning. To address this, we propose modeling CoT as a certain rule-based
stochastic process over directed acyclic graphs (DAGs), where nodes represent
intermediate derivation states and edges encode rule applications. Within this
framework, we introduce logical closeness, a metric that quantifies how well a
model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG
structure, providing evaluation beyond classical PASS@k metrics. Building on
this, we introduce the DAG-MATH CoT format and construct a benchmark that
guides LLMs to generate CoT trajectories in this format, thereby enabling the
evaluation of their reasoning ability under our framework. Across standard
mathematical reasoning datasets, our analysis uncovers statistically
significant differences in reasoning fidelity among representative LLM
families-even when PASS@k is comparable-highlighting gaps between final-answer
accuracy and rule-consistent derivation. Our framework provides a balance
between free-form CoT and formal proofs systems, offering actionable
diagnostics for LLMs reasoning evaluation. Our benchmark and code are available
at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.

</details>


### [5] [Surfer 2: The Next Generation of Cross-Platform Computer Use Agents](https://arxiv.org/abs/2510.19949)
*Mathieu Andreux,Märt Bakler,Yanael Barbier,Hamza Ben Chekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Nathan Bout,Matthias Brunel,Aleix Cambray,Pierre-Louis Cedoz,Antoine Chassang,Gautier Cloix,Ethan Connelly,Alexandra Constantinou,Ramzi De Coster,Hubert de la Jonquiere,Aurélien Delfosse,Maxime Delpit,Alexis Deprez,Augustin Derupti,Mathieu Diaz,Shannon D'Souza,Julie Dujardin,Abai Edmund,Michael Eickenberg,Armand Fatalot,Wissem Felissi,Isaac Herring,Xavier Koegler,Erwan Le Jumeau de Kergaradec,Aurélien Lac,Maxime Langevin,Corentin Lauverjat,Antonio Loison,Avshalom Manevich,Axel Moyal,Axel Nguyen Kerbel,Marinela Parovic,Julien Revelle,Guillaume Richard,Mats Richter,Ronan Riochet,María Santos,Romain Savidan,Laurent Sifre,Maxime Theillard,Marc Thibault,Ivan Valentini,Tony Wu,Laura Yie,Kai Yuan,Jevgenij Zubovskij*

Main category: cs.AI

TL;DR: Surfer 2是一个基于视觉观察的统一架构，在网页、桌面和移动环境中实现最先进性能，无需任务特定微调即可超越所有先前系统。


<details>
  <summary>Details</summary>
Motivation: 解决现有代理系统依赖环境特定接口、限制跨平台部署的问题，构建能够在网页、桌面和移动环境中通用的智能代理。

Method: 集成层次化上下文管理、解耦规划与执行、以及带自适应恢复的自我验证，实现长任务周期的可靠操作。

Result: 在WebVoyager上达到97.1%准确率，WebArena 69.6%，OSWorld 60.1%，AndroidWorld 87.1%，多尝试下在所有基准测试中超越人类表现。

Conclusion: 系统化编排能够放大基础模型能力，仅通过视觉交互实现通用计算机控制，同时需要下一代视觉语言模型来实现帕累托最优的成本效率。

Abstract: Building agents that generalize across web, desktop, and mobile environments
remains an open challenge, as prior systems rely on environment-specific
interfaces that limit cross-platform deployment. We introduce Surfer 2, a
unified architecture operating purely from visual observations that achieves
state-of-the-art performance across all three environments. Surfer 2 integrates
hierarchical context management, decoupled planning and execution, and
self-verification with adaptive recovery, enabling reliable operation over long
task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on
WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior
systems without task-specific fine-tuning. With multiple attempts, Surfer 2
exceeds human performance on all benchmarks. These results demonstrate that
systematic orchestration amplifies foundation model capabilities and enables
general-purpose computer control through visual interaction alone, while
calling for a next-generation vision language model to achieve Pareto-optimal
cost-efficiency.

</details>


### [6] [RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs](https://arxiv.org/abs/2510.19954)
*Joseph Meyer,Divyansha Lachi,Reza Mohammadi,Roshan Reddy Upendra,Eva L. Dyer,Mark Li,Tom Palczewski*

Main category: cs.AI

TL;DR: RELATE是一个模式无关的图神经网络特征编码器，使用共享的模态特定编码器处理多模态节点属性，通过交叉注意力聚合特征，在保持性能的同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络依赖模式特定的特征编码器，需要为每种节点类型和特征列分别设计模块，这阻碍了可扩展性和参数共享。

Method: 使用共享的模态特定编码器处理分类、数值、文本和时间属性，然后通过Perceiver风格的交叉注意力模块将特征聚合为固定大小的置换不变节点表示。

Result: 在RelBench基准测试中，RELATE与ReLGNN和HGT配合使用时，性能达到模式特定编码器的97%以上，同时参数数量减少多达5倍。

Conclusion: RELATE支持不同的数据模式，使通用图神经网络能够进行多数据集预训练，为关系图数据的基础模型铺平了道路。

Abstract: Relational multi-table data is common in domains such as e-commerce,
healthcare, and scientific research, and can be naturally represented as
heterogeneous temporal graphs with multi-modal node attributes. Existing graph
neural networks (GNNs) rely on schema-specific feature encoders, requiring
separate modules for each node type and feature column, which hinders
scalability and parameter sharing. We introduce RELATE (Relational Encoder for
Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature
encoder that can be used with any general purpose GNN. RELATE employs shared
modality-specific encoders for categorical, numerical, textual, and temporal
attributes, followed by a Perceiver-style cross-attention module that
aggregates features into a fixed-size, permutation-invariant node
representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,
where it achieves performance within 3% of schema-specific encoders while
reducing parameter counts by up to 5x. This design supports varying schemas and
enables multi-dataset pretraining for general-purpose GNNs, paving the way
toward foundation models for relational graph data.

</details>


### [7] [A new wave of vehicle insurance fraud fueled by generative AI](https://arxiv.org/abs/2510.19957)
*Amir Hever,Itai Orr*

Main category: cs.AI

TL;DR: 生成式AI正在加剧保险欺诈，使大规模快速伪造事故证据成为可能。保险公司采用AI检测工具应对，但面临误报、漏报和欺诈者不断进化的挑战。


<details>
  <summary>Details</summary>
Motivation: 保险欺诈每年造成数百亿美元损失，生成式AI技术让欺诈者能轻松伪造逼真的事故照片、损坏证据和虚假身份，加剧了保险欺诈问题。

Method: 保险公司部署AI驱动的深度伪造检测软件和增强验证流程，但现有缓解策略存在局限性，包括检测工具的误报漏报问题。

Result: 检测工具存在误报和漏报问题，欺诈者不断调整策略规避自动检查，形成生成式AI与检测技术之间的军备竞赛。

Conclusion: 打击AI驱动的保险欺诈仍面临持续挑战，需要更先进的解决方案来检测、缓解和威慑这种新型欺诈浪潮。

Abstract: Generative AI is supercharging insurance fraud by making it easier to falsify
accident evidence at scale and in rapid time. Insurance fraud is a pervasive
and costly problem, amounting to tens of billions of dollars in losses each
year. In the vehicle insurance sector, fraud schemes have traditionally
involved staged accidents, exaggerated damage, or forged documents. The rise of
generative AI, including deepfake image and video generation, has introduced
new methods for committing fraud at scale. Fraudsters can now fabricate highly
realistic crash photos, damage evidence, and even fake identities or documents
with minimal effort, exploiting AI tools to bolster false insurance claims.
Insurers have begun deploying countermeasures such as AI-based deepfake
detection software and enhanced verification processes to detect and mitigate
these AI-driven scams. However, current mitigation strategies face significant
limitations. Detection tools can suffer from false positives and negatives, and
sophisticated fraudsters continuously adapt their tactics to evade automated
checks. This cat-and-mouse arms race between generative AI and detection
technology, combined with resource and cost barriers for insurers, means that
combating AI-enabled insurance fraud remains an ongoing challenge. In this
white paper, we present UVeye layered solution for vehicle fraud, representing
a major leap forward in the ability to detect, mitigate and deter this new wave
of fraud.

</details>


### [8] [AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits](https://arxiv.org/abs/2510.19964)
*Nitsa J Herzog,Rejwan Bin Sulaiman,David J Herzog,Rose Fong*

Main category: cs.AI

TL;DR: 该研究使用机器学习模型预测学术成功，通过领导力人格特质分析129名环境工程硕士生，发现随机森林分类器在包含17个人格特征和领导力标记时达到87.50%的准确率。


<details>
  <summary>Details</summary>
Motivation: 探索AI技术在个性化学习中的潜力，通过领导力人格特质预测学术成功，为早期识别学生优劣势和制定个性化学习策略提供机会。

Method: 收集129名硕士生的5项领导力人格测试数据（23个特征），结合平均成绩进行探索性数据分析和相关性分析，使用皮尔逊相关系数进行特征选择，并调优7种机器学习算法。

Result: 随机森林分类器表现最佳，包含17个人格特征和领导力标记的模型准确率达87.50%，不包含该特征的模型准确率为85.71%。

Conclusion: 该研究为早期识别学生优劣势和选择最适合的个性化学习策略提供了额外机会，证明了领导力人格特质在预测学术成功方面的有效性。

Abstract: The study explores the potential of AI technologies in personalized learning,
suggesting the prediction of academic success through leadership personality
traits and machine learning modelling. The primary data were obtained from 129
master's students in the Environmental Engineering Department, who underwent
five leadership personality tests with 23 characteristics. Students used
self-assessment tools that included Personality Insight, Workplace Culture,
Motivation at Work, Management Skills, and Emotion Control tests. The test
results were combined with the average grade obtained from academic reports.
The study employed exploratory data analysis and correlation analysis. Feature
selection utilized Pearson correlation coefficients of personality traits. The
average grades were separated into three categories: fail, pass, and excellent.
The modelling process was performed by tuning seven ML algorithms, such as SVM,
LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance
was achieved with the RF classifier, which yielded an accuracy of 87.50% for
the model incorporating 17 personality trait features and the leadership mark
feature, and an accuracy of 85.71% for the model excluding this feature. In
this way, the study offers an additional opportunity to identify students'
strengths and weaknesses at an early stage of their education process and
select the most suitable strategies for personalized learning.

</details>


### [9] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: 提出了一种使用大型语言模型将秘密文本隐藏在看似正常的文本中的协议，可以在相同长度的文本中嵌入隐藏信息，且编码解码过程快速高效。


<details>
  <summary>Details</summary>
Motivation: 探索文本与作者意图的分离可能性，展示LLM如何能够创建表面上合理但实际包含隐藏信息的文本，这对AI安全和文本信任构成挑战。

Method: 使用简单的协议，利用开源LLM（即使是80亿参数的模型）在相同长度的文本中编码和解码隐藏信息，整个过程可在笔记本电脑上快速完成。

Result: 证明了即使是中等规模的LLM也能实现高质量的文本隐藏，一个摘要长度的信息可以在几秒钟内完成编码和解码。

Conclusion: 这种协议的存在表明文本与作者意图之间存在根本性分离，进一步削弱了对书面通信的信任，对AI安全提出了紧迫问题，并挑战了我们对LLM知识理解的传统认知。

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [10] [AI PB: A Grounded Generative Agent for Personalized Investment Insights](https://arxiv.org/abs/2510.20099)
*Daewoo Park,Suho Park,Inseok Hong,Hanwool Lee,Junkyu Park,Sangjun Lee,Jeongman An,Hyunbin Loh*

Main category: cs.AI

TL;DR: AI PB是一个生产级生成式代理系统，部署在真实零售金融环境中，能够主动生成基于事实、合规且个性化的投资洞察，而非被动回答问题。


<details>
  <summary>Details</summary>
Motivation: 在金融领域开发能够主动提供个性化投资建议的AI系统，同时确保数据安全、合规性和可靠性，以应对高风险金融环境的需求。

Method: 采用组件化编排层进行确定性路由决策，结合混合检索管道（OpenSearch+金融领域嵌入模型）和多阶段推荐机制（规则启发式、序列行为建模、上下文多臂老虎机），全系统在韩国金融监管下本地部署。

Result: 通过人工质量评估和系统指标验证，证明了基于事实的生成、显式路由和分层安全机制能够在高风险金融环境中提供可信的AI洞察。

Conclusion: 在金融等高风险领域，通过明确的系统架构设计和安全机制，可以实现可靠且合规的生成式AI应用。

Abstract: We present AI PB, a production-scale generative agent deployed in real retail
finance. Unlike reactive chatbots that answer queries passively, AI PB
proactively generates grounded, compliant, and user-specific investment
insights. It integrates (i) a component-based orchestration layer that
deterministically routes between internal and external LLMs based on data
sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the
finance-domain embedding model, and (iii) a multi-stage recommendation
mechanism combining rule heuristics, sequential behavioral modeling, and
contextual bandits. Operating fully on-premises under Korean financial
regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100
GPUs. Through human QA and system metrics, we demonstrate that grounded
generation with explicit routing and layered safety can deliver trustworthy AI
insights in high-stakes finance.

</details>


### [11] [Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions](https://arxiv.org/abs/2510.20102)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Sangmi Chai*

Main category: cs.AI

TL;DR: HCLA是一个以人为中心的多智能体系统，用于数字资产交易异常检测，通过自然语言交互提供可解释的分析结果。


<details>
  <summary>Details</summary>
Motivation: 为了解决金融取证中非专家用户难以理解和信任传统异常检测系统的问题，需要开发透明且可交互的检测工具。

Method: 系统采用三角色架构（解析、检测、解释），通过对话工作流将用户自然语言查询转换为XGBoost检测器的输入，并提供基于特征的叙事解释。

Result: 在比特币混币数据集上，基线检测器达到高准确率，HCLA系统额外提供了可解释性和交互式优化能力。

Conclusion: 人在回路的设计提高了金融取证系统的透明度和可信度，使非专家用户能够更好地理解和信任检测结果。

Abstract: We present HCLA, a human-centered multi-agent system for anomaly detection in
digital asset transactions. The system links three roles: Parsing, Detection,
and Explanation, into a conversational workflow that lets non-experts ask
questions in natural language, inspect structured analytics, and obtain
context-aware rationales. Implemented with an open-source web UI, HCLA
translates user intents into a schema for a classical detector (XGBoost in our
prototype) and returns narrative explanations grounded in the underlying
features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the
baseline detector reaches strong accuracy, while HCLA adds interpretability and
interactive refinement. We describe the architecture, interaction loop,
dataset, evaluation protocol, and limitations, and discuss how a
human-in-the-loop design improves transparency and trust in financial
forensics.

</details>


### [12] [The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice](https://arxiv.org/abs/2510.20109)
*Joshua Yuvaraj*

Main category: cs.AI

TL;DR: 论文认为需要重新评估AI在法律实践中的使用，提出了验证-价值悖论：AI带来的效率提升会被相应的验证需求抵消，导致净价值往往为零


<details>
  <summary>Details</summary>
Motivation: 基于律师因提交不准确的AI生成内容而受处罚的案例，认为现有评估AI使用的范式需要重新审视，考虑到AI与现实脱节、缺乏透明度以及律师的核心职责

Method: 提出了验证-价值悖论作为替代模型，该模型更全面地反映了AI的特性和律师的职责要求

Result: AI在法律实践中的效率提升会被相应的验证需求所抵消，导致净价值往往可以忽略不计

Conclusion: 需要重新思考AI在法律实践和教育中的使用，强调对真相的忠诚和公民责任等核心价值观的重要性

Abstract: It is often claimed that machine learning-based generative AI products will
drastically streamline and reduce the cost of legal practice. This enthusiasm
assumes lawyers can effectively manage AI's risks. Cases in Australia and
elsewhere in which lawyers have been reprimanded for submitting inaccurate
AI-generated content to courts suggest this paradigm must be revisited. This
paper argues that a new paradigm is needed to evaluate AI use in practice,
given (a) AI's disconnection from reality and its lack of transparency, and (b)
lawyers' paramount duties like honesty, integrity, and not to mislead the
court. It presents an alternative model of AI use in practice that more
holistically reflects these features (the verification-value paradox). That
paradox suggests increases in efficiency from AI use in legal practice will be
met by a correspondingly greater imperative to manually verify any outputs of
that use, rendering the net value of AI use often negligible to lawyers. The
paper then sets out the paradox's implications for legal practice and legal
education, including for AI use but also the values that the paradox suggests
should undergird legal practice: fidelity to the truth and civic
responsibility.

</details>


### [13] [TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning](https://arxiv.org/abs/2510.20188)
*Morris Yu-Chao Huang,Zhen Tan,Mohan Zhang,Pingzhi Li,Zhuo Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: TRUST是一个去中心化的AI审计框架，通过共识机制、分层DAG分解、区块链账本和隐私保护分段，解决大语言模型推理链验证的鲁棒性、可扩展性、透明性和隐私性问题。


<details>
  <summary>Details</summary>
Motivation: 现有审计方法集中化、不透明且难以扩展，无法有效验证大语言模型推理链的忠实性和无害性，在关键领域部署专有模型存在重大风险。

Method: 采用共识机制确保在30%恶意参与者下仍能保证正确性；使用分层DAG分解实现可扩展的并行审计；通过区块链账本记录验证决策确保公共问责；采用隐私保护分段仅共享部分推理步骤保护专有逻辑。

Result: 在多个LLM（GPT-OSS、DeepSeek-r1、Qwen）和推理任务（数学、医学、科学、人文）上的实验表明，TRUST能有效检测推理缺陷，并对抗性审计者保持鲁棒性。

Conclusion: TRUST开创了去中心化AI审计，为大语言模型的安全可信部署提供了实用路径。

Abstract: Large Language Models generate complex reasoning chains that reveal their
decision-making, yet verifying the faithfulness and harmlessness of these
intermediate steps remains a critical unsolved problem. Existing auditing
methods are centralized, opaque, and hard to scale, creating significant risks
for deploying proprietary models in high-stakes domains. We identify four core
challenges: (1) Robustness: Centralized auditors are single points of failure,
prone to bias or attacks. (2) Scalability: Reasoning traces are too long for
manual verification. (3) Opacity: Closed auditing undermines public trust. (4)
Privacy: Exposing full reasoning risks model theft or distillation. We propose
TRUST, a transparent, decentralized auditing framework that overcomes these
limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing
correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG
decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A
blockchain ledger that records all verification decisions for public
accountability. (4) Privacy-preserving segmentation, sharing only partial
reasoning steps to protect proprietary logic. We provide theoretical guarantees
for the security and economic incentives of the TRUST framework. Experiments
across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math,
medical, science, humanities) show TRUST effectively detects reasoning flaws
and remains robust against adversarial auditors. Our work pioneers
decentralized AI auditing, offering a practical path toward safe and
trustworthy LLM deployment.

</details>


### [14] [The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI](https://arxiv.org/abs/2510.20190)
*Marcelo Maciel Amaral,Raymond Aschheim*

Main category: cs.AI

TL;DR: 论文提出AGI发展中的锁定阶段理论，即从开放模仿转向身份巩固的过程，并开发了检测指标。实验表明不同规模模型呈现不同表现：小模型有性能权衡，中规模模型基本无成本，大规模量化模型出现瞬时不稳定。


<details>
  <summary>Details</summary>
Motivation: 基于人类发展类比，假设AGI进步需要经历锁定阶段，从开放模仿转向稳定的身份巩固，这对AGI可靠性和安全性至关重要。

Method: 形式化锁定阶段概念，将其与学习动态中的已知现象联系，提出操作性检测指标，并在不同规模模型上进行实验验证。

Result: 行为巩固快速且非线性，但对通用能力的影响各异：小模型有性能权衡，中规模模型基本无成本，大规模量化模型出现瞬时不稳定。

Conclusion: 身份巩固是AGI级可靠性的前提，也是关键安全控制点：身份可被工程化设计以提高可靠性，但也可能在扩展过程中自发形成，可能固化不可预测的目标和行为。

Abstract: Large language models (LLMs) remain broadly open and highly steerable: they
imitate at scale, accept arbitrary system prompts, and readily adopt multiple
personae. By analogy to human development, we hypothesize that progress toward
artificial general intelligence (AGI) involves a lock-in phase: a transition
from open imitation to identity consolidation, in which goal structures,
refusals, preferences, and internal representations become comparatively stable
and resistant to external steering. We formalize this phase, link it to known
phenomena in learning dynamics, and propose operational metrics for onset
detection. Experimentally, we demonstrate that while the behavioral
consolidation is rapid and non-linear, its side-effects on general capabilities
are not monolithic. Our results reveal a spectrum of outcomes--from performance
trade-offs in small models, through largely cost-free adoption in mid-scale
models, to transient instabilities in large, quantized models. We argue that
such consolidation is a prerequisite for AGI-level reliability and also a
critical control point for safety: identities can be deliberately engineered
for reliability, yet may also emerge spontaneously during scaling, potentially
hardening unpredictable goals and behaviors.

</details>


### [15] [Merge and Conquer: Evolutionarily Optimizing AI for 2048](https://arxiv.org/abs/2510.20205)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.AI

TL;DR: 该论文研究了在动态环境中优化AI的进化训练方法，使用2048游戏作为测试平台，比较了单智能体系统和双智能体系统在游戏性能上的表现。


<details>
  <summary>Details</summary>
Motivation: 优化AI在动态环境中的表现是机器学习研究的基本挑战，2048游戏结合了策略性和随机性元素，为研究决策制定、长期规划和动态适应提供了理想平台。

Method: 实现了两种系统：双智能体元提示系统（一个"思考者"LLM优化游戏策略给"执行者"LLM使用）和基于蒙特卡洛树搜索价值函数优化的单智能体系统，还实验了回滚功能以避免性能退化。

Result: 单智能体系统取得了显著改进，每个训练周期平均增加473.2分，且呈现明显上升趋势（相关性ρ=0.607），LLM对游戏的理解也随着高级策略的发展而增长。双智能体系统改进有限，显示了元提示的内在局限性。

Conclusion: 进化优化技术在非确定性环境中改善AI性能具有潜力，单智能体方法比双智能体元提示方法更有效。

Abstract: Optimizing artificial intelligence (AI) for dynamic environments remains a
fundamental challenge in machine learning research. In this paper, we examine
evolutionary training methods for optimizing AI to solve the game 2048, a 2D
sliding puzzle. 2048, with its mix of strategic gameplay and stochastic
elements, presents an ideal playground for studying decision-making, long-term
planning, and dynamic adaptation. We implemented two distinct systems: a
two-agent metaprompting system where a "thinker" large language model (LLM)
agent refines gameplay strategies for an "executor" LLM agent, and a
single-agent system based on refining a value function for a limited Monte
Carlo Tree Search. We also experimented with rollback features to avoid
performance degradation. Our results demonstrate the potential of evolutionary
refinement techniques in improving AI performance in non-deterministic
environments. The single-agent system achieved substantial improvements, with
an average increase of 473.2 points per cycle, and with clear upward trends
(correlation $\rho$=0.607) across training cycles. The LLM's understanding of
the game grew as well, shown in its development of increasingly advanced
strategies. Conversely, the two-agent system did not garner much improvement,
highlighting the inherent limits of meta-prompting.

</details>


### [16] [Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods](https://arxiv.org/abs/2510.20252)
*Tianyi Zhang,Xiaolin Zhou,Yunzhe Wang,Erik Cambria,David Traum,Rui Mao*

Main category: cs.AI

TL;DR: 该论文评估了大型语言模型在个体化认知模拟中的能力，通过构建基于新出版小说的数据集和11条件认知评估框架，测试了7个现成LLM在作者风格模仿任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM能够表面模仿人类行为，但其模拟更深层次个体化认知过程的能力尚不清楚，需要开发评估方法来理解LLM在认知模拟方面的局限性。

Method: 构建基于新出版小说（晚于测试LLM发布日期）的数据集，提出11条件认知评估框架，测试不同认知表征方法（如语言特征、概念映射、基于档案的信息）在作者风格模仿任务中的效果。

Result: 结果显示概念和语言特征的组合在个体化认知模拟中特别有效，在整体评估中优于基于静态档案的线索。LLM在模仿语言风格方面比叙事结构更有效，突显了其在更深层次认知模拟中的局限性。

Conclusion: 这些发现为开发适应个体思维和表达方式的AI系统奠定了基础，推动了更个性化和人类对齐的创意技术的发展。

Abstract: Individualized cognitive simulation (ICS) aims to build computational models
that approximate the thought processes of specific individuals. While large
language models (LLMs) convincingly mimic surface-level human behavior such as
role-play, their ability to simulate deeper individualized cognitive processes
remains poorly understood. To address this gap, we introduce a novel task that
evaluates different cognitive representation methods in ICS. We construct a
dataset from recently published novels (later than the release date of the
tested LLMs) and propose an 11-condition cognitive evaluation framework to
benchmark seven off-the-shelf LLMs in the context of authorial style emulation.
We hypothesize that effective cognitive representations can help LLMs generate
storytelling that better mirrors the original author. Thus, we test different
cognitive representations, e.g., linguistic features, concept mappings, and
profile-based information. Results show that combining conceptual and
linguistic features is particularly effective in ICS, outperforming static
profile-based cues in overall evaluation. Importantly, LLMs are more effective
at mimicking linguistic style than narrative structure, underscoring their
limits in deeper cognitive simulation. These findings provide a foundation for
developing AI systems that adapt to individual ways of thinking and expression,
advancing more personalized and human-aligned creative technologies.

</details>


### [17] [Using Large Language Models for Abstraction of Planning Domains - Extended Version](https://arxiv.org/abs/2510.20258)
*Bita Banihashemi,Megh Patel,Yves Lespérance*

Main category: cs.AI

TL;DR: 使用大型语言模型通过上下文学习生成抽象PDDL领域和问题实例，以自然语言指定的抽象目标为基础，验证了GPT-4o在简单场景中能有效合成规划领域抽象。


<details>
  <summary>Details</summary>
Motivation: 动态领域的抽象生成对智能体的规划、推理和解释能力至关重要，但如何选择与目的对齐的抽象仍是一个挑战。

Method: 在PDDL中建模智能体具体行为，利用LLMs的上下文学习能力，根据自然语言指定的抽象目标生成抽象PDDL领域和问题实例，并通过符号验证工具和专家评估。

Result: GPT-4o在简单设置中通常能合成有用的规划领域抽象，但在动作抽象方面表现优于相关谓词的抽象。

Conclusion: LLMs在生成规划领域抽象方面具有潜力，特别是在动作抽象任务上表现良好，但在谓词抽象方面仍需改进。

Abstract: Generating an abstraction of a dynamic domain that aligns with a given
purpose remains a significant challenge given that the choice of such an
abstraction can impact an agent's ability to plan, reason, and provide
explanations effectively. We model the agent's concrete behaviors in PDDL and
investigate the use of in-context learning with large language models (LLMs)
for the generation of abstract PDDL domains and problem instances, given an
abstraction objective specified in natural language. The benchmark examples we
use are new and have not been part of the data any LLMs have been trained on.
We consider three categories of abstractions: abstraction of choice of
alternative concrete actions, abstraction of sequences of concrete actions, and
abstraction of action/predicate parameters, as well as combinations of these.
The generated abstract PDDL domains and problem instances are then checked by
symbolic validation tools as well as human experts. Our experiments show that
GPT-4o can generally synthesize useful planning domain abstractions in simple
settings, although it is better at abstracting over actions than over the
associated fluents.

</details>


### [18] [Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction](https://arxiv.org/abs/2510.20275)
*Yunzhi Liu,Haokai Tan,Rushi Kanjaria,Lihuan Li,Flora D. Salim*

Main category: cs.AI

TL;DR: STaBERT模型通过整合POI语义信息和时间描述符，显著提升了人类移动性预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型要么只建模位置序列，要么仅将时间信息作为辅助输入，未能充分利用兴趣点(POI)提供的丰富语义上下文。

Method: 提出STaBERT模型，在BERT基础上融合POI嵌入和时间描述符，构建统一的语义增强移动性表示。

Result: 单城市预测的GEO-BLEU得分从0.34提升到0.75；多城市预测从0.34提升到0.56。

Conclusion: 整合POI语义和时间信息能显著改善人类移动性预测性能。

Abstract: Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.

</details>


### [19] [Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation](https://arxiv.org/abs/2510.20310)
*Mingliang Zhai,Hansheng Liang,Xiaomeng Fan,Zhi Gao,Chuanhao Li,Che Sun,Xu Bin,Yuwei Wu,Yunde Jia*

Main category: cs.AI

TL;DR: ToolEQA是一个集成外部工具和多步推理的EQA智能体，通过工具提供有用信息来改进探索方向，从而用更短探索距离生成更准确回答。


<details>
  <summary>Details</summary>
Motivation: 现有EQA方法直接使用视觉语言模型探索环境而不进行显式思考或规划，限制了推理能力，导致探索效率低下和回答无效。

Method: 设计ToolEQA智能体，集成外部工具与多步推理；开发自动生成EQA任务的数据生成管道，构建包含18K任务的EQA-RT数据集。

Result: 在EQA-RT-Seen和EQA-RT-Unseen测试集上，ToolEQA比最先进基线方法成功率提升9.2~20.2%，比零样本版本高10%。在HM-EQA、OpenEQA和EXPRESS-Bench数据集上也达到最先进性能。

Conclusion: ToolEQA通过集成外部工具和多步推理，显著提升了EQA任务的性能和效率，展现了良好的泛化能力。

Abstract: Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.

</details>


### [20] [Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems](https://arxiv.org/abs/2510.20332)
*Anna Arias-Duart,Maria Eugenia Cardello,Atia Cortés*

Main category: cs.AI

TL;DR: 该论文基于AI4HealthyAging项目经验，识别了临床数据收集中存在的多种偏见类型，并提出了改善AI系统公平性和鲁棒性的实用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在医疗领域有巨大潜力，但由于训练数据的质量和公平性问题，AI解决方案在真实临床实践中的整合仍然有限。数据收集过程中的偏见是主要障碍。

Method: 基于西班牙国家研发计划AI4HealthyAging项目的实践经验，通过检测临床数据收集过程中的偏见，识别了历史偏见、代表性偏见和测量偏见等多种偏见类型。

Result: 在多个用例中识别出在性别、年龄、居住地、社会经济地位、设备和标签等变量上存在的各种偏见表现形式。

Conclusion: 提出了改善临床问题设计和数据收集公平性和鲁棒性的实用建议，希望这些发现和经验能为未来开发更公平的医疗AI系统提供指导。

Abstract: Artificial intelligence (AI) holds great promise for transforming healthcare.
However, despite significant advances, the integration of AI solutions into
real-world clinical practice remains limited. A major barrier is the quality
and fairness of training data, which is often compromised by biased data
collection practices. This paper draws on insights from the AI4HealthyAging
project, part of Spain's national R&D initiative, where our task was to detect
biases during clinical data collection. We identify several types of bias
across multiple use cases, including historical, representation, and
measurement biases. These biases manifest in variables such as sex, gender,
age, habitat, socioeconomic status, equipment, and labeling. We conclude with
practical recommendations for improving the fairness and robustness of clinical
problem design and data collection. We hope that our findings and experience
contribute to guiding future projects in the development of fairer AI systems
in healthcare.

</details>


### [21] [Collateral Damage Assessment Model for AI System Target Engagement in Military Operations](https://arxiv.org/abs/2510.20337)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 提出了一种用于军事行动中AI系统目标打击的附带损害评估模型，该模型在知识表示与推理架构中整合了时间、空间和力量维度，通过分层结构捕获AI系统的类别、架构组件、打击向量和上下文方面。


<details>
  <summary>Details</summary>
Motivation: 在AI系统在战场中作用日益重要的时代，确保负责任的目标打击需要对潜在附带效应进行严格评估。

Method: 采用设计科学方法论，构建统一的知识表示与推理架构，整合时间、空间和力量维度，考虑传播、严重性、可能性和评估指标，通过实例化进行演示和评估。

Result: 开发了一个分层结构模型，能够清晰表示并增强透明推理机制，为构建负责任和可信赖的智能系统奠定了基础。

Conclusion: 该模型为评估军事行动中打击AI系统产生的效应提供了基础，有助于构建负责任和可信赖的智能系统。

Abstract: In an era where AI (Artificial Intelligence) systems play an increasing role
in the battlefield, ensuring responsible targeting demands rigorous assessment
of potential collateral effects. In this context, a novel collateral damage
assessment model for target engagement of AI systems in military operations is
introduced. The model integrates temporal, spatial, and force dimensions within
a unified Knowledge Representation and Reasoning (KRR) architecture following a
design science methodological approach. Its layered structure captures the
categories and architectural components of the AI systems to be engaged
together with corresponding engaging vectors and contextual aspects. At the
same time, spreading, severity, likelihood, and evaluation metrics are
considered in order to provide a clear representation enhanced by transparent
reasoning mechanisms. Further, the model is demonstrated and evaluated through
instantiation which serves as a basis for further dedicated efforts that aim at
building responsible and trustworthy intelligent systems for assessing the
effects produced by engaging AI systems in military operations.

</details>


### [22] [LLM-empowered knowledge graph construction: A survey](https://arxiv.org/abs/2510.20345)
*Haonan Bian*

Main category: cs.AI

TL;DR: 本调查系统回顾了LLM赋能知识图谱构建的最新进展，分析了LLM如何重塑传统的本体工程、知识提取和知识融合三层流程，并探讨了基于模式和无模式两种构建范式的互补优势。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的出现，知识图谱构建正从基于规则和统计的流程转向语言驱动和生成式框架，需要系统梳理这一范式转变的技术进展。

Method: 从两个互补视角回顾LLM驱动方法：基于模式的范式强调结构、规范化和一致性；无模式范式强调灵活性、适应性和开放发现。

Result: 系统分析了各阶段的代表性框架、技术机制及其局限性，为理解LLM与知识图谱的协同演进提供了全面视角。

Conclusion: 展望了关键趋势和未来研究方向，包括基于KG的LLM推理、智能体系统的动态知识记忆以及多模态KG构建，旨在弥合符号知识工程与神经语义理解之间的鸿沟。

Abstract: Knowledge Graphs (KGs) have long served as a fundamental infrastructure for
structured knowledge representation and reasoning. With the advent of Large
Language Models (LLMs), the construction of KGs has entered a new
paradigm-shifting from rule-based and statistical pipelines to language-driven
and generative frameworks. This survey provides a comprehensive overview of
recent progress in LLM-empowered knowledge graph construction, systematically
analyzing how LLMs reshape the classical three-layered pipeline of ontology
engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual
foundations, and then review emerging LLM-driven approaches from two
complementary perspectives: schema-based paradigms, which emphasize structure,
normalization, and consistency; and schema-free paradigms, which highlight
flexibility, adaptability, and open discovery. Across each stage, we synthesize
representative frameworks, analyze their technical mechanisms, and identify
their limitations.
  Finally, the survey outlines key trends and future research directions,
including KG-based reasoning for LLMs, dynamic knowledge memory for agentic
systems, and multimodal KG construction. Through this systematic review, we aim
to clarify the evolving interplay between LLMs and knowledge graphs, bridging
symbolic knowledge engineering and neural semantic understanding toward the
development of adaptive, explainable, and intelligent knowledge systems.

</details>


### [23] [IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation](https://arxiv.org/abs/2510.20377)
*Tianyi Zhang,Florian Mai,Lucie Flek*

Main category: cs.AI

TL;DR: 提出了IKnow框架，通过指令-响应对话格式的自监督目标，在无需外部资源的情况下实现语言模型的持续预训练，避免指令跟随能力退化。


<details>
  <summary>Details</summary>
Motivation: 传统的持续预训练方法会降低指令调优模型的指令跟随能力和语义表示，且现有解决方案需要访问原始基础模型或外部领域知识库，这在基础模型权重因安全原因被保留或缺乏可靠外部语料库的情况下不现实。

Method: IKnow框架将自监督目标重新表述为指令-响应对话格式，利用文本本身嵌入的领域知识，在更深语义层次进行编码学习。

Result: 该方法不依赖外部资源，能够有效保持模型的指令跟随能力，同时实现领域知识的持续适应。

Conclusion: IKnow提供了一个简单通用的持续预训练框架，在无需外部依赖的情况下解决了指令调优模型在持续学习中的退化问题。

Abstract: Continual pretraining promises to adapt large language models (LLMs) to new
domains using only unlabeled test-time data, but naively applying standard
self-supervised objectives to instruction-tuned models is known to degrade
their instruction-following capability and semantic representations. Existing
fixes assume access to the original base model or rely on knowledge from an
external domain-specific database - both of which pose a realistic barrier in
settings where the base model weights are withheld for safety reasons or
reliable external corpora are unavailable. In this work, we propose
Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general
framework that formulates novel self-supervised objectives in the
instruction-response dialogue format. Rather than depend- ing on external
resources, IKnow leverages domain knowledge embedded within the text itself and
learns to encode it at a deeper semantic level.

</details>


### [24] [A computational model and tool for generating more novel opportunities in professional innovation processes](https://arxiv.org/abs/2510.20402)
*Neil Maiden,Konstantinos Zachos,James Lockerbie,Kostas Petrianakis,Amanda Brown*

Main category: cs.AI

TL;DR: 提出了一种基于创造力理论的计算模型，用于生成更具新颖性的创新机会，在酒店业创新项目中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的创新机会生成方法在保持实用性的同时难以产生足够新颖的方案，需要开发专门的计算模型来平衡新颖性和实用性。

Method: 开发了包含五个功能模块的计算模型，这些功能旨在协同工作以生成既新颖又有用的创新机会，并在酒店业创新项目中进行测试。

Result: 该计算模型生成的创新机会在新颖性和/或实用性方面优于Notebook LM和ChatGPT4o，但并非所有功能模块都对提升新颖性有贡献。

Conclusion: 该计算模型在生成创新机会方面表现出色，但需要进一步优化功能模块以提高新颖性生成能力。

Abstract: This paper presents a new computational model of creative outcomes, informed
by creativity theories and techniques, which was implemented to generate more
novel opportunities for innovation projects. The model implemented five
functions that were developed to contribute to the generation of innovation
opportunities with higher novelty without loss of usefulness. The model was
evaluated using opportunities generated for an innovation project in the
hospitality sector. The evaluation revealed that the computational model
generated outcomes that were more novel and/or useful than outcomes from
Notebook LM and ChatGPT4o. However, not all model functions contributed to the
generation of more novel opportunities, leading to new directions for further
model development

</details>


### [25] [Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$](https://arxiv.org/abs/2510.20457)
*Louis Mozart Kamdem Teyou,Luke Friedrichs,N'Dah Jean Kouagou,Caglar Demir,Yasir Mahmood,Stefan Heindorf,Axel-Cyrille Ngonga Ngomo*

Main category: cs.AI

TL;DR: 提出一种名为EBR的神经推理器，使用嵌入来近似符号推理器的结果，解决了传统描述逻辑推理器对不一致和错误数据不鲁棒的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号概念学习方法大多依赖描述逻辑推理器，但这些推理器对知识库中的不一致和错误数据不鲁棒，限制了在实际知识库中的应用。

Method: EBR神经推理器利用嵌入来近似符号推理器的结果，仅需要检索原子概念和存在限制的实例，就能检索或近似SHOIQ描述逻辑中任何概念的实例集。

Result: 实验表明，EBR在缺失和错误数据的情况下表现鲁棒，优于现有推理器。

Conclusion: EBR神经推理器提供了一种鲁棒的推理方法，能够处理现实世界知识库中的不一致和错误数据问题。

Abstract: Concept learning exploits background knowledge in the form of description
logic axioms to learn explainable classification models from knowledge bases.
Despite recent breakthroughs in neuro-symbolic concept learning, most
approaches still cannot be deployed on real-world knowledge bases. This is due
to their use of description logic reasoners, which are not robust against
inconsistencies nor erroneous data. We address this challenge by presenting a
novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to
approximate the results of a symbolic reasoner. We show that EBR solely
requires retrieving instances for atomic concepts and existential restrictions
to retrieve or approximate the set of instances of any concept in the
description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with
state-of-the-art reasoners. Our results suggest that EBR is robust against
missing and erroneous data in contrast to existing reasoners.

</details>


### [26] [FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic](https://arxiv.org/abs/2510.20467)
*Yiwen Peng,Thomas Bonald,Fabian M. Suchanek*

Main category: cs.AI

TL;DR: FLORA是一种基于模糊逻辑的知识图谱对齐方法，无需训练数据，可同时对齐实体和关系，并提供可解释的结果。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱对齐方法主要关注实体级对齐，缺乏可解释性且需要训练数据。

Method: 使用模糊逻辑进行迭代式整体对齐，支持悬空实体，并具有可证明的收敛性。

Result: 在主要基准测试中达到了最先进的性能。

Conclusion: FLORA是一种简单有效的无监督知识图谱对齐方法，具有可解释性和高精度。

Abstract: Knowledge graph alignment is the task of matching equivalent entities (that
is, instances and classes) and relations across two knowledge graphs. Most
existing methods focus on pure entity-level alignment, computing the similarity
of entities in some embedding space. They lack interpretable reasoning and need
training data to work. In this paper, we propose FLORA, a simple yet effective
method that (1) is unsupervised, i.e., does not require training data, (2)
provides a holistic alignment for entities and relations iteratively, (3) is
based on fuzzy logic and thus delivers interpretable results, (4) provably
converges, (5) allows dangling entities, i.e., entities without a counterpart
in the other KG, and (6) achieves state-of-the-art results on major benchmarks.

</details>


### [27] [Lost in Translation: Policymakers are not really listening to Citizen Concerns about AI](https://arxiv.org/abs/2510.20568)
*Susan Ariel Aaronson,Michael Moreno*

Main category: cs.AI

TL;DR: 该研究比较了澳大利亚、哥伦比亚和美国三个国家在AI治理中的公众参与情况，发现政府未能建立有效的公众对话机制，参与率低且反馈响应不足，导致参与式AI治理的承诺与实践之间存在差距。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨政府在AI治理中如何收集和回应公众意见，以及这种参与过程是否能够建立公众对AI及其治理的信任。

Method: 采用景观分析方法，比较三个国家（澳大利亚、哥伦比亚、美国）征集AI风险和政策反馈的方式，以及这些反馈如何影响治理决策。

Result: 研究发现：1）三国参与率均低于人口1%；2）政府缺乏吸引多样化声音的努力；3）官员对反馈的响应有限；4）未能建立有效的反馈循环机制。

Conclusion: 当前参与式AI治理方法难以建立信任和合法性，因为决策者未能充分倾听和回应公众关切。作者提出8条改进建议，包括提升AI素养、扩大参与范围、使用创新参与方法等。

Abstract: The worlds people have strong opinions about artificial intelligence (AI),
and they want policymakers to listen. Governments are inviting public comment
on AI, but as they translate input into policy, much of what citizens say is
lost. Policymakers are missing a critical opportunity to build trust in AI and
its governance. This paper compares three countries, Australia, Colombia, and
the United States, that invited citizens to comment on AI risks and policies.
Using a landscape analysis, the authors examined how each government solicited
feedback and whether that input shaped governance. Yet in none of the three
cases did citizens and policymakers establish a meaningful dialogue.
Governments did little to attract diverse voices or publicize calls for
comment, leaving most citizens unaware or unprepared to respond. In each
nation, fewer than one percent of the population participated. Moreover,
officials showed limited responsiveness to the feedback they received, failing
to create an effective feedback loop. The study finds a persistent gap between
the promise and practice of participatory AI governance. The authors conclude
that current approaches are unlikely to build trust or legitimacy in AI because
policymakers are not adequately listening or responding to public concerns.
They offer eight recommendations: promote AI literacy; monitor public feedback;
broaden outreach; hold regular online forums; use innovative engagement
methods; include underrepresented groups; respond publicly to input; and make
participation easier.

</details>


### [28] [Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting](https://arxiv.org/abs/2510.20591)
*Ali Rajaei,Peter Palensky,Jochen L. Cremer*

Main category: cs.AI

TL;DR: 该论文提出了一种图神经网络加速的网络拓扑优化方法，用于缓解输电网拥堵和降低再调度成本，实现了比现有求解器快4个数量级的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有求解器无法在近实时内解决大规模系统的混合整数非线性网络拓扑优化问题，而传统机器学习方法在泛化到未见拓扑、不同运行条件和系统方面存在局限性。

Method: 开发了异构边缘感知消息传递神经网络来预测有效的母线分裂动作作为候选拓扑优化解，该方法考虑了线性化交流潮流，能够捕捉局部流量模式。

Result: 在GOC 2000总线系统上实现了高达4个数量级的加速，在一分钟内提供交流可行解，最优性差距仅为2.3%。

Conclusion: 该方法在拓扑和跨系统泛化方面取得了显著进展，为实现大规模系统的近实时网络拓扑优化迈出了重要一步。

Abstract: Network topology optimization (NTO) via busbar splitting can mitigate
transmission grid congestion and reduce redispatch costs. However, solving this
mixed-integer non-linear problem for large-scale systems in near-real-time is
currently intractable with existing solvers. Machine learning (ML) approaches
have emerged as a promising alternative, but they have limited generalization
to unseen topologies, varying operating conditions, and different systems,
which limits their practical applicability. This paper formulates NTO for
congestion management problem considering linearized AC PF, and proposes a
graph neural network (GNN)-accelerated approach. We develop a heterogeneous
edge-aware message passing NN to predict effective busbar splitting actions as
candidate NTO solutions. The proposed GNN captures local flow patterns,
achieves generalization to unseen topology changes, and improves
transferability across systems. Case studies show up to 4 orders-of-magnitude
speed-up, delivering AC-feasible solutions within one minute and a 2.3%
optimality gap on the GOC 2000-bus system. These results demonstrate a
significant step toward near-real-time NTO for large-scale systems with
topology and cross-system generalization.

</details>


### [29] [What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation](https://arxiv.org/abs/2510.20603)
*Heejin Do,Jaehui Hwang,Dongyoon Han,Seong Joon Oh,Sangdoo Yun*

Main category: cs.AI

TL;DR: 提出CaSE方法，通过评估推理步骤的相关性和连贯性来改进LLM推理能力，而不仅仅是检查最终答案的正确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法只评估最终答案正确性，忽略了推理过程的质量，无法为模型改进提供细粒度信号。

Method: 引入因果逐步评估(CaSE)方法，将推理质量分解为相关性和连贯性两个维度，仅使用前文信息评估每个推理步骤以避免后见之明偏差。

Result: 在MRa-GSM8K和MRa-MATH基准上验证了CaSE与人工评估的一致性，使用CaSE评估的数据进行训练能直接提升最终任务性能。

Conclusion: CaSE提供了一个可扩展的框架来分析、调试和改进LLM推理，证明了超越有效性检查的实际价值。

Abstract: Evaluating large language models (LLMs) on final-answer correctness is the
dominant paradigm. This approach, however, provides a coarse signal for model
improvement and overlooks the quality of the underlying reasoning process. We
argue that a more granular evaluation of reasoning offers a more effective path
to building robust models. We decompose reasoning quality into two dimensions:
relevance and coherence. Relevance measures if a step is grounded in the
problem; coherence measures if it follows logically from prior steps. To
measure these aspects reliably, we introduce causal stepwise evaluation (CaSE).
This method assesses each reasoning step using only its preceding context,
which avoids hindsight bias. We validate CaSE against human judgments on our
new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we
show that curating training data with CaSE-evaluated relevance and coherence
directly improves final task performance. Our work provides a scalable
framework for analyzing, debugging, and improving LLM reasoning, demonstrating
the practical value of moving beyond validity checks.

</details>


### [30] [Efficient Algorithms for Computing Random Walk Centrality](https://arxiv.org/abs/2510.20604)
*Changan Liu,Zixuan Xie,Ahad N. Zehmakan,Zhongzhi Zhang*

Main category: cs.AI

TL;DR: 提出了两种可扩展的随机游走中心性算法，使用近似Cholesky分解和生成树采样，在近线性时间内提供强近似保证


<details>
  <summary>Details</summary>
Motivation: 随机游走中心性能够捕捉丰富的图结构信息，但在大型网络中计算现有方法计算成本过高

Method: 基于新公式化的随机游走中心性，开发了两种算法：一种利用近似Cholesky分解和稀疏逆估计，另一种通过采样根生成树

Result: 在包含超过1000万个节点的大型真实网络上的实验证明了所提算法的效率和近似质量

Conclusion: 提出的算法能够高效计算随机游走中心性，为大规模图分析提供了实用解决方案

Abstract: Random walk centrality is a fundamental metric in graph mining for
quantifying node importance and influence, defined as the weighted average of
hitting times to a node from all other nodes. Despite its ability to capture
rich graph structural information and its wide range of applications, computing
this measure for large networks remains impractical due to the computational
demands of existing methods. In this paper, we present a novel formulation of
random walk centrality, underpinning two scalable algorithms: one leveraging
approximate Cholesky factorization and sparse inverse estimation, while the
other sampling rooted spanning trees. Both algorithms operate in near-linear
time and provide strong approximation guarantees. Extensive experiments on
large real-world networks, including one with over 10 million nodes,
demonstrate the efficiency and approximation quality of the proposed
algorithms.

</details>


### [31] [Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms](https://arxiv.org/abs/2510.20621)
*Riccardo Guidotti,Martina Cinquini,Marta Marchiori Manerba,Mattia Setzu,Francesco Spinnato*

Main category: cs.AI

TL;DR: MIMOSA框架是一个可解释性优先的预测模型生成方法，在保持性能的同时嵌入因果性、公平性和隐私性等关键伦理属性。


<details>
  <summary>Details</summary>
Motivation: 需要构建可信赖的AI系统，在真实应用中促进信任、问责和安全采用自动化决策模型。

Method: 形式化定义了监督学习设置，涵盖表格数据、时间序列、图像、文本等多种数据类型，并分析了特征重要性、规则和实例三类可解释模型家族。

Result: 建立了可解释模型与伦理属性之间的理论联系，提供了形式化定义、评估指标和验证程序。

Conclusion: 该框架为开发准确、可解释、公平、隐私保护和因果感知的可信赖AI系统奠定了理论基础。

Abstract: Interpretable-by-design models are crucial for fostering trust,
accountability, and safe adoption of automated decision-making models in
real-world applications. In this paper we formalize the ground for the MIMOSA
(Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a
comprehensive methodology for generating predictive models that balance
interpretability with performance while embedding key ethical properties. We
formally define here the supervised learning setting across diverse
decision-making tasks and data types, including tabular data, time series,
images, text, transactions, and trajectories. We characterize three major
families of interpretable models: feature importance, rule, and instance based
models. For each family, we analyze their interpretability dimensions,
reasoning mechanisms, and complexity. Beyond interpretability, we formalize
three critical ethical properties, namely causality, fairness, and privacy,
providing formal definitions, evaluation metrics, and verification procedures
for each. We then examine the inherent trade-offs between these properties and
discuss how privacy requirements, fairness constraints, and causal reasoning
can be embedded within interpretable pipelines. By evaluating ethical measures
during model generation, this framework establishes the theoretical foundations
for developing AI systems that are not only accurate and interpretable but also
fair, privacy-preserving, and causally aware, i.e., trustworthy.

</details>


### [32] [Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications](https://arxiv.org/abs/2510.20632)
*Shuyi Xie,Ziqin Liew,Hailing Zhang,Haibo Zhang,Ling Hu,Zhiqiang Zhou,Shuman Liu,Anxiang Zeng*

Main category: cs.AI

TL;DR: EcomEval是一个全面的多语言多模态基准测试，用于评估LLM在电子商务领域的表现，解决了现有评估工具在任务多样性、模态覆盖和数据真实性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有电子商务评估基准存在任务多样性有限、缺乏多模态数据、使用合成数据以及语言覆盖狭窄等问题，无法可靠评估模型在复杂真实购物场景中的表现。

Method: 采用半自动流程，首先由大模型生成候选回答，然后由50多名具有电子商务和多语言专业知识的专家标注者进行审查和修改，确保参考答案的质量和可扩展性。

Result: EcomEval涵盖6个类别和37个任务（包括8个多模态任务），主要来源于真实客户查询和交易日志，反映了真实业务交互的噪声和异构性质。

Conclusion: EcomEval提供了一个全面的多语言多模态基准测试，能够对LLM在电子商务领域进行挑战导向和细粒度评估，特别覆盖了5种低资源东南亚语言。

Abstract: Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet
their capabilities in specialized domains remain underexplored. In e-commerce,
existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping
MMLU-suffer from limited task diversity (e.g., lacking product guidance and
after-sales issues), limited task modalities (e.g., absence of multimodal
data), synthetic or curated data, and a narrow focus on English and Chinese,
leaving practitioners without reliable tools to assess models on complex,
real-world shopping scenarios. We introduce EcomEval, a comprehensive
multilingual and multimodal benchmark for evaluating LLMs in e-commerce.
EcomEval covers six categories and 37 tasks (including 8 multimodal tasks),
sourced primarily from authentic customer queries and transaction logs,
reflecting the noisy and heterogeneous nature of real business interactions. To
ensure both quality and scalability of reference answers, we adopt a
semi-automatic pipeline in which large models draft candidate responses
subsequently reviewed and modified by over 50 expert annotators with strong
e-commerce and multilingual expertise. We define difficulty levels for each
question and task category by averaging evaluation scores across models with
different sizes and capabilities, enabling challenge-oriented and fine-grained
assessment. EcomEval also spans seven languages-including five low-resource
Southeast Asian languages-offering a multilingual perspective absent from prior
work.

</details>


### [33] [Fluidity Index: Next-Generation Super-intelligence Benchmarks](https://arxiv.org/abs/2510.20636)
*Eric Ngoiya,Tianshu Bao*

Main category: cs.AI

TL;DR: 本文提出了流动性指数(FI)来量化模型在动态扩展环境中的适应性，通过评估初始、当前和未来环境状态偏差来衡量响应准确性，区分闭式和开放式基准测试。


<details>
  <summary>Details</summary>
Motivation: 需要量化模型在动态扩展环境中的适应能力，评估模型对状态变化的理解、预测和调整能力，为超级智能模型设定适应性标准。

Method: 引入流动性指数(FI)基准测试，基于环境状态偏差评估响应准确性，区分闭式和开放式基准测试，重点关注闭环开放式真实世界基准测试。

Result: 建立了量化模型适应性的评估框架，能够测量模型在动态环境中的上下文切换和连续性保持能力。

Conclusion: 真正超级智能的模型应至少具备二阶适应性，通过数字补充实现自我维持计算以达到最佳流动性。

Abstract: This paper introduces the Fluidity Index (FI) to quantify model adaptability
in dynamic, scaling environments. The benchmark evaluates response accuracy
based on deviations in initial, current, and future environment states,
assessing context switching and continuity. We distinguish between closed-ended
and open-ended benchmarks, prioritizing closed-loop open-ended real-world
benchmarks to test adaptability. The approach measures a model's ability to
understand, predict, and adjust to state changes in scaling environments. A
truly super-intelligent model should exhibit at least second-order
adaptability, enabling self-sustained computation through digital replenishment
for optimal fluidity.

</details>


### [34] [Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges](https://arxiv.org/abs/2510.20641)
*Andrea Agiollo,Andrea Omicini*

Main category: cs.AI

TL;DR: 本文对将机器学习集成到理性智能体架构中的方法进行了系统化分析，特别关注BDI（信念-欲望-意图）范式，识别了关键研究机会和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型在感知和认知任务中展现出类人能力，将其集成到理性智能体架构中的框架日益受到关注，但现有研究缺乏系统性和一致性。

Method: 使用BDI范式作为参考框架，对现有方法进行细粒度系统化分析。

Result: 分析揭示了增强理性智能体的机器学习文献快速发展，并识别了关键研究方向。

Conclusion: 提出了设计有效理性机器学习智能体的关键研究机会和开放挑战。

Abstract: Thanks to the remarkable human-like capabilities of machine learning (ML)
models in perceptual and cognitive tasks, frameworks integrating ML within
rational agent architectures are gaining traction. Yet, the landscape remains
fragmented and incoherent, often focusing on embedding ML into generic agent
containers while overlooking the expressive power of rational
architectures--such as Belief-Desire-Intention (BDI) agents. This paper
presents a fine-grained systematisation of existing approaches, using the BDI
paradigm as a reference. Our analysis illustrates the fast-evolving literature
on rational agents enhanced by ML, and identifies key research opportunities
and open challenges for designing effective rational ML agents.

</details>


### [35] [The Shape of Reasoning: Topological Analysis of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2510.20665)
*Xue Wen Tan,Nathaniel Tan,Galen Lee,Stanley Kok*

Main category: cs.AI

TL;DR: 提出基于拓扑数据分析的框架来自动评估大语言模型推理轨迹的质量，相比传统图指标有更高预测能力


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型推理轨迹质量的方法依赖专家标注，费时费力且不可靠，需要自动化的评估框架

Method: 使用拓扑数据分析方法捕捉推理轨迹的几何结构，通过拓扑特征进行自动化评估

Result: 拓扑特征在评估推理质量方面比标准图指标具有显著更高的预测能力，表明有效推理更适合用高维几何结构而非纯关系图来捕捉

Conclusion: 紧凑且稳定的拓扑特征集能可靠指示轨迹质量，为未来强化学习算法提供了实用的质量信号

Abstract: Evaluating the quality of reasoning traces from large language models remains
understudied, labor-intensive, and unreliable: current practice relies on
expert rubrics, manual annotation, and slow pairwise judgments. Automated
efforts are dominated by graph-based proxies that quantify structural
connectivity but do not clarify what constitutes high-quality reasoning; such
abstractions can be overly simplistic for inherently complex processes. We
introduce a topological data analysis (TDA)-based evaluation framework that
captures the geometry of reasoning traces and enables label-efficient,
automated assessment. In our empirical study, topological features yield
substantially higher predictive power for assessing reasoning quality than
standard graph metrics, suggesting that effective reasoning is better captured
by higher-dimensional geometric structures rather than purely relational
graphs. We further show that a compact, stable set of topological features
reliably indicates trace quality, offering a practical signal for future
reinforcement learning algorithms.

</details>


### [36] [Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs](https://arxiv.org/abs/2510.20691)
*Yanlin Song,Ben Liu,Víctor Gutiérrez-Basulto,Zhiwei Hu,Qianqian Xie,Min Peng,Sophia Ananiadou,Jeff Z. Pan*

Main category: cs.AI

TL;DR: Graph-RFT是一个两阶段强化微调的KGQA框架，通过'规划-KG搜索-网络搜索-思考'范式，让LLM在不完整知识条件下自主规划并自适应检索KG和网络资源。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA方法难以充分利用KG中的丰富知识和LLM的推理能力，特别是在复杂场景下。它们通常假设KG覆盖完整，缺乏判断何时需要外部信息的机制，且推理过程局部短视，无法保持连贯的多步规划。

Method: Graph-RFT采用两阶段方法：1）链式思维微调方法，使用定制的规划-检索数据集激活结构化推理；2）规划-检索引导的强化学习过程，整合显式规划和检索动作，采用多奖励设计实现覆盖感知的检索调度。

Result: 该方法能够将复杂问题分解为有序子问题，使用逻辑表达式指导工具调用，实现全局一致的多步推理，并有效结合KG和网络检索。

Conclusion: Graph-RFT通过自主规划和自适应检索调度，解决了现有KGQA方法在复杂场景下的推理失败问题，提升了在不完整知识条件下的问答性能。

Abstract: Knowledge Graph Question Answering aims to answer natural language questions
by reasoning over structured knowledge graphs. While large language models have
advanced KGQA through their strong reasoning capabilities, existing methods
continue to struggle to fully exploit both the rich knowledge encoded in KGs
and the reasoning capabilities of LLMs, particularly in complex scenarios. They
often assume complete KG coverage and lack mechanisms to judge when external
information is needed, and their reasoning remains locally myopic, failing to
maintain coherent multi-step planning, leading to reasoning failures even when
relevant knowledge exists. We propose Graph-RFT, a novel two-stage
reinforcement fine-tuning KGQA framework with a
'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to
perform autonomous planning and adaptive retrieval scheduling across KG and web
sources under incomplete knowledge conditions. Graph-RFT introduces a
chain-of-thought fine-tuning method with a customized plan-retrieval dataset
activates structured reasoning and resolves the GRPO cold-start problem. It
then introduces a novel plan-retrieval guided reinforcement learning process
integrates explicit planning and retrieval actions with a multi-reward design,
enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired
planning module to decompose complex questions into ordered subquestions, and
logical expression to guide tool invocation for globally consistent multi-step
reasoning. This reasoning retrieval process is optimized with a multi-reward
combining outcome and retrieval specific signals, enabling the model to learn
when and how to combine KG and web retrieval effectively.

</details>


### [37] [A Coherence-Based Measure of AGI](https://arxiv.org/abs/2510.20784)
*Fares Fourati*

Main category: cs.AI

TL;DR: 本文提出了一种基于广义均值积分的AGI一致性度量方法，替代了传统算术平均值的补偿性假设，强调跨认知领域的平衡能力。


<details>
  <summary>Details</summary>
Motivation: 现有AGI定义使用算术平均值，假设领域间能力可以相互补偿，但这不能反映真正的通用智能所需的跨领域平衡能力。

Method: 提出基于广义均值积分的AGI一致性度量，通过连续补偿性指数覆盖算术、几何和调和均值，用曲线下面积(AUC)量化不同补偿性假设下的鲁棒性。

Result: 应用该方法评估GPT-4和GPT-5的CHC领域得分，发现尽管算术得分较高(GPT-5达24%)，但一致性调整的AUC显示两者距离真正的通用能力仍有很大差距。

Conclusion: 广义均值积分提供了一个原则性、可解释且更严格的AGI度量基础，能够更好地衡量向真正通用智能的进展。

Abstract: Recent work by \citet{hendrycks2025agidefinition} formalized
\textit{Artificial General Intelligence} (AGI) as the arithmetic mean of
proficiencies across cognitive domains derived from the Cattell--Horn--Carroll
(CHC) model of human cognition. While elegant, this definition assumes
\textit{compensability} -- that exceptional ability in some domains can offset
failure in others. True general intelligence, however, should reflect
\textit{coherent sufficiency}: balanced competence across all essential
domains. We propose a coherence-aware measure of AGI based on the integral of
generalized means over a continuum of compensability exponents. This
formulation spans arithmetic, geometric, and harmonic regimes, and the
resulting \textit{area under the curve} (AUC) quantifies robustness under
varying compensability assumptions. Unlike the arithmetic mean, which rewards
specialization, the AUC penalizes imbalance and captures inter-domain
dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,
the coherence-adjusted AUC reveals that both systems remain far from general
competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating
the generalized mean thus yields a principled, interpretable, and stricter
foundation for measuring genuine progress toward AGI.

</details>


### [38] [Real Deep Research for AI, Robotics and Beyond](https://arxiv.org/abs/2510.20809)
*Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang*

Main category: cs.AI

TL;DR: 提出了一个名为Real Deep Research (RDR)的可泛化分析管道，用于系统分析研究领域，识别新兴趋势和跨领域机会，特别应用于AI和机器人领域。


<details>
  <summary>Details</summary>
Motivation: AI和机器人领域每年产生超过10,000篇论文，研究人员难以跟上快速发展的趋势、跨学科工作和探索新领域的需求。

Method: 构建RDR管道，能够系统分析任何研究领域，识别新兴趋势，发现跨领域机会，并为新研究提供具体起点。

Result: 成功应用于AI和机器人领域，特别关注基础模型和机器人技术进展，并扩展到其他科学领域。

Conclusion: RDR框架为AI及其他领域的研究人员提供了有价值的分析工具，帮助他们在快速发展的研究环境中保持更新。

Abstract: With the rapid growth of research in AI and robotics now producing over
10,000 papers annually it has become increasingly difficult for researchers to
stay up to date. Fast evolving trends, the rise of interdisciplinary work, and
the need to explore domains beyond one's expertise all contribute to this
challenge. To address these issues, we propose a generalizable pipeline capable
of systematically analyzing any research area: identifying emerging trends,
uncovering cross domain opportunities, and offering concrete starting points
for new inquiry. In this work, we present Real Deep Research (RDR) a
comprehensive framework applied to the domains of AI and robotics, with a
particular focus on foundation models and robotics advancements. We also
briefly extend our analysis to other areas of science. The main paper details
the construction of the RDR pipeline, while the appendix provides extensive
results across each analyzed topic. We hope this work sheds light for
researchers working in the field of AI and beyond.

</details>
