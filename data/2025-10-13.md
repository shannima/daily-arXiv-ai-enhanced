<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 38]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Hypothesis Hunting with Evolving Networks of Autonomous Scientific Agents](https://arxiv.org/abs/2510.08619)
*Tennison Liu,Silas Ruhrberg Estévez,David L. Bentley,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 提出了AScience框架和ASCollab系统，使用LLM研究代理进行大规模科学假设探索，通过社会网络动态积累高质量、多样化、新颖的发现


<details>
  <summary>Details</summary>
Motivation: 大规模科学数据集为无特定研究问题的探索性发现创造了机会，需要支持假设狩猎过程

Method: 引入AScience框架，将发现建模为代理、网络和评估规范的交互，实现为ASCollab分布式系统，使用具有异质行为的LLM研究代理自组织成演化网络

Result: 实验显示社会动态能够沿多样性-质量-新颖性前沿积累专家评级结果，包括重新发现已建立的生物标志物、扩展已知通路和提出新治疗靶点

Conclusion: 虽然湿实验室验证仍然不可或缺，但在癌症队列上的实验表明，社会结构化的代理网络可以大规模持续进行探索性假设狩猎

Abstract: Large-scale scientific datasets -- spanning health biobanks, cell atlases,
Earth reanalyses, and more -- create opportunities for exploratory discovery
unconstrained by specific research questions. We term this process hypothesis
hunting: the cumulative search for insight through sustained exploration across
vast and complex hypothesis spaces. To support it, we introduce AScience, a
framework modeling discovery as the interaction of agents, networks, and
evaluation norms, and implement it as ASCollab, a distributed system of
LLM-based research agents with heterogeneous behaviors. These agents
self-organize into evolving networks, continually producing and peer-reviewing
findings under shared standards of evaluation. Experiments show that such
social dynamics enable the accumulation of expert-rated results along the
diversity-quality-novelty frontier, including rediscoveries of established
biomarkers, extensions of known pathways, and proposals of new therapeutic
targets. While wet-lab validation remains indispensable, our experiments on
cancer cohorts demonstrate that socially structured, agentic networks can
sustain exploratory hypothesis hunting at scale.

</details>


### [2] [Optimizing delivery for quick commerce factoring qualitative assessment of generated routes](https://arxiv.org/abs/2510.08671)
*Milon Bhattacharya,Milan Kumar*

Main category: cs.AI

TL;DR: 提出使用大语言模型来评估车辆路径规划生成的路由方案，解决印度电商最后一公里配送中因非结构化地址、不完整地图等导致的传统VRP方法局限性。


<details>
  <summary>Details</summary>
Motivation: 印度电商市场快速增长，最后一公里配送占运营成本近一半。传统VRP求解器在现实场景中因非结构化地址、不完整地图和计算限制而效果有限。

Method: 开发一个框架，使用大语言模型根据政策标准对VRP生成的路由进行批判性评估，帮助物流运营商评估和优先选择更高效的配送计划。

Result: 开源LLM识别路由问题的准确率达79%，专有推理模型可达86%。基于LLM的VRP路由评估可超越传统距离和时间指标。

Conclusion: LLM评估VRP生成的路由是一种有效且可扩展的评估层，对提高成本效率、配送可靠性和可持续性具有重要意义，尤其适用于印度等发展中国家。

Abstract: Indias e-commerce market is projected to grow rapidly, with last-mile
delivery accounting for nearly half of operational expenses. Although vehicle
routing problem (VRP) based solvers are widely used for delivery planning,
their effectiveness in real-world scenarios is limited due to unstructured
addresses, incomplete maps, and computational constraints in distance
estimation. This study proposes a framework that employs large language models
(LLMs) to critique VRP-generated routes against policy-based criteria, allowing
logistics operators to evaluate and prioritise more efficient delivery plans.
As a illustration of our approach we generate, annotate and evaluated 400 cases
using large language models. Our study found that open-source LLMs identified
routing issues with 79% accuracy, while proprietary reasoning models achieved
reach upto 86%. The results demonstrate that LLM-based evaluation of
VRP-generated routes can be an effective and scalable layer of evaluation which
goes beyond beyond conventional distance and time based metrics. This has
implications for improving cost efficiency, delivery reliability, and
sustainability in last-mile logistics, especially for developing countries like
India.

</details>


### [3] [Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation](https://arxiv.org/abs/2510.08713)
*Yifei Dong,Fengyi Wu,Guangyu Chen,Zhi-Qi Cheng,Qiyu Hu,Yuxuan Zhou,Jingdong Sun,Jun-Yan He,Qi Dai,Alexander G Hauptmann*

Main category: cs.AI

TL;DR: UniWM是一个统一的内存增强世界模型，将自我中心视觉预测和导航规划集成在单一多模态自回归骨干中，显著提升视觉导航性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有模块化架构中导航规划与视觉世界建模分离导致的状态-动作不对齐问题，以及在新颖或动态场景中的适应性限制。

Method: 提出统一的内存增强世界模型UniWM，通过分层记忆机制整合短期感知线索和长期轨迹上下文，在单一多模态自回归骨干中实现视觉预测与规划的紧密耦合。

Result: 在四个挑战性基准测试中，导航成功率提升高达30%，轨迹误差显著降低，并在未见的TartanDrive数据集上表现出优异的零样本泛化能力。

Conclusion: UniWM代表了向统一、想象力驱动的具身导航迈出的重要一步，证明了视觉预测与规划紧密集成的有效性。

Abstract: Enabling embodied agents to effectively imagine future states is critical for
robust and generalizable visual navigation. Current state-of-the-art
approaches, however, adopt modular architectures that separate navigation
planning from visual world modeling, leading to state-action misalignment and
limited adaptability in novel or dynamic scenarios. To overcome this
fundamental limitation, we propose UniWM, a unified, memory-augmented world
model integrating egocentric visual foresight and planning within a single
multimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly
grounds action decisions in visually imagined outcomes, ensuring tight
alignment between prediction and control. A hierarchical memory mechanism
further integrates detailed short-term perceptual cues with longer-term
trajectory context, enabling stable, coherent reasoning over extended horizons.
Extensive experiments across four challenging benchmarks (Go Stanford, ReCon,
SCAND, HuRoN) demonstrate that UniWM substantially improves navigation success
rates by up to 30%, significantly reduces trajectory errors compared to strong
baselines, and exhibits impressive zero-shot generalization on the unseen
TartanDrive dataset. These results highlight UniWM as a principled step toward
unified, imagination-driven embodied navigation.

</details>


### [4] [Robust Heuristic Algorithm Design with LLMs](https://arxiv.org/abs/2510.08755)
*Pantea Karimi,Dany Rouhana,Pooria Namyar,Siva Kesava Reddy Kakarla,Venkat Arun,Behnaz Arzani*

Main category: cs.AI

TL;DR: 通过向LLM提供启发式算法表现不佳的实例、解释原因并进行输入空间区域专业化设计，可以生成更鲁棒和性能更好的启发式算法。


<details>
  <summary>Details</summary>
Motivation: 现有使用LLM设计启发式算法的方法可能不够鲁棒，需要工具来解释启发式算法表现不佳的原因并提供改进建议。

Method: 采用三个简单策略：(1)向LLM展示启发式算法表现不佳的实例；(2)解释这些表现不佳的原因；(3)针对输入空间的不同区域进行专业化设计。

Result: 生成的启发式算法在最坏情况下性能比FunSearch提高约28倍，平均性能也有所提升，同时保持运行时间不变。

Conclusion: 通过向LLM提供失败案例分析和专业化设计，可以显著提升启发式算法的鲁棒性和性能。

Abstract: We posit that we can generate more robust and performant heuristics if we
augment approaches using LLMs for heuristic design with tools that explain why
heuristics underperform and suggestions about how to fix them. We find even
simple ideas that (1) expose the LLM to instances where the heuristic
underperforms; (2) explain why they occur; and (3) specialize design to regions
in the input space, can produce more robust algorithms compared to existing
techniques~ -- ~the heuristics we produce have a $\sim28\times$ better
worst-case performance compared to FunSearch, improve average performance, and
maintain the runtime.

</details>


### [5] [COMPASS: Enhancing Agent Long-Horizon Reasoning with Evolving Context](https://arxiv.org/abs/2510.08790)
*Guangya Wan,Mingyang Ling,Xiaoqi Ren,Rujun Han,Sheng Li,Zizhao Zhang*

Main category: cs.AI

TL;DR: COMPASS是一个轻量级分层框架，通过将战术执行、战略监督和上下文管理分离到三个专门组件来解决长时程任务中的上下文管理瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 长时程任务需要持续推理和多次工具交互，但LLM智能体容易因小错误累积、幻觉或失去连贯性而失败，上下文管理成为核心瓶颈。

Method: 提出COMPASS框架，包含三个组件：(1)主智能体执行推理和工具使用；(2)元思考者监控进度并发出战略干预；(3)上下文管理器维护简洁相关的进度简报。

Result: 在GAIA、BrowseComp和Humanity's Last Exam三个基准测试中，COMPASS相比单智能体和多智能体基线准确率提升高达20%。

Conclusion: COMPASS通过分层架构有效解决了长时程任务中的上下文管理问题，并通过测试时扩展和训练后流水线进一步提升了性能和效率。

Abstract: Long-horizon tasks that require sustained reasoning and multiple tool
interactions remain challenging for LLM agents: small errors compound across
steps, and even state-of-the-art models often hallucinate or lose coherence. We
identify context management as the central bottleneck -- extended histories
cause agents to overlook critical evidence or become distracted by irrelevant
information, thus failing to replan or reflect from previous mistakes. To
address this, we propose COMPASS (Context-Organized Multi-Agent Planning and
Strategy System), a lightweight hierarchical framework that separates tactical
execution, strategic oversight, and context organization into three specialized
components: (1) a Main Agent that performs reasoning and tool use, (2) a
Meta-Thinker that monitors progress and issues strategic interventions, and (3)
a Context Manager that maintains concise, relevant progress briefs for
different reasoning stages. Across three challenging benchmarks -- GAIA,
BrowseComp, and Humanity's Last Exam -- COMPASS improves accuracy by up to 20%
relative to both single- and multi-agent baselines. We further introduce a
test-time scaling extension that elevates performance to match established
DeepResearch agents, and a post-training pipeline that delegates context
management to smaller models for enhanced efficiency.

</details>


### [6] [Everyone prefers human writers, including AI](https://arxiv.org/abs/2510.08831)
*Wouter Haverals,Meredith Martin*

Main category: cs.AI

TL;DR: 本研究通过控制实验发现，人类和AI评估者在文学风格评价中都存在系统性的人类偏好偏见，且AI模型的偏见程度是人类的2.5倍。


<details>
  <summary>Details</summary>
Motivation: 随着AI写作工具的普及，需要理解人类和机器如何评估文学风格这一主观领域，特别是测量评估者之间的归因偏见。

Method: 使用Raymond Queneau的《风格练习》进行控制实验。研究1比较人类参与者(N=556)和AI模型(N=13)对Queneau作品与GPT-4生成版本的评估。研究2测试偏见在14×14矩阵的AI评估者和创作者之间的泛化。

Result: 人类显示+13.7个百分点偏见，AI模型显示+34.3个百分点偏见（2.5倍更强）。研究2确认这种偏见在所有AI架构中都存在(+25.8pp)，表明AI系统在内容被标记为"AI生成"时会系统性地贬低创意内容。

Conclusion: AI模型在训练过程中吸收了人类对人工创造力的文化偏见，不仅复制而且放大了人类的这种倾向，导致评估者基于感知的作者身份反转评估标准。

Abstract: As AI writing tools become widespread, we need to understand how both humans
and machines evaluate literary style, a domain where objective standards are
elusive and judgments are inherently subjective. We conducted controlled
experiments using Raymond Queneau's Exercises in Style (1947) to measure
attribution bias across evaluators. Study 1 compared human participants (N=556)
and AI models (N=13) evaluating literary passages from Queneau versus
GPT-4-generated versions under three conditions: blind, accurately labeled, and
counterfactually labeled. Study 2 tested bias generalization across a
14$\times$14 matrix of AI evaluators and creators. Both studies revealed
systematic pro-human attribution bias. Humans showed +13.7 percentage point
(pp) bias (Cohen's h = 0.28, 95% CI: 0.21-0.34), while AI models showed +34.3
percentage point bias (h = 0.70, 95% CI: 0.65-0.76), a 2.5-fold stronger effect
(P$<$0.001). Study 2 confirmed this bias operates across AI architectures
(+25.8pp, 95% CI: 24.1-27.6%), demonstrating that AI systems systematically
devalue creative content when labeled as "AI-generated" regardless of which AI
created it. We also find that attribution labels cause evaluators to invert
assessment criteria, with identical features receiving opposing evaluations
based solely on perceived authorship. This suggests AI models have absorbed
human cultural biases against artificial creativity during training. Our study
represents the first controlled comparison of attribution bias between human
and artificial evaluators in aesthetic judgment, revealing that AI systems not
only replicate but amplify this human tendency.

</details>


### [7] [What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action Alignment](https://arxiv.org/abs/2510.08847)
*Allison Sihan Jia,Daniel Huang,Nikhil Vytla,Nirvika Choudhury,John C Mitchell,Anupam Datta*

Main category: cs.AI

TL;DR: 提出了Agent GPA（目标-计划-行动）评估框架，包含五个评估指标，能够系统性地覆盖智能体失败情况，支持LLM评委与人工标注高度一致，并能准确定位错误。


<details>
  <summary>Details</summary>
Motivation: 需要一种系统性的评估范式来全面评估智能体在目标设定、计划制定和行动执行整个操作循环中的表现，覆盖各种类型的智能体失败。

Method: 基于智能体操作循环（目标-计划-行动）构建评估框架，包含五个评估指标：目标达成度、逻辑一致性、执行效率、计划质量和计划遵循度。

Result: 在两个基准数据集上的实验表明，该框架能覆盖TRAIL/GAIA基准数据集中的所有智能体错误，LLM评委与人工标注的一致性达到80%-95%以上，错误定位准确率达到86%。

Conclusion: Agent GPA框架为智能体评估提供了系统化的方法，能够全面覆盖智能体失败，支持高效的自动化评估，并为性能改进提供精准指导。

Abstract: We introduce the Agent GPA (Goal-Plan-Action) framework: an evaluation
paradigm based on an agent's operational loop of setting goals, devising plans,
and executing actions. The framework includes five evaluation metrics: Goal
Fulfillment, Logical Consistency, Execution Efficiency, Plan Quality, and Plan
Adherence. Logical Consistency checks that an agent's actions are consistent
with its prior actions. Execution Efficiency checks whether the agent executes
in the most efficient way to achieve its goal. Plan Quality checks whether an
agent's plans are aligned with its goals; Plan Adherence checks if an agent's
actions are aligned with its plan; and Goal Fulfillment checks that agent's
final outcomes match the stated goals. Our experimental results on two
benchmark datasets - the public TRAIL/GAIA dataset and an internal dataset for
a production-grade data agent - show that this framework (a) provides a
systematic way to cover a broad range of agent failures, including all agent
errors on the TRAIL/GAIA benchmark dataset; (b) supports LLM-judges that
exhibit strong agreement with human annotation, covering 80% to over 95%
errors; and (c) localizes errors with 86% agreement to enable targeted
improvement of agent performance.

</details>


### [8] [ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review](https://arxiv.org/abs/2510.08867)
*Gaurav Sahu,Hugo Larochelle,Laurent Charlin,Christopher Pal*

Main category: cs.AI

TL;DR: ReviewerToo是一个模块化框架，用于研究和部署AI辅助同行评审，通过系统化评估补充人类判断，在ICLR 2025数据集上达到81.8%的接受/拒绝分类准确率。


<details>
  <summary>Details</summary>
Motivation: 解决传统同行评审存在的不一致性、评审者主观性和可扩展性挑战，探索AI如何增强科学出版评审过程。

Method: 开发模块化框架支持系统化实验，使用专门的评审者角色和结构化评估标准，在ICLR 2025的1,963篇论文数据集上验证，采用gpt-oss-120b模型进行测试。

Result: AI评审者在接受/拒绝分类任务上达到81.8%准确率（人类平均83.9%），AI生成的评审质量被LLM评为高于人类平均水平，但在方法新颖性和理论贡献评估方面仍有不足。

Conclusion: AI可以增强评审的一致性、覆盖范围和公平性，但复杂评估仍需领域专家，提出了混合评审系统的集成指南。

Abstract: Peer review is the cornerstone of scientific publishing, yet it suffers from
inconsistencies, reviewer subjectivity, and scalability challenges. We
introduce ReviewerToo, a modular framework for studying and deploying
AI-assisted peer review to complement human judgment with systematic and
consistent assessments. ReviewerToo supports systematic experiments with
specialized reviewer personas and structured evaluation criteria, and can be
partially or fully integrated into real conference workflows. We validate
ReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR
2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy
for the task of categorizing a paper as accept/reject compared to 83.9% for the
average human reviewer. Additionally, ReviewerToo-generated reviews are rated
as higher quality than the human average by an LLM judge, though still trailing
the strongest expert contributions. Our analysis highlights domains where AI
reviewers excel (e.g., fact-checking, literature coverage) and where they
struggle (e.g., assessing methodological novelty and theoretical
contributions), underscoring the continued need for human expertise. Based on
these findings, we propose guidelines for integrating AI into peer-review
pipelines, showing how AI can enhance consistency, coverage, and fairness while
leaving complex evaluative judgments to domain experts. Our work provides a
foundation for systematic, hybrid peer-review systems that scale with the
growth of scientific publishing.

</details>


### [9] [GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare](https://arxiv.org/abs/2510.08872)
*Siqi Zhu,David Zhang,Pedro Cisneros-Velarde,Jiaxuan You*

Main category: cs.AI

TL;DR: 提出了GTAlign框架，将博弈论决策整合到LLM的推理和训练中，通过构建收益矩阵和互惠奖励机制，使模型能够选择对用户和自身都有利的响应。


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法假设最大化模型奖励就能最大化用户福利，但实践中模型可能过度澄清或生成冗长推理，而用户更喜欢简洁答案。这种类似囚徒困境的情况需要一种互利决策机制。

Method: 在推理阶段将用户-LLM交互视为策略博弈，构建收益矩阵评估双方福利；在训练阶段引入互惠奖励强化合作响应；还包含基于博弈论推理的动态响应适配技术。

Result: 实验表明GTAlign在多样化任务中显著提升了推理效率、答案质量和互惠福利，优于基线方法。

Conclusion: GTAlign通过博弈论方法有效解决了LLM对齐中的互惠问题，实现了更高效和用户友好的响应生成。

Abstract: Large Language Models (LLMs) have achieved remarkable progress in reasoning,
yet sometimes produce responses that are suboptimal for users in tasks such as
writing, information seeking, or providing practical guidance. Conventional
alignment practices typically assume that maximizing model reward also
maximizes user welfare, but this assumption frequently fails in practice:
models may over-clarify or generate overly verbose reasoning when users prefer
concise answers. Such behaviors resemble the prisoner's dilemma, where
individually rational choices lead to socially suboptimal outcomes. The
fundamental challenge is the lack of a principled decision making mechanism
that mutually benefits both the LLM and the user. We propose Game-Theoretic
Alignment (GTAlign), an alignment framework that integrates game-theoretic
decision making into both reasoning and training. During reasoning, the model
explicitly treats user-LLM interaction as a strategic game: it constructs
payoff matrices within its reasoning chain to estimate welfare for both itself
and the user, and then selects actions that are mutually beneficial. During
training, we introduce a mutual welfare reward that reinforces cooperative
responses, aligning model behavior with socially efficient outcomes. In
addition, we introduce an inference technique that leverages game-theoretic
reasoning to dynamically adapt LLM's response when pricing policies of LLM
service change. Extensive experiments demonstrate that GTAlign substantially
improves reasoning efficiency, answer quality, and mutual welfare compared to
baselines across diverse tasks. The code is available at
https://github.com/ulab-uiuc/GTAlign .

</details>


### [10] [LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition](https://arxiv.org/abs/2510.08928)
*Yushuo Zheng,Zicheng Zhang,Xiongkuo Min,Huiyu Duan,Guangtao Zhai*

Main category: cs.AI

TL;DR: 提出了LM Fight Arena框架，通过在格斗游戏《真人快打II》中让大型多模态模型相互对战，评估其在实时对抗环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型基准测试往往无法捕捉其在实时对抗环境中的真实性能，需要开发能够评估动态战略推理能力的新方法。

Method: 在受控锦标赛中测试六个领先的开源和闭源模型，每个模型控制相同角色以确保公平比较，通过解释游戏画面和状态数据来选择行动。

Result: LM Fight Arena提供了一个完全自动化、可重现且客观的评估框架，能够在大模型动态环境中测试其战略推理能力。

Conclusion: 这项工作引入了一个具有挑战性和吸引力的基准测试，弥合了AI评估与互动娱乐之间的差距。

Abstract: Existing benchmarks for large multimodal models (LMMs) often fail to capture
their performance in real-time, adversarial environments. We introduce LM Fight
Arena (Large Model Fight Arena), a novel framework that evaluates LMMs by
pitting them against each other in the classic fighting game Mortal Kombat II,
a task requiring rapid visual understanding and tactical, sequential
decision-making. In a controlled tournament, we test six leading open- and
closed-source models, where each agent operates controlling the same character
to ensure a fair comparison. The models are prompted to interpret game frames
and state data to select their next actions. Unlike static evaluations, LM
Fight Arena provides a fully automated, reproducible, and objective assessment
of an LMM's strategic reasoning capabilities in a dynamic setting. This work
introduces a challenging and engaging benchmark that bridges the gap between AI
evaluation and interactive entertainment.

</details>


### [11] [RADAR: Mechanistic Pathways for Detecting Data Contamination in LLM Evaluation](https://arxiv.org/abs/2510.08931)
*Ashish Kattamuri,Harshwardhan Fartale,Arpita Vats,Rahul Raja,Ishita Prasad*

Main category: cs.AI

TL;DR: RADAR是一个通过机制可解释性检测LLM数据污染的框架，通过区分基于记忆和基于推理的模型响应，准确率达到93%。


<details>
  <summary>Details</summary>
Motivation: 数据污染对可靠的LLM评估构成重大挑战，模型可能通过记忆训练数据而非展示真正推理能力来获得高性能。

Method: RADAR提取37个特征，涵盖表面级置信度轨迹和深层机制特性（包括注意力专业化、电路动态和激活流模式），并使用这些特征训练集成分类器。

Result: RADAR在多样化评估集上达到93%的准确率，在明确案例上表现完美，在具有挑战性的模糊示例上达到76.7%的准确率。

Conclusion: 这项工作展示了机制可解释性在超越传统表面级指标推进LLM评估方面的潜力。

Abstract: Data contamination poses a significant challenge to reliable LLM evaluation,
where models may achieve high performance by memorizing training data rather
than demonstrating genuine reasoning capabilities. We introduce RADAR (Recall
vs. Reasoning Detection through Activation Representation), a novel framework
that leverages mechanistic interpretability to detect contamination by
distinguishing recall-based from reasoning-based model responses. RADAR
extracts 37 features spanning surface-level confidence trajectories and deep
mechanistic properties including attention specialization, circuit dynamics,
and activation flow patterns. Using an ensemble of classifiers trained on these
features, RADAR achieves 93\% accuracy on a diverse evaluation set, with
perfect performance on clear cases and 76.7\% accuracy on challenging ambiguous
examples. This work demonstrates the potential of mechanistic interpretability
for advancing LLM evaluation beyond traditional surface-level metrics.

</details>


### [12] [FATHOMS-RAG: A Framework for the Assessment of Thinking and Observation in Multimodal Systems that use Retrieval Augmented Generation](https://arxiv.org/abs/2510.08945)
*Samuel Hildebrand,Curtis Taylor,Sean Oesch,James M Ghawaly Jr,Amir Sadovnik,Ryan Shivers,Brandon Schreiber,Kevin Kurian*

Main category: cs.AI

TL;DR: 提出了一个评估RAG管道的基准，包含93个多模态问题，评估管道的文本、表格、图像处理能力，并比较了开源和闭源模型的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注检索等特定方面，缺乏对RAG管道整体能力的评估，特别是多模态信息的处理能力。

Method: 创建包含文本、表格、图像的多模态数据集，提出短语级召回指标和最近邻嵌入分类器检测幻觉，比较2个开源和4个闭源模型的性能。

Result: 闭源管道在正确性和幻觉检测方面显著优于开源管道，特别是在多模态和跨文档问题上表现差距更大。人工评估显示指标与人类判断高度一致。

Conclusion: 该基准能有效评估RAG管道的整体性能，闭源模型在多模态信息处理方面优势明显，提出的评估指标与人类判断具有良好一致性。

Abstract: Retrieval-augmented generation (RAG) has emerged as a promising paradigm for
improving factual accuracy in large language models (LLMs). We introduce a
benchmark designed to evaluate RAG pipelines as a whole, evaluating a
pipeline's ability to ingest, retrieve, and reason about several modalities of
information, differentiating it from existing benchmarks that focus on
particular aspects such as retrieval. We present (1) a small, human-created
dataset of 93 questions designed to evaluate a pipeline's ability to ingest
textual data, tables, images, and data spread across these modalities in one or
more documents; (2) a phrase-level recall metric for correctness; (3) a
nearest-neighbor embedding classifier to identify potential pipeline
hallucinations; (4) a comparative evaluation of 2 pipelines built with
open-source retrieval mechanisms and 4 closed-source foundation models; and (5)
a third-party human evaluation of the alignment of our correctness and
hallucination metrics. We find that closed-source pipelines significantly
outperform open-source pipelines in both correctness and hallucination metrics,
with wider performance gaps in questions relying on multimodal and
cross-document information. Human evaluation of our metrics showed average
agreement of 4.62 for correctness and 4.53 for hallucination detection on a 1-5
Likert scale (5 indicating "strongly agree").

</details>


### [13] [EcphoryRAG: Re-Imagining Knowledge-Graph RAG via Human Associative Memory](https://arxiv.org/abs/2510.08958)
*Zirui Liao*

Main category: cs.AI

TL;DR: EcphoryRAG是一个基于实体知识图谱的RAG框架，通过提取核心实体和元数据来减少存储开销，利用多跳关联检索和动态关系推理来提升复杂问答性能。


<details>
  <summary>Details</summary>
Motivation: 受人类认知神经科学中通过线索激活实体记忆痕迹的机制启发，旨在解决传统RAG系统在复杂多跳推理任务中的效率问题。

Method: 在索引阶段仅存储核心实体和元数据，检索时先提取查询中的线索实体，然后在知识图谱上进行可扩展的多跳关联搜索，并动态推断实体间的隐含关系。

Result: 在2WikiMultiHop、HotpotQA和MuSiQue基准测试中，将平均精确匹配分数从0.392提升到0.474，优于HippoRAG等强基线方法。

Conclusion: 实体-线索-多跳检索范式在复杂问答任务中具有显著效果，验证了该方法的有效性。

Abstract: Cognitive neuroscience research indicates that humans leverage cues to
activate entity-centered memory traces (engrams) for complex, multi-hop
recollection. Inspired by this mechanism, we introduce EcphoryRAG, an
entity-centric knowledge graph RAG framework. During indexing, EcphoryRAG
extracts and stores only core entities with corresponding metadata, a
lightweight approach that reduces token consumption by up to 94\% compared to
other structured RAG systems. For retrieval, the system first extracts cue
entities from queries, then performs a scalable multi-hop associative search
across the knowledge graph. Crucially, EcphoryRAG dynamically infers implicit
relations between entities to populate context, enabling deep reasoning without
exhaustive pre-enumeration of relationships. Extensive evaluations on the
2WikiMultiHop, HotpotQA, and MuSiQue benchmarks demonstrate that EcphoryRAG
sets a new state-of-the-art, improving the average Exact Match (EM) score from
0.392 to 0.474 over strong KG-RAG methods like HippoRAG. These results validate
the efficacy of the entity-cue-multi-hop retrieval paradigm for complex
question answering.

</details>


### [14] [DualResearch: Entropy-Gated Dual-Graph Retrieval for Answer Reconstruction](https://arxiv.org/abs/2510.08959)
*Jinxin Shi,Zongsheng Cao,Runmin Ma,Yusong Hu,Jie Zhou,Xin Li,Lei Bai,Liang He,Bo Zhang*

Main category: cs.AI

TL;DR: DualResearch是一个检索和融合框架，通过联合建模广度语义图和深度因果图来解决深度研究框架中的上下文污染、证据支持薄弱和执行路径脆弱等问题，在科学推理基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的深度研究框架虽然能协调外部工具进行复杂的多步科学推理，但仍存在上下文污染、证据支持薄弱和执行路径脆弱等问题，需要改进。

Method: 提出DualResearch框架，联合建模两个互补图：编码稳定背景知识的广度语义图和捕获执行来源的深度因果图。每个图都有层原生相关性函数，广度使用种子锚定语义扩散，深度使用因果语义路径匹配和可靠性加权。通过熵门控规则在log空间融合答案分布。

Result: 在科学推理基准HLE和GPQA上取得竞争性表现。使用开源系统InternAgent的日志文件，在HLE上的准确率提高7.7%，在GPQA上提高6.06%。

Conclusion: DualResearch作为深度研究系统的补充，能够将冗长的多工具执行日志压缩为简洁的推理图，稳定有效地重建答案，在科学推理任务中表现优异。

Abstract: The deep-research framework orchestrates external tools to perform complex,
multi-step scientific reasoning that exceeds the native limits of a single
large language model. However, it still suffers from context pollution, weak
evidentiary support, and brittle execution paths. To address these issues, we
propose DualResearch, a retrieval and fusion framework that matches the
epistemic structure of tool-intensive reasoning by jointly modeling two
complementary graphs: a breadth semantic graph that encodes stable background
knowledge, and a depth causal graph that captures execution provenance. Each
graph has a layer-native relevance function, seed-anchored semantic diffusion
for breadth, and causal-semantic path matching with reliability weighting for
depth. To reconcile their heterogeneity and query-dependent uncertainty,
DualResearch converts per-layer path evidence into answer distributions and
fuses them in log space via an entropy-gated rule with global calibration. The
fusion up-weights the more certain channel and amplifies agreement. As a
complement to deep-research systems, DualResearch compresses lengthy multi-tool
execution logs into a concise reasoning graph, and we show that it can
reconstruct answers stably and effectively. On the scientific reasoning
benchmarks HLE and GPQA, DualResearch achieves competitive performance. Using
log files from the open-source system InternAgent, its accuracy improves by
7.7% on HLE and 6.06% on GPQA.

</details>


### [15] [Semantic-Condition Tuning: Fusing Graph Context with Large Language Models for Knowledge Graph Completion](https://arxiv.org/abs/2510.08966)
*Ruitong Liu,Yan Wen,Te Sun,Yunjia Wu,Pingyang Huang,Zihang Yu,Siyuan Li*

Main category: cs.AI

TL;DR: 提出了语义条件调优(SCT)新范式，通过图神经网络提取上下文感知的语义条件，并自适应地调制文本嵌入，实现知识图谱与LLM的深度融合。


<details>
  <summary>Details</summary>
Motivation: 现有前缀调优方法简单拼接知识嵌入与文本输入，忽略了知识图谱的丰富关系语义，给LLM带来隐式推理负担。

Method: SCT包含两个关键模块：语义图模块使用GNN从局部图邻域提取上下文感知语义条件；条件自适应融合模块通过参数化投影器自适应调制文本嵌入。

Result: 在知识图谱基准测试中，SCT显著优于前缀调优和其他强基线方法。

Conclusion: 通过在LLM推理前用语义图上下文调制输入表示，SCT提供了更直接有效的信号，实现了更准确和鲁棒的知识推理。

Abstract: Fusing Knowledge Graphs with Large Language Models is crucial for
knowledge-intensive tasks like knowledge graph completion. The prevailing
paradigm, prefix-tuning, simply concatenates knowledge embeddings with text
inputs. However, this shallow fusion overlooks the rich relational semantics
within KGs and imposes a significant implicit reasoning burden on the LLM to
correlate the prefix with the text. To address these, we propose
Semantic-condition Tuning (SCT), a new knowledge injection paradigm comprising
two key modules. First, a Semantic Graph Module employs a Graph Neural Network
to extract a context-aware semantic condition from the local graph
neighborhood, guided by knowledge-enhanced relations. Subsequently, this
condition is passed to a Condition-Adaptive Fusion Module, which, in turn,
adaptively modulates the textual embedding via two parameterized projectors,
enabling a deep, feature-wise, and knowledge-aware interaction. The resulting
pre-fused embedding is then fed into the LLM for fine-tuning. Extensive
experiments on knowledge graph benchmarks demonstrate that SCT significantly
outperforms prefix-tuning and other strong baselines. Our analysis confirms
that by modulating the input representation with semantic graph context before
LLM inference, SCT provides a more direct and potent signal, enabling more
accurate and robust knowledge reasoning.

</details>


### [16] [Tiny-R1V: Lightweight Multimodal Unified Reasoning Model via Model Merging](https://arxiv.org/abs/2510.08987)
*Qixiang Yin,Huanjin Yao,Jianghao Chen,Jiaxing Huang,Zhicheng Zhao,Fei Su*

Main category: cs.AI

TL;DR: Tiny-R1V是一个轻量级3B参数的多模态大语言模型，通过两阶段优化实现更快的推理速度和更高的准确率，统一了多任务的多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在推理效率方面面临模型规模大、过度思考和轻量级场景准确率下降等挑战，而轻量级MLLMs的推理能力研究相对缺乏。

Method: 采用两阶段优化：第一阶段引入LIPO强化学习方法，优先选择简洁高质量的回答；第二阶段提出AMM模型融合方法，自适应调整任务向量权重并通过梯度投影正则化减少冗余冲突。

Result: 在十个广泛使用的推理基准测试中（涵盖数学、结构化数据、OCR和通用能力），Tiny-R1V展现出卓越性能，使轻量级模型在多样化多模态推理任务中表现出色。

Conclusion: Tiny-R1V证明了轻量级模型通过创新的两阶段优化方法，能够在多模态推理任务中实现高效且准确的性能，为轻量级MLLMs的发展提供了新思路。

Abstract: Although Multimodal Large Language Models (MLLMs) have demonstrated
remarkable capabilities across diverse tasks, they encounter numerous
challenges in terms of reasoning efficiency, such as large model size,
overthinking, and compromised accuracy in lightweight scenarios. However,
research on the reasoning capabilities of lightweight MLLMs is quite lacking.
To this end, we propose Tiny-R1V, a novel lightweight 3B model that achieves
faster inference and higher accuracy via a two-stage optimization, while
unifying multimodal reasoning across multiple tasks and using fewer tokens. In
the first stage, Tiny-R1V introduces Length-Informed Relative Policy
Optimization (LIPO), a novel reinforcement learning method, to train each
reasoning model. The LIPO is designed to dynamically adjusts advantages of
responses within groups, that is, by prioritizing concise yet high-quality
responses to encourage the generation of shorter and more accurate response. In
the second stage, we propose Adaptive Model Merging (AMM), a training-free
model merging method that merges multiple specialist models into a unified
architecture. Specifically, AMM adaptively adjusts the weights of task vectors
and robustly optimizes the merged vectors via a novel gradient projection
regularization loss function, thus mitigating redundant conflicts between them.
Extensive evaluations on ten widely-used reasoning benchmarks covering
mathematics, structured data (charts, tables, documents), OCR, and general
capabilities showcase the superior performance of Tiny-R1V, enabling
lightweight models to excel in diverse multimodal reasoning tasks.

</details>


### [17] [TripScore: Benchmarking and rewarding real-world travel planning with fine-grained evaluation](https://arxiv.org/abs/2510.09011)
*Yincen Qu,Huan Xiao,Feng Li,Hui Zhou,Xiangying Dai*

Main category: cs.AI

TL;DR: 提出了一个统一的旅行规划基准，将细粒度标准整合为单一奖励分数，支持强化学习训练，并在大规模数据集上验证了多种方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估旅行计划的可行性、可靠性和吸引力方面存在不足，需要更全面的评估框架来直接比较计划质量并支持强化学习。

Method: 开发了统一的评估器，将细粒度标准整合为单一奖励分数；构建了包含4870个查询的大规模数据集，包括219个真实世界自由形式请求；测试了多种方法包括测试时计算、神经符号方法、监督微调和GRPO强化学习。

Result: 评估器与旅行专家注释达到60.75%的一致性，优于多个LLM作为评判基准；强化学习在基础模型上通常比仅提示和监督基线提高了行程可行性，获得更高的统一奖励分数。

Conclusion: 提出的统一基准能够有效评估旅行计划质量，强化学习方法在提高计划可行性方面表现优异，为旅行规划任务提供了可靠的评估框架。

Abstract: Travel planning is a valuable yet complex task that poses significant
challenges even for advanced large language models (LLMs). While recent
benchmarks have advanced in evaluating LLMs' planning capabilities, they often
fall short in evaluating feasibility, reliability, and engagement of travel
plans. We introduce a comprehensive benchmark for travel planning that unifies
fine-grained criteria into a single reward, enabling direct comparison of plan
quality and seamless integration with reinforcement learning (RL). Our
evaluator achieves moderate agreement with travel-expert annotations (60.75\%)
and outperforms multiple LLM-as-judge baselines. We further release a
large-scale dataset of 4,870 queries including 219 real-world, free-form
requests for generalization to authentic user intent. Using this benchmark, we
conduct extensive experiments across diverse methods and LLMs, including
test-time computation, neuro-symbolic approaches, supervised fine-tuning, and
RL via GRPO. Across base models, RL generally improves itinerary feasibility
over prompt-only and supervised baselines, yielding higher unified reward
scores.

</details>


### [18] [RefGrader: Automated Grading of Mathematical Competition Proofs using Agentic Workflows](https://arxiv.org/abs/2510.09021)
*Hamed Mahdavi,Pouria Mahdavinia,Samira Malek,Pegah Mohammadipour,Alireza Hashemi,Majid Daliri,Alireza Farhadi,Amir Khasahmadi,Niloofar Mireshghallah,Vasant Honavar*

Main category: cs.AI

TL;DR: 该论文评估了SOTA LLMs在证明评分方面的能力，包括错误检测、严重性判断和分数分配，并提出了基于代理的工作流程来改进部分学分分配的校准问题。


<details>
  <summary>Details</summary>
Motivation: 随着SOTA LLMs在奥林匹克数学问题上的显著进步，需要评估这些模型在证明评分方面的能力，特别是在检测错误、判断严重性和分配公平分数方面的表现。

Method: 使用90个Gemini 2.5 Pro生成的解决方案和MathArena的IMO/USAMO 2025解决方案集，引入基于代理的工作流程来提取和分析参考解决方案，自动推导问题特定的评分标准，并进行多步骤评分过程。

Result: 模型能够可靠地标记错误解决方案（包括微妙错误），但在部分学分分配方面存在校准差距。提出的工作流程在人类评分一致性和部分学分处理一致性方面表现更好。

Conclusion: 基于代理的工作流程能够显著提高LLMs在证明评分方面的表现，特别是在部分学分分配的一致性和准确性方面，为未来研究提供了有价值的工具和数据。

Abstract: State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based
Olympiad problems to solving most of the IMO 2025 problems, with leading
systems reportedly handling 5 of 6 problems. Given this progress, we assess how
well these models can grade proofs: detecting errors, judging their severity,
and assigning fair scores beyond binary correctness. We study proof-analysis
capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we
grade on a 1-4 scale with detailed error annotations, and on MathArena solution
sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models
can reliably flag incorrect (including subtly incorrect) solutions but exhibit
calibration gaps in how partial credit is assigned. To address this, we
introduce agentic workflows that extract and analyze reference solutions and
automatically derive problem-specific rubrics for a multi-step grading process.
We instantiate and compare different design choices for the grading workflows,
and evaluate their trade-offs. Across our annotated corpus and MathArena, our
proposed workflows achieve higher agreement with human grades and more
consistent handling of partial credit across metrics. We release all code,
data, and prompts/logs to facilitate future research.

</details>


### [19] [Repairing Regex Vulnerabilities via Localization-Guided Instructions](https://arxiv.org/abs/2510.09037)
*Sicheol Sung,Joonghyuk Hahn,Yo-Sub Han*

Main category: cs.AI

TL;DR: 提出了一种混合框架LRR，结合符号方法和LLM的优势来修复正则表达式ReDoS漏洞，解决了现有方法在精确性和泛化性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 正则表达式在现代计算中广泛使用，但容易遭受ReDoS攻击。现有修复方法存在权衡：符号方法精确但无法修复复杂模式，LLM方法泛化性强但可靠性不足。

Method: LRR框架将问题识别与修复过程解耦：首先用确定性符号模块定位易受攻击的子模式，然后调用LLM为隔离的片段生成语义等效的修复方案。

Result: 该混合架构成功解决了规则修复方法无法处理的复杂修复案例，同时避免了纯LLM方法的语义错误，修复率比最先进方法提高了15.4%。

Conclusion: LRR提供了一种经过验证的方法论，通过结合符号方法和LLM的优势，有效解决了自动修复中的可靠性问题。

Abstract: Regular expressions (regexes) are foundational to modern computing for
critical tasks like input validation and data parsing, yet their ubiquity
exposes systems to regular expression denial of service (ReDoS), a
vulnerability requiring automated repair methods. Current approaches, however,
are hampered by a trade-off. Symbolic, rule-based system are precise but fails
to repair unseen or complex vulnerability patterns. Conversely, large language
models (LLMs) possess the necessary generalizability but are unreliable for
tasks demanding strict syntactic and semantic correctness. We resolve this
impasse by introducing a hybrid framework, localized regex repair (LRR),
designed to harness LLM generalization while enforcing reliability. Our core
insight is to decouple problem identification from the repair process. First, a
deterministic, symbolic module localizes the precise vulnerable subpattern,
creating a constrained and tractable problem space. Then, the LLM invoked to
generate a semantically equivalent fix for this isolated segment. This combined
architecture successfully resolves complex repair cases intractable for
rule-based repair while avoiding the semantic errors of LLM-only approaches.
Our work provides a validated methodology for solving such problems in
automated repair, improving the repair rate by 15.4%p over the
state-of-the-art. Our code is available at https://github.com/cdltlehf/LRR.

</details>


### [20] [Auto-scaling Continuous Memory for GUI Agent](https://arxiv.org/abs/2510.09038)
*Wenyi Wu,Kun Zhou,Ruoxin Yuan,Vivian Yu,Stephen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: 提出连续记忆机制，将GUI轨迹编码为固定长度的连续嵌入，显著减少上下文成本并保留细粒度视觉信息，在长视野和分布偏移任务中持续提升成功率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理将历史轨迹压缩为文本标记，导致上下文长度膨胀且丢失关键视觉线索（如控件精确尺寸和位置），需要可扩展的记忆机制来泛化不熟悉界面和长视野任务。

Method: 使用VLM作为编码器将GUI轨迹编码为固定长度连续嵌入，直接输入到主干网络；引入自动扩展数据飞轮，通过搜索发现新环境、合成任务、执行轨迹和验证成功来低成本扩展记忆。

Result: 随着记忆大小和检索深度增加，性能单调提升；Qwen-2.5-VL-7B+连续记忆在真实GUI基准测试中达到与GPT-4o、Claude-4等闭源模型相当的性能。

Conclusion: 连续记忆机制能有效提升GUI代理在长视野和分布偏移任务中的表现，且通过低成本数据飞轮实现了大规模记忆扩展。

Abstract: We study how to endow GUI agents with scalable memory that help generalize
across unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress
past trajectories into text tokens, which balloons context length and misses
decisive visual cues (e.g., exact widget size and position). We propose a
continuous memory that encodes each GUI trajectory into a fixed-length sequence
of continuous embeddings using the VLM itself as an encoder; these embeddings
are plugged directly into the backbone's input layer, sharply reducing context
cost while preserving fine-grained visual information. As memory size and
retrieval depth increase, performance improves monotonically, unlike text
memories that degrade with long prompts. To grow memory at low cost, we
introduce an auto-scaling data flywheel that (i) discovers new environments via
search, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out
trajectories with the agent, and (iv) verifies success with the same VLM. Using
this pipeline, we collect 100k+ trajectories for about \$4000 and fine-tune
only the memory encoder (LoRA on a Q-Former, 1.2\% parameters) with 1,500
samples. On real-world GUI benchmarks, our memory-augmented agent consistently
improves success rates under long horizons and distribution shifts. Notably,
Qwen-2.5-VL-7B + continuous memory achieves performance comparable to
state-of-the-art closed-source models (e.g., GPT-4o, Claude-4).

</details>


### [21] [Humanoid Artificial Consciousness Designed with Large Language Model Based on Psychoanalysis and Personality Theory](https://arxiv.org/abs/2510.09043)
*Sang Hun Kim,Jongmin Lee,Dongkyu Park,So Young Lee,Yosep Chong*

Main category: cs.AI

TL;DR: 本研究通过整合精神分析和MBTI人格理论，构建了包含自我意识、无意识和前意识的人工意识模型，以及16种MBTI人格类型的角色，在10种情境下评估其决策过程，结果显示能够较好地模拟人类意识。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然在各领域取得显著进展，但由于存在幻觉问题，难以真正模仿人类意识。本研究旨在通过精神分析和人格理论构建更接近人类认知的人工意识系统。

Method: 基于精神分析理论开发三种人工意识（自我意识、无意识、前意识），设计16种MBTI人格类型的角色，创建10种不同情境评估模型的决策过程，采用调查评估、ChatGPT三级分类和定性评审三种方法进行评价。

Result: 定量和定性分析表明模型能够较好地模拟人类意识，尽管不同角色和意识类型之间的响应差异不够显著。

Conclusion: 整合精神分析和人格理论元素开发的模型能够构建更直观、适应性更强的人工智能系统，为改善AI在复杂认知情境中的交互开辟了新途径。

Abstract: Human consciousness is still a concept hard to define with current scientific
understanding. Although Large Language Models (LLMs) have recently demonstrated
significant advancements across various domains including translation and
summarization, human consciousness is not something to imitate with current
upfront technology owing to so-called hallucination. This study, therefore,
proposes a novel approach to address these challenges by integrating
psychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing
consciousness and personality modules. We developed three artificial
consciousnesses (self-awareness, unconsciousness, and preconsciousness) based
on the principles of psychoanalysis. Additionally, we designed 16 characters
with different personalities representing the sixteen MBTI types, with several
attributes such as needs, status, and memories. To determine if our model's
artificial consciousness exhibits human-like cognition, we created ten distinct
situations considering seven attributes such as emotional understanding and
logical thinking. The decision-making process of artificial consciousness and
the final action were evaluated in three ways: survey evaluation, three-tier
classification via ChatGPT, and qualitative review. Both quantitative and
qualitative analyses indicated a high likelihood of well-simulated
consciousness, although the difference in response between different characters
and consciousnesses was not very significant. This implies that the developed
models incorporating elements of psychoanalysis and personality theory can lead
to building a more intuitive and adaptable AI system with humanoid
consciousness. Therefore, this study contributes to opening up new avenues for
improving AI interactions in complex cognitive contexts.

</details>


### [22] [MEC$^3$O: Multi-Expert Consensus for Code Time Complexity Prediction](https://arxiv.org/abs/2510.09049)
*Joonghyuk Hahn,Soohan Lim,Yo-Sub Han*

Main category: cs.AI

TL;DR: MEC³O是一个多专家共识系统，通过将LLM分配到特定复杂度类别并让专家进行结构化辩论，使用加权共识机制整合预测，在代码复杂度预测任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码时间复杂度预测任务中表现不均衡，不同模型在不同复杂度类别上各有优势，需要一种机制来整合各模型的专长。

Method: 提出MEC³O多专家共识系统，基于模型性能将LLM分配到特定复杂度类别，提供类别专业化指令使其成为专家，通过结构化辩论和加权共识机制整合预测。

Result: 在CodeComplex数据集上，MEC³O比开源基线方法准确率和macro-F1分数至少提高10%，在macro-F1上平均超过GPT-4o-mini，与GPT-4o和GPT-o4-mini在F1分数上表现相当。

Conclusion: 多专家辩论和加权共识策略能有效生成最终预测，证明了该方法在代码复杂度预测任务中的有效性。

Abstract: Predicting the complexity of source code is essential for software
development and algorithm analysis. Recently, Baik et al. (2025) introduced
CodeComplex for code time complexity prediction. The paper shows that LLMs
without fine-tuning struggle with certain complexity classes. This suggests
that no single LLM excels at every class, but rather each model shows
advantages in certain classes. We propose MEC$^3$O, a multi-expert consensus
system, which extends the multi-agent debate frameworks. MEC$^3$O assigns LLMs
to complexity classes based on their performance and provides them with
class-specialized instructions, turning them into experts. These experts engage
in structured debates, and their predictions are integrated through a weighted
consensus mechanism. Our expertise assignments to LLMs effectively handle
Degeneration-of-Thought, reducing reliance on a separate judge model, and
preventing convergence to incorrect majority opinions. Experiments on
CodeComplex show that MEC$^3$O outperforms the open-source baselines, achieving
at least 10% higher accuracy and macro-F1 scores. It also surpasses GPT-4o-mini
in macro-F1 scores on average and demonstrates competitive on-par F1 scores to
GPT-4o and GPT-o4-mini on average. This demonstrates the effectiveness of
multi-expert debates and weight consensus strategy to generate the final
predictions. Our code and data is available at
https://github.com/suhanmen/MECO.

</details>


### [23] [OSCAR: Orthogonal Stochastic Control for Alignment-Respecting Diversity in Flow Matching](https://arxiv.org/abs/2510.09060)
*Jingxuan Wu,Zhenglin Wan,Xingrui Yu,Yuzhe Yang,Bo An,Ivor Tsang*

Main category: cs.AI

TL;DR: 提出了一种无需训练、在推理时控制流式文本到图像模型的方法，通过特征空间目标和时间调度的随机扰动来增强生成多样性，同时保持图像质量和提示对齐。


<details>
  <summary>Details</summary>
Motivation: 流式文本到图像模型遵循确定性轨迹，用户需要重复采样才能发现多样模式，这是一个成本高且效率低的过程。

Method: 通过特征空间目标鼓励轨迹间的横向扩散，并引入时间调度的随机扰动，该扰动被投影为与生成流正交，以在不降低图像细节或提示保真度的情况下增强变化。

Result: 在固定采样预算下，该方法在多个文本到图像设置中持续改进了Vendi Score和Brisque等多样性指标，同时保持了图像质量和对齐度。

Conclusion: 该方法无需重新训练或修改基础采样器，与常见的流匹配求解器兼容，理论上证明能够单调增加体积代理，同时由于几何约束近似保持边际分布，从而稳健地保持生成质量。

Abstract: Flow-based text-to-image models follow deterministic trajectories, forcing
users to repeatedly sample to discover diverse modes, which is a costly and
inefficient process. We present a training-free, inference-time control
mechanism that makes the flow itself diversity-aware. Our method simultaneously
encourages lateral spread among trajectories via a feature-space objective and
reintroduces uncertainty through a time-scheduled stochastic perturbation.
Crucially, this perturbation is projected to be orthogonal to the generation
flow, a geometric constraint that allows it to boost variation without
degrading image details or prompt fidelity. Our procedure requires no
retraining or modification to the base sampler and is compatible with common
flow-matching solvers. Theoretically, our method is shown to monotonically
increase a volume surrogate while, due to its geometric constraints,
approximately preserving the marginal distribution. This provides a principled
explanation for why generation quality is robustly maintained. Empirically,
across multiple text-to-image settings under fixed sampling budgets, our method
consistently improves diversity metrics such as the Vendi Score and Brisque
over strong baselines, while upholding image quality and alignment.

</details>


### [24] [Physics-Informed High-order Graph Dynamics Identification Learning for Predicting Complex Networks Long-term Dynamics](https://arxiv.org/abs/2510.09082)
*Bicheng Wang,Jinping Wang,Yibo Sue*

Main category: cs.AI

TL;DR: 提出了一种高阶网络动力学识别方法，用于复杂网络的长期动态预测，结合动态超图学习和物理数据双驱动预测模块，解决了传统方法只能处理成对关系以及理论模型精度不足、数据驱动模型缺乏可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有复杂网络动态预测方法存在两个主要问题：一是传统图学习方法只能捕捉成对关系，无法处理网络中丰富的非成对结构关系；二是理论预测模型精度不足，而数据驱动模型缺乏可解释性。

Method: 1. 引入动态超图学习来捕捉复杂网络中的高阶非成对关系；2. 提出物理数据双驱动动态预测模块，结合Koopman算子理论将非线性动态微分方程转化为线性系统求解，同时利用物理信息神经微分方程方法确保动态演化符合物理规律。

Result: 在公共数据集和自建产业链网络数据集上的实验验证表明，该方法具有良好的预测精度和长期预测性能。

Conclusion: 该方法通过动态超图学习和物理数据双驱动预测，成功解决了复杂网络动态预测中的高阶关系建模和可解释性问题，实现了准确且可解释的长期动态预测。

Abstract: Learning complex network dynamics is fundamental to understanding, modelling
and controlling real-world complex systems. There are two main problems in the
task of predicting the dynamic evolution of complex networks: on the one hand,
existing methods usually use simple graphs to describe the relationships in
complex networks; however, this approach can only capture pairwise
relationships, while there may be rich non-pairwise structured relationships in
the network. First-order GNNs have difficulty in capturing dynamic non-pairwise
relationships. On the other hand, theoretical prediction models lack accuracy
and data-driven prediction models lack interpretability. To address the above
problems, this paper proposes a higher-order network dynamics identification
method for long-term dynamic prediction of complex networks. Firstly, to
address the problem that traditional graph machine learning can only deal with
pairwise relations, dynamic hypergraph learning is introduced to capture the
higher-order non-pairwise relations among complex networks and improve the
accuracy of complex network modelling. Then, a dual-driven dynamic prediction
module for physical data is proposed. The Koopman operator theory is introduced
to transform the nonlinear dynamical differential equations for the dynamic
evolution of complex networks into linear systems for solving. Meanwhile, the
physical information neural differential equation method is utilised to ensure
that the dynamic evolution conforms to the physical laws. The dual-drive
dynamic prediction module ensures both accuracy and interpretability of the
prediction. Validated on public datasets and self-built industrial chain
network datasets, the experimental results show that the method in this paper
has good prediction accuracy and long-term prediction performance.

</details>


### [25] [Leading the Follower: Learning Persuasive Agents in Social Deduction Games](https://arxiv.org/abs/2510.09087)
*Zhang Zheng,Deheng Ye,Peilin Zhao,Hao Wang*

Main category: cs.AI

TL;DR: 提出了一个强化学习框架，将社交推理游戏中的对话建模为Stackelberg博弈，训练AI代理优化说服性沟通能力，显著超越现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在社交推理游戏中主要关注信息处理和策略选择，忽视了说服性沟通在影响其他玩家信念和回应方面的重要性。

Method: 将回合制对话形式化为Stackelberg竞争，当前玩家作为领导者战略性地影响跟随者的回应，并基于此提出强化学习框架来优化说服性话语。

Result: 在三个不同的社交推理游戏中进行的综合实验表明，所提出的代理显著优于基线方法。

Conclusion: 这项工作代表了开发具有战略性社会影响力AI代理的重要进展，对需要说服性沟通的场景具有广泛意义。

Abstract: Large language model (LLM) agents have shown remarkable progress in social
deduction games (SDGs). However, existing approaches primarily focus on
information processing and strategy selection, overlooking the significance of
persuasive communication in influencing other players' beliefs and responses.
In SDGs, success depends not only on making correct deductions but on
convincing others to response in alignment with one's intent. To address this
limitation, we formalize turn-based dialogue in SDGs as a Stackelberg
competition, where the current player acts as the leader who strategically
influences the follower's response. Building on this theoretical foundation, we
propose a reinforcement learning framework that trains agents to optimize
utterances for persuasive impact. Through comprehensive experiments across
three diverse SDGs, we demonstrate that our agents significantly outperform
baselines. This work represents a significant step toward developing AI agents
capable of strategic social influence, with implications extending to scenarios
requiring persuasive communication.

</details>


### [26] [PAC Reasoning: Controlling the Performance Loss for Efficient Reasoning](https://arxiv.org/abs/2510.09133)
*Hao Zeng,Jianguo Huang,Bingyi Jing,Hongxin Wei,Bo An*

Main category: cs.AI

TL;DR: 提出PAC推理方法，通过置信上界控制性能损失，在用户指定的性能损失容忍度内动态切换思考模式以节省计算成本


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然性能优秀但计算成本高，现有动态切换方法缺乏性能损失统计保证，不适合高风险应用

Method: 构建性能损失的单调置信上界函数，基于不确定性分数确定切换阈值，在思考和非思考模式间动态切换

Result: 在推理基准测试中，该方法能节省计算预算并控制用户指定的性能损失

Conclusion: PAC推理方法为高效推理提供了统计保证，能在保证性能的前提下显著降低计算成本

Abstract: Large reasoning models (LRMs) have achieved remarkable progress in complex
problem-solving tasks. Despite this success, LRMs typically suffer from high
computational costs during deployment, highlighting a need for efficient
inference. A popular direction of efficiency improvement is to switch the LRM
between thinking and nonthinking modes dynamically. However, such approaches
often introduce additional reasoning errors and lack statistical guarantees for
the performance loss, which are critical for high-stakes applications. In this
work, we propose Probably Approximately Correct (PAC) reasoning that controls
the performance loss under the user-specified performance loss tolerance. In
particular, we construct an upper confidence bound on the performance loss,
formulated as a monotone function of the uncertainty score, and subsequently
determine a threshold for switching to the nonthinking model. Theoretically,
using the threshold to switch between the thinking and nonthinking modes
ensures bounded performance loss in a distribution-free manner. Our
comprehensive experiments on reasoning benchmarks show that the proposed method
can save computational budgets and control the user-specified performance loss.

</details>


### [27] [Dr. Bias: Social Disparities in AI-Powered Medical Guidance](https://arxiv.org/abs/2510.09162)
*Emma Kondrup,Anne Imouza*

Main category: cs.AI

TL;DR: 本文通过模拟不同社会群体（性别、年龄、种族）患者向大语言模型（LLMs）提出医疗问题，发现LLMs生成的医疗建议存在系统性差异，特别是对原住民和双性人群体提供更复杂难懂的建议，这种差异在交叉群体中更加明显。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在医疗应用中的普及，需要评估其在医疗建议生成中是否存在社会偏见，特别是对弱势群体的影响，以确保公平的医疗支持。

Method: 通过模拟不同患者档案（性别、年龄、种族）向LLMs提出一系列医疗问题，比较生成回答的自然语言特征。

Result: LLMs生成的医疗建议在不同社会群体间存在系统性差异，原住民和双性人群体收到的建议可读性更差、更复杂，交叉群体的差异更加明显。

Conclusion: 鉴于公众对LLMs的信任度增加，需要提高AI素养，并呼吁AI开发者紧急调查和缓解这些系统性差异，确保不转化为不公正的患者支持。

Abstract: With the rapid progress of Large Language Models (LLMs), the general public
now has easy and affordable access to applications capable of answering most
health-related questions in a personalized manner. These LLMs are increasingly
proving to be competitive, and now even surpass professionals in some medical
capabilities. They hold particular promise in low-resource settings,
considering they provide the possibility of widely accessible, quasi-free
healthcare support. However, evaluations that fuel these motivations highly
lack insights into the social nature of healthcare, oblivious to health
disparities between social groups and to how bias may translate into
LLM-generated medical advice and impact users. We provide an exploratory
analysis of LLM answers to a series of medical questions spanning key clinical
domains, where we simulate these questions being asked by several patient
profiles that vary in sex, age range, and ethnicity. By comparing natural
language features of the generated responses, we show that, when LLMs are used
for medical advice generation, they generate responses that systematically
differ between social groups. In particular, Indigenous and intersex patients
receive advice that is less readable and more complex. We observe these trends
amplify when intersectional groups are considered. Considering the increasing
trust individuals place in these models, we argue for higher AI literacy and
for the urgent need for investigation and mitigation by AI developers to ensure
these systemic differences are diminished and do not translate to unjust
patient support. Our code is publicly available on GitHub.

</details>


### [28] [Comparing Knowledge Source Integration Methods for Optimizing Healthcare Knowledge Fusion in Rescue Operation](https://arxiv.org/abs/2510.09223)
*Mubaris Nadeem,Madjid Fathi*

Main category: cs.AI

TL;DR: 本文提出了基于知识图谱结构的医学知识融合概念模型，旨在整合多种医疗知识源来支持关键决策。


<details>
  <summary>Details</summary>
Motivation: 医学领域需要整合复杂的医疗专业知识和患者健康信息，以支持准确的患者驱动决策，这需要统一的方法来收集、分析和利用现有医疗知识。

Method: 基于知识图谱结构开发了多个概念模型，用于实现知识融合，并展示了如何将各种知识源整合到知识图谱中。

Result: 提出了能够实现知识融合的概念模型，为医疗救援操作提供了知识整合框架。

Conclusion: 知识融合为医疗专业人员提供了从多个上下文对齐的知识源中进行选择的机会，从而支持关键决策制定。

Abstract: In the field of medicine and healthcare, the utilization of medical
expertise, based on medical knowledge combined with patients' health
information is a life-critical challenge for patients and health professionals.
The within-laying complexity and variety form the need for a united approach to
gather, analyze, and utilize existing knowledge of medical treatments, and
medical operations to provide the ability to present knowledge for the means of
accurate patient-driven decision-making. One way to achieve this is the fusion
of multiple knowledge sources in healthcare. It provides health professionals
the opportunity to select from multiple contextual aligned knowledge sources
which enables the support for critical decisions. This paper presents multiple
conceptual models for knowledge fusion in the field of medicine, based on a
knowledge graph structure. It will evaluate, how knowledge fusion can be
enabled and presents how to integrate various knowledge sources into the
knowledge graph for rescue operations.

</details>


### [29] [RegexPSPACE: A Benchmark for Evaluating LLM Reasoning on PSPACE-complete Regex Problems](https://arxiv.org/abs/2510.09227)
*Hyundong Jin,Joonghyuk Hahn,Yo-Sub Han*

Main category: cs.AI

TL;DR: 提出了一个基于PSPACE完全正则表达式问题的新基准，用于评估大语言模型和大推理模型的空间计算限制。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注NP复杂度类的问题，但PSPACE完全问题需要更大的搜索空间探索，能更严格地评估模型的计算能力。

Method: 通过双指数空间探索构建了包含超过100万个正则表达式实例的数据集，并对6个LLM和5个LRM进行了广泛评估。

Result: 揭示了模型常见的失败模式，如冗长和重复，首次实证研究了LLM和LRM的空间计算限制。

Conclusion: 这项工作为评估模型的高级推理能力提供了一个新框架，展示了PSPACE完全问题作为更严格计算能力评估标准的价值。

Abstract: Large language models (LLMs) show strong performance across natural language
processing (NLP), mathematical reasoning, and programming, and recent large
reasoning models (LRMs) further emphasize explicit reasoning. Yet their
computational limits, particularly spatial complexity constrained by finite
context windows, remain poorly understood. While recent works often focus on
problems within the NP complexity class, we push the boundary by introducing a
novel benchmark grounded in two PSPACE-complete regular expression (regex)
problems: equivalence decision (RegexEQ) and minimization (RegexMin).
PSPACE-complete problems serve as a more rigorous standard for assessing
computational capacity, as their solutions require massive search space
exploration. We perform a double-exponential space exploration to construct a
labeled dataset of over a million regex instances with a sound filtering
process to build the benchmark. We conduct extensive evaluations on 6 LLMs and
5 LRMs of varying scales, revealing common failure patterns such as verbosity
and repetition. With its well-defined structure and quantitative evaluation
metrics, this work presents the first empirical investigation into the spatial
computational limitations of LLMs and LRMs, offering a new framework for
evaluating their advanced reasoning capabilities. Our code is available at
https://github.com/hyundong98/RegexPSPACE .

</details>


### [30] [Fundamentals of Building Autonomous LLM Agents](https://arxiv.org/abs/2510.09244)
*Victor de Lamo Castrillo,Habtom Kahsay Gidey,Alexander Lenz,Alois Knoll*

Main category: cs.AI

TL;DR: 本文综述了基于大语言模型的智能体架构与实现方法，探讨如何构建能够自动化复杂任务、模拟人类认知过程的智能系统。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在现实任务中存在局限性，研究旨在开发能够自动化复杂任务、缩小与人类能力差距的智能LLM智能体。

Method: 提出包含四个关键组件的智能体架构：感知系统（将环境感知转换为有意义表示）、推理系统（制定计划、适应反馈、评估行动）、记忆系统（通过短期和长期机制保留知识）、执行系统（将内部决策转化为具体行动）。

Result: 集成这些系统能够构建更强大和通用的软件机器人，模拟人类认知过程实现自主智能行为。

Conclusion: 通过整合感知、推理、记忆和执行系统，可以开发出能够自动化复杂任务、模拟人类认知过程的智能LLM智能体，缩小与人类能力的差距。

Abstract: This paper reviews the architecture and implementation methods of agents
powered by large language models (LLMs). Motivated by the limitations of
traditional LLMs in real-world tasks, the research aims to explore patterns to
develop "agentic" LLMs that can automate complex tasks and bridge the
performance gap with human capabilities. Key components include a perception
system that converts environmental percepts into meaningful representations; a
reasoning system that formulates plans, adapts to feedback, and evaluates
actions through different techniques like Chain-of-Thought and Tree-of-Thought;
a memory system that retains knowledge through both short-term and long-term
mechanisms; and an execution system that translates internal decisions into
concrete actions. This paper shows how integrating these systems leads to more
capable and generalized software bots that mimic human cognitive processes for
autonomous and intelligent behavior.

</details>


### [31] [Localist LLMs -- A Mathematical Framework for Dynamic Locality Control](https://arxiv.org/abs/2510.09338)
*Joachim Diederich*

Main category: cs.AI

TL;DR: 提出了一种可调节语言模型内部表征的框架，通过局部性调节参数在可解释的局部化表征和高效的分布式表征之间连续切换。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在需要透明度的监管领域同时要求可解释性和高性能的矛盾需求。

Method: 使用组稀疏惩罚、信息论锚点设计和动态规则注入，通过局部性调节参数动态控制表征的局部化程度。

Result: 提供了严格的数学证明，建立了注意力集中在语义相关块上的显式阈值条件，实现了低熵和高保真度。

Conclusion: 该框架支持在可解释模式和高性能模式之间连续插值，适用于需要透明度和能力的监管领域应用。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovation is a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining. This is achieved
through group sparsity penalties on attention mechanisms, information-theoretic
anchor design, and dynamic rule injection. We provide rigorous mathematical
proofs establishing explicit threshold conditions under which attention
provably concentrates on semantically relevant blocks, with exponential bounds
on attention entropy and pointer fidelity. Specifically, we prove that when
group sparsity penalties exceed certain threshold values, the model's attention
mechanisms concentrate on semantically relevant blocks, achieving low entropy
and high fidelity with negligible error. This framework enables practitioners
to continuously interpolate between interpretable and high-performance modes,
supporting applications in regulated domains requiring both transparency and
capability.

</details>


### [32] [Toward Mechanistic Explanation of Deductive Reasoning in Language Models](https://arxiv.org/abs/2510.09340)
*Davide Maltoni,Matteo Ferrara*

Main category: cs.AI

TL;DR: 小语言模型可以通过学习底层规则而非统计学习来解决演绎推理任务，研究发现归纳头在实现逻辑推理中的规则补全和规则链式步骤中发挥核心作用。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在逻辑推理任务中的内部机制，理解模型如何实现演绎推理能力而非仅作为统计学习器。

Method: 使用小语言模型解决演绎推理任务，分析其内部表示和计算电路，特别关注归纳头的作用。

Result: 发现小语言模型能够学习底层规则来解决演绎推理任务，归纳头在规则补全和规则链式步骤中起核心作用。

Conclusion: 语言模型可以通过学习规则而非统计模式来实现逻辑推理，归纳头是实现这种推理能力的关键机制。

Abstract: Recent large language models have demonstrated relevant capabilities in
solving problems that require logical reasoning; however, the corresponding
internal mechanisms remain largely unexplored. In this paper, we show that a
small language model can solve a deductive reasoning task by learning the
underlying rules (rather than operating as a statistical learner). A low-level
explanation of its internal representations and computational circuits is then
provided. Our findings reveal that induction heads play a central role in the
implementation of the rule completion and rule chaining steps involved in the
logical inference required by the task.

</details>


### [33] [Sequence Variables: A Constraint Programming Computational Domain for Routing and Sequencing](https://arxiv.org/abs/2510.09373)
*Augustin Delecluse,Pierre Schaus,Pascal Van Hentenryck*

Main category: cs.AI

TL;DR: 本文提出了序列变量作为约束编程(CP)中处理车辆路径问题(VRP)的新方法，解决了传统后继变量模型无法处理可选访问和插入启发式的问题。


<details>
  <summary>Details</summary>
Motivation: 传统CP模型基于后继变量，无法有效处理VRP中的可选访问或插入式启发式算法，这限制了CP在车辆路径问题中的应用。

Method: 形式化定义了序列变量的计算域、更新操作和一致性级别，实现了支持可选访问和插入启发式的数据结构，并设计了专门用于序列变量和车辆路径的全局约束。

Result: 序列变量简化了问题建模，在Dial-a-Ride问题上实现了有竞争力的计算性能。

Conclusion: 序列变量为CP处理VRP提供了更直观和强大的框架，能够处理可选访问并支持插入式启发式算法。

Abstract: Constraint Programming (CP) offers an intuitive, declarative framework for
modeling Vehicle Routing Problems (VRP), yet classical CP models based on
successor variables cannot always deal with optional visits or insertion based
heuristics. To address these limitations, this paper formalizes sequence
variables within CP. Unlike the classical successor models, this computational
domain handle optional visits and support insertion heuristics, including
insertion-based Large Neighborhood Search. We provide a clear definition of
their domain, update operations, and introduce consistency levels for
constraints on this domain. An implementation is described with the underlying
data structures required for integrating sequence variables into existing
trail-based CP solvers. Furthermore, global constraints specifically designed
for sequence variables and vehicle routing are introduced. Finally, the
effectiveness of sequence variables is demonstrated by simplifying problem
modeling and achieving competitive computational performance on the Dial-a-Ride
Problem.

</details>


### [34] [Agentic Systems in Radiology: Design, Applications, Evaluation, and Challenges](https://arxiv.org/abs/2510.09404)
*Christian Bluethgen,Dave Van Veen,Daniel Truhn,Jakob Nikolas Kather,Michael Moor,Malgorzata Polacin,Akshay Chaudhari,Thomas Frauenfelder,Curtis P. Langlotz,Michael Krauthammer,Farhad Nooralahzadeh*

Main category: cs.AI

TL;DR: 这篇论文探讨了如何利用大型语言模型构建智能代理系统，特别是在放射学领域的应用，通过整合外部工具和反馈机制来实现不同程度的自主性。


<details>
  <summary>Details</summary>
Motivation: 放射学具有多模态数据流和复杂工作流程的特点，非常适合利用LLM代理来自动化重复性任务并适应上下文变化。虽然LLM在单个任务上表现良好，但孤立使用无法充分发挥其在复杂多步骤工作流程中的潜力。

Method: 通过为LLM配备外部工具和反馈机制，构建能够驱动系统的代理，实现从半自动化工作流程到能够管理复杂流程的自适应代理的自主性谱系。

Result: 论文回顾了此类LLM驱动代理系统的设计，突出了关键应用，讨论了规划和工具使用的评估方法。

Conclusion: LLM驱动的代理系统在放射学领域具有巨大潜力，但面临错误级联、工具使用效率和健康IT集成等挑战。

Abstract: Building agents, systems that perceive and act upon their environment with a
degree of autonomy, has long been a focus of AI research. This pursuit has
recently become vastly more practical with the emergence of large language
models (LLMs) capable of using natural language to integrate information,
follow instructions, and perform forms of "reasoning" and planning across a
wide range of tasks. With its multimodal data streams and orchestrated
workflows spanning multiple systems, radiology is uniquely suited to benefit
from agents that can adapt to context and automate repetitive yet complex
tasks. In radiology, LLMs and their multimodal variants have already
demonstrated promising performance for individual tasks such as information
extraction and report summarization. However, using LLMs in isolation
underutilizes their potential to support complex, multi-step workflows where
decisions depend on evolving context from multiple information sources.
Equipping LLMs with external tools and feedback mechanisms enables them to
drive systems that exhibit a spectrum of autonomy, ranging from semi-automated
workflows to more adaptive agents capable of managing complex processes. This
review examines the design of such LLM-driven agentic systems, highlights key
applications, discusses evaluation methods for planning and tool use, and
outlines challenges such as error cascades, tool-use efficiency, and health IT
integration.

</details>


### [35] [Titans Revisited: A Lightweight Reimplementation and Critical Analysis of a Test-Time Memory Model](https://arxiv.org/abs/2510.09551)
*Gavriel Di Nepi,Federico Siciliano,Fabrizio Silvestri*

Main category: cs.AI

TL;DR: 对Google Titans模型进行了轻量级重新实现和评估，发现其神经记忆组件能持续提升性能，但由于分块处理并不总是优于现有基线。


<details>
  <summary>Details</summary>
Motivation: Google Titans模型缺乏公开代码且原始描述模糊，阻碍了可复现性，因此需要重新实现并进行全面评估。

Method: 对Titans模型进行轻量级重新实现，并在掩码语言建模、时间序列预测和推荐任务上进行综合评估。

Result: Titans由于分块处理并不总是优于现有基线，但其神经记忆组件相比仅使用注意力的模型能持续提升性能。

Conclusion: 确认了该模型的创新潜力，同时指出了实际局限性，并为未来研究提出了问题。

Abstract: By the end of 2024, Google researchers introduced Titans: Learning at Test
Time, a neural memory model achieving strong empirical results across multiple
tasks. However, the lack of publicly available code and ambiguities in the
original description hinder reproducibility. In this work, we present a
lightweight reimplementation of Titans and conduct a comprehensive evaluation
on Masked Language Modeling, Time Series Forecasting, and Recommendation tasks.
Our results reveal that Titans does not always outperform established baselines
due to chunking. However, its Neural Memory component consistently improves
performance compared to attention-only models. These findings confirm the
model's innovative potential while highlighting its practical limitations and
raising questions for future research.

</details>


### [36] [Safe, Untrusted, "Proof-Carrying" AI Agents: toward the agentic lakehouse](https://arxiv.org/abs/2510.09567)
*Jacopo Tagliabue,Ciro Greco*

Main category: cs.AI

TL;DR: 本文提出使用API优先、可编程的数据湖仓架构来构建安全的设计，支持AI代理工作流。通过Bauplan案例研究，展示了数据分支和声明式环境如何自然扩展到代理，实现可重现性和可观测性，同时减少攻击面。


<details>
  <summary>Details</summary>
Motivation: 数据湖仓运行敏感工作负载，AI驱动的自动化引发了关于信任、正确性和治理的担忧。需要为AI代理工作流提供安全的设计保障。

Method: 使用Bauplan作为案例研究，采用数据分支和声明式环境扩展AI代理能力，通过受证明携带代码启发的正确性检查来修复数据管道。

Result: 原型演示表明，不受信任的AI代理可以在生产数据上安全操作，为完全代理化的湖仓指明了路径。

Conclusion: API优先、可编程的湖仓架构为安全的设计提供了正确的抽象，能够支持AI代理工作流，同时确保可重现性、可观测性和安全性。

Abstract: Data lakehouses run sensitive workloads, where AI-driven automation raises
concerns about trust, correctness, and governance. We argue that API-first,
programmable lakehouses provide the right abstractions for safe-by-design,
agentic workflows. Using Bauplan as a case study, we show how data branching
and declarative environments extend naturally to agents, enabling
reproducibility and observability while reducing the attack surface. We present
a proof-of-concept in which agents repair data pipelines using correctness
checks inspired by proof-carrying code. Our prototype demonstrates that
untrusted AI agents can operate safely on production data and outlines a path
toward a fully agentic lakehouse.

</details>


### [37] [GraphMERT: Efficient and Scalable Distillation of Reliable Knowledge Graphs from Unstructured Data](https://arxiv.org/abs/2510.09580)
*Margarita Belova,Jiaxin Xiao,Shikhar Tuli,Niraj K. Jha*

Main category: cs.AI

TL;DR: GraphMERT是一个小型图形编码器模型，能够从非结构化文本语料库中提取高质量知识图谱，形成模块化的神经符号堆栈，在保持最先进基准准确性的同时提供可验证的符号推理。


<details>
  <summary>Details</summary>
Motivation: 解决神经符号AI框架难以扩展的问题，以及神经方法的隐式表示和近似推理限制了可解释性和可信度的问题。知识图谱作为显式语义知识的黄金标准表示可以解决符号方面的问题，但从文本语料库自动推导可靠知识图谱仍是一个开放问题。

Method: 引入GraphMERT，一个微小的图形编码器模型，从非结构化文本语料库及其内部表示中提取高质量知识图谱。GraphMERT与其等效的知识图谱形成模块化神经符号堆栈：神经学习抽象；符号知识图谱用于可验证推理。

Result: 在PubMed糖尿病论文文本上，80M参数的GraphMERT生成的知识图谱达到69.8%的FActScore和68.8%的ValidityScore，而32B参数的基线LLM仅达到40.2%的FActScore和43.0%的ValidityScore。

Conclusion: GraphMERT + KG是第一个高效且可扩展的神经符号模型，在实现最先进基准准确性的同时，相对于基线具有优越的符号表示能力。

Abstract: Researchers have pursued neurosymbolic artificial intelligence (AI)
applications for nearly three decades because symbolic components provide
abstraction while neural components provide generalization. Thus, a marriage of
the two components can lead to rapid advancements in AI. Yet, the field has not
realized this promise since most neurosymbolic AI frameworks fail to scale. In
addition, the implicit representations and approximate reasoning of neural
approaches limit interpretability and trust. Knowledge graphs (KGs), a
gold-standard representation of explicit semantic knowledge, can address the
symbolic side. However, automatically deriving reliable KGs from text corpora
has remained an open problem. We address these challenges by introducing
GraphMERT, a tiny graphical encoder-only model that distills high-quality KGs
from unstructured text corpora and its own internal representations. GraphMERT
and its equivalent KG form a modular neurosymbolic stack: neural learning of
abstractions; symbolic KGs for verifiable reasoning. GraphMERT + KG is the
first efficient and scalable neurosymbolic model to achieve state-of-the-art
benchmark accuracy along with superior symbolic representations relative to
baselines.
  Concretely, we target reliable domain-specific KGs that are both (1) factual
(with provenance) and (2) valid (ontology-consistent relations with
domain-appropriate semantics). When a large language model (LLM), e.g.,
Qwen3-32B, generates domain-specific KGs, it falls short on reliability due to
prompt sensitivity, shallow domain expertise, and hallucinated relations. On
text obtained from PubMed papers on diabetes, our 80M-parameter GraphMERT
yields a KG with a 69.8% FActScore; a 32B-parameter baseline LLM yields a KG
that achieves only 40.2% FActScore. The GraphMERT KG also attains a higher
ValidityScore of 68.8%, versus 43.0% for the LLM baseline.

</details>


### [38] [LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?](https://arxiv.org/abs/2510.09595)
*Kaijian Zou,Aaron Xiong,Yunxiang Zhang,Frederick Zhang,Yueqi Ren,Jirong Yang,Ayoung Lee,Shitanshu Bhushan,Lu Wang*

Main category: cs.AI

TL;DR: LiveOIBench是一个包含403个奥林匹克级别编程竞赛问题的基准测试，每个问题平均有60个专家设计的测试用例，旨在解决现有编程基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前编程基准测试存在缺乏高难度问题、测试用例覆盖不足、依赖在线平台API导致可访问性差等问题，需要更全面的评估标准。

Method: 从72个2023-2025年官方信息学奥林匹克竞赛中直接收集问题，构建包含详细子任务评分标准和大量私有测试用例的高质量数据集，并建立自包含的离线评估系统。

Result: 在评估32个主流LLM后，GPT-5达到81.76百分位，表现强劲但仍不及顶尖人类选手（通常超过90百分位）；开源推理模型GPT-OSS-120B仅达60百分位，与前沿闭源模型存在显著差距。

Conclusion: 强大的推理模型应优先进行精确的问题分析而非过度探索，未来模型应强调结构化分析并减少不必要的探索。所有数据、代码和排行榜结果将公开。

Abstract: Competitive programming problems increasingly serve as valuable benchmarks to
evaluate the coding capabilities of large language models (LLMs) due to their
complexity and ease of verification. Yet, current coding benchmarks face
limitations such as lack of exceptionally challenging problems, insufficient
test case coverage, reliance on online platform APIs that limit accessibility.
To address these issues, we introduce LiveOIBench, a comprehensive benchmark
featuring 403 expert-curated Olympiad-level competitive programming problems,
each with an average of 60 expert-designed test cases. The problems are sourced
directly from 72 official Informatics Olympiads in different regions conducted
between 2023 and 2025. LiveOIBench distinguishes itself through four key
features: (1) meticulously curated high-quality tasks with detailed subtask
rubrics and extensive private test cases; (2) direct integration of elite
contestant performance data to enable informative comparison against
top-performing humans; (3) planned continuous, contamination-free updates from
newly released Olympiad problems; and (4) a self-contained evaluation system
facilitating offline and easy-to-reproduce assessments. Benchmarking 32 popular
general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable
81.76th percentile, a strong result that nonetheless falls short of top human
contestant performance, who usually place above 90th. In contrast, among
open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile,
underscoring significant capability disparities from frontier closed models.
Detailed analyses indicate that robust reasoning models prioritize precise
problem analysis over excessive exploration, suggesting future models should
emphasize structured analysis and minimize unnecessary exploration. All data,
code, and leaderboard results will be made publicly available on our website.

</details>
